<!DOCTYPE html>
<html>

<head>
    
    <meta charset="utf-8">
<meta http-equiv="X-UA-Compatible" content="chrome=1">
<meta name="HandheldFriendly" content="True">
<meta name="MobileOptimized" content="320">
<meta name="viewport" content="width=device-width, initial-scale=1.0">
<meta name="referrer" content="no-referrer">

<meta name="description" content="Ocean on GitHub.">

<meta name="twitter:card" content="summary">
<meta name="twitter:domain" content="https://oceanbao.github.io/">

<meta name="twitter:image" content="https://oceanbao.github.io/tn.png">
<meta name="twitter:title" property="og:title" itemprop="title name" content="Ocean Ode">
<meta name="twitter:description" property="og:description" itemprop="description" content="Ocean on GitHub.">
<meta name="og:type" content="website">
<meta name="og:url" content="https://oceanbao.github.io/">
<meta name="og:image" itemprop="image primaryImageOfPage" content="https://oceanbao.github.io/tn.png">

<link rel="shortcut icon" href="https://oceanbao.github.io/oceanicon.jpg" id="favicon">
<link rel="stylesheet" href="https://oceanbao.github.io/css/style.css">

<link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Didact+Gothic">


    

    
    
    
    <title>
        
        Scrapy
        
    </title>
</head>

<body>

    <div class="wrap">
        <div class="section" id="title">Scrapy</div>
        <aside>
            <nav id="TableOfContents">
<ul>
<li>
<ul>
<li><a href="#table-of-contents">Table of Contents</a></li>
</ul></li>
<li><a href="#basic-crawling">Basic Crawling</a>
<ul>
<li>
<ul>
<li><a href="#ur-2im">$UR^2IM$</a></li>
<li><a href="#scrapy-project">Scrapy Project</a></li>
<li><a href="#defining-items">Defining ITEMS</a></li>
<li><a href="#writing-spiders">Writing Spiders</a></li>
<li><a href="#populating-item">Populating ITEM</a></li>
<li><a href="#saving-to-files">Saving to Files</a></li>
<li><a href="#clean-up-item-loader-and-housekeeping-fields">Clean Up - ITEM LOADER and housekeeping fields</a></li>
<li><a href="#creating-contracts">Creating Contracts</a>
<ul>
<li><a href="#recap-code">RECAP CODE</a></li>
</ul></li>
<li><a href="#more-urls">MORE URLS</a></li>
<li><a href="#two-direction-crawling">Two-Direction Crawling</a></li>
<li><a href="#two-direction-crawling-with-crawlspider">Two-Direction Crawling with CrawlSpider</a></li>
</ul></li>
</ul></li>
<li><a href="#from-scrapy-to-a-mobile-app">From Scrapy to a MOBILE APP</a>
<ul>
<li>
<ul>
<li><a href="#choosing-a-mobile-app-framework">Choosing a Mobile App Framework</a></li>
</ul></li>
</ul></li>
<li><a href="#quick-spider-recipes">Quick Spider Recipes</a>
<ul>
<li>
<ul>
<li><a href="#a-spider-that-logs-in">A spider that logs in</a></li>
<li><a href="#a-spider-that-uses-json-apis-and-ajax-pages">A spider that uses JSON APIs and AJAX pages</a></li>
<li><a href="#passing-arguments-between-responses">Passing arguments between responses</a></li>
<li><a href="#30x-faster-spider">30x Faster Spider</a></li>
<li><a href="#spider-crawling-based-on-excel-file">Spider crawling based on Excel file</a></li>
</ul></li>
</ul></li>
<li><a href="#deploying-to-scrapinghub">Deploying to Scrapinghub</a>
<ul>
<li>
<ul>
<li><a href="#programmatic-access-to-scrapinghub-jobs-data">Programmatic Access to Scrapinghub Jobs/Data</a></li>
<li><a href="#scheduling-recurring-crawls">Scheduling Recurring Crawls</a></li>
</ul></li>
</ul></li>
<li><a href="#configuration-and-management">Configuration and Management</a>
<ul>
<li>
<ul>
<li><a href="#settings">Settings</a></li>
<li><a href="#essential-settings">Essential Settings</a></li>
<li><a href="#further-settings">Further settings</a></li>
</ul></li>
</ul></li>
<li><a href="#programming-scrapy">Programming Scrapy</a>
<ul>
<li>
<ul>
<li><a href="#scrapy-is-a-twisted-application">SCRAPY IS A TWISTED APPLICATION</a></li>
<li><a href="#deferreds-and-deferred-chains">DEFERREDS AND DEFERRED CHAINS</a></li>
</ul></li>
</ul></li>
<li><a href="#false">False</a></li>
<li><a href="#true">True</a></li>
<li><a href="#3">3</a></li>
<li><a href="#flase">Flase</a></li>
<li><a href="#foo-called">foo called</a></li>
<li><a href="#true-1">True</a></li>
<li><a href="#4">4</a></li>
<li><a href="#twisted-a-python-tale">~<em>~ Twisted - A Python tale ~</em>~</a></li>
<li><a href="#hello-i-m-a-developer-and-i-mainly-setup-wordpress">Hello, I&rsquo;m a developer and I mainly setup Wordpress.</a></li>
<li><a href="#i-do-this-all-day-for-our-customers">I do this all day for our customers</a></li>
<li><a href="#let-s-run-it">Let&rsquo;s run it</a></li>
<li><a href="#twisted-has-a-slightly-different-approach">Twisted has a slightly different approach</a>
<ul>
<li>
<ul>
<li><a href="#signals">Signals</a></li>
<li><a href="#example-2-an-extension-measuring-thorugput-and-latencies">Example 2 - an extension measuring thorugput and latencies</a></li>
<li><a href="#extending-beyond-middlewares">EXTENDING BEYOND MIDDLEWARES</a></li>
</ul></li>
</ul></li>
<li><a href="#pipeline-recipes">PIPELINE Recipes</a>
<ul>
<li>
<ul>
<li><a href="#using-rest-apis">Using REST APIs</a></li>
</ul></li>
</ul></li>
<li><a href="#official-scrapy-tutorial">Official Scrapy Tutorial</a>
<ul>
<li><a href="#udemy-video-summary">Udemy Video Summary</a>
<ul>
<li><a href="#1-scrapy-architecture">(1) Scrapy Architecture</a></li>
<li><a href="#2-avoiding-ban">(2) Avoiding Ban</a></li>
<li><a href="#3-runspider-for-standalone-scripting">(3) Runspider for standalone scripting</a></li>
<li><a href="#4-scrapy-spiders-crawlspider-has-more-functions-such-as-rule-which-need-importing-itself">(4) scrapy.spiders.CrawlSpider has more functions such as RULE, which need importing itself</a></li>
<li><a href="#5-scrapy-http-request-method-used-under-ordinary-parse-self-response-without-callback-to-loop-through-new-request-url-to-parse">(5) scrapy.http.Request method used under ordinary parse(self,response) WITHOUT callback to loop through new Request(url) to parse() !!</a></li>
<li><a href="#6-relative-url-fixing-e-g-images">(6) relative URL fixing, e.g. images</a></li>
<li><a href="#7-define-functions-to-extract-well-formated-datapoints-e-g-tables-of-data">(7) Define functions to extract well-formated datapoints, e.g. tables of data</a></li>
<li><a href="#example-code">EXAMPLE CODE</a></li>
<li><a href="#8-arguments-e-g-isolating-book-categories">(8) Arguments: e.g. isolating &lsquo;book categories&rsquo;</a></li>
<li><a href="#9-scrapy-functions-executed-at-end-of-crawling">(9) Scrapy Functions: executed at end of crawling</a></li>
<li><a href="#10-feeding">(10) Feeding</a></li>
<li><a href="#11-image-download-via-built-in-imagespipeline">(11) Image Download via built-in ImagesPipeline</a></li>
</ul></li>
</ul></li>
</ul>
</nav>
        </aside>
        <div class="section" id="content">

<h1>Learning Scrapy</h1>

<h2 id="table-of-contents">Table of Contents</h2>

<ol>
<li><a href="#basic">Basic Crawling</a></li>
<li><a href="#mobile">Mobile App</a></li>
<li><a href="#recipe">Spider Recipes</a></li>
<li><a href="#hub">Scrapinghub</a></li>
<li><a href="#config">Configuration &amp; Management</a></li>
<li><a href="#programming">Programming Scrapy</a></li>
<li><a href="#pipeline">Pipeline Recipe</a></li>
<li><a href="#official">Official Tutorial</a></li>
</ol>

<h1 id="basic-crawling">Basic Crawling</h1>

<p><a name="basic"></a></p>

<h3 id="ur-2im">$UR^2IM$</h3>

<ul>
<li>URL</li>
<li>Request</li>
<li>Response</li>
<li>Items</li>
<li>More URLs (recurring to Request)</li>
</ul>

<p><code>scrapy shell -s USER_AGENT=&quot;Mozilla/5.0&quot; &lt;URL&gt;</code></p>

<p><code>response.body[:50]</code></p>

<p>Actual value is gained via <code>extract()</code> or <code>re()</code></p>

<h3 id="scrapy-project">Scrapy Project</h3>

<p>Shell is mere utility aiding testing, real codes start with Project.</p>

<p><code>scrapy startproject properties</code></p>

<p>This chapter focuses on <code>items.py</code> and <code>spiders</code> directory.</p>

<h3 id="defining-items">Defining ITEMS</h3>

<ul>
<li>Redefine class to fitting name</li>
<li>NOTE: Declaring a field NOT equal filling it on every spider</li>

<li><p>Fields</p>

<ul>
<li><strong>images</strong> - images pipeline will auto-fill this based on <code>image_urls</code></li>
<li><strong>location</strong> - Geocoding pipeline will auto-fill this</li>
<li>Self-defined <em>housekeeping</em> fields for debugging</li>
<li>url - response.url</li>
<li>project - self.settings.get(&lsquo;BOT_NAME&rsquo;)</li>
<li>spider - self.name</li>
<li>server - socket.gethostname()</li>
<li>date - datetime.datetime.now()</li>
</ul></li>

<li><p>With a list of fields, it&rsquo;s easy to mod and cutomise <code>class</code> default:</p>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-python" data-lang="python"><span style="color:#f92672">from</span> scrapy.item <span style="color:#f92672">import</span> Item, Field

<span style="color:#66d9ef">class</span> <span style="color:#a6e22e">PropertiesItem</span>(Item):
<span style="color:#75715e"># Primary fields</span>
title <span style="color:#f92672">=</span> Field()
price <span style="color:#f92672">=</span> Field()
description <span style="color:#f92672">=</span> Field()
address <span style="color:#f92672">=</span> Field()
image_urls <span style="color:#f92672">=</span> Field()
    
<span style="color:#75715e"># Calculated fields</span>
images <span style="color:#f92672">=</span> Field()
location <span style="color:#f92672">=</span> Field()
    
<span style="color:#75715e"># Housekeeping fields</span>
url <span style="color:#f92672">=</span> Field()
project <span style="color:#f92672">=</span> Field()
spider <span style="color:#f92672">=</span> Field()
server <span style="color:#f92672">=</span> Field()
date <span style="color:#f92672">=</span> Field()</code></pre></div></li>
</ul>

<h3 id="writing-spiders">Writing Spiders</h3>

<p>Halfway, typically one spider per website or a section of website if large. A spider code implements $UR^2IM$ process. TIP: spider or project? A project groups <code>items</code> and spiders, designed for same type over many sites, as above can be used generally.</p>

<p><code>scrape genspider basic/crawl web</code></p>

<p>TIP: Scrapy has many subdir but all cmd assumes root dir where lies <code>scrapy.cfg</code> file. Whenever referring to &lsquo;packages and modules&rsquo;, they are set as to map to directory structure. E.g. <code>ocean.spiders.basic</code> is under <code>ocean/spiders</code> directory.</p>

<p>The <code>self</code> reference in <code>parse()</code> enables functionality of spider.</p>

<p>Start coding and use <code>log()</code> to output info in the primary fields table.</p>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-python" data-lang="python"><span style="color:#66d9ef">def</span> <span style="color:#a6e22e">parse</span>(self, response):
    self<span style="color:#f92672">.</span>log(<span style="color:#e6db74">&#34;title: </span><span style="color:#e6db74">%s</span><span style="color:#e6db74">&#34;</span> <span style="color:#f92672">%</span> response<span style="color:#f92672">.</span>xpath(<span style="color:#e6db74">&#39;//*[@itemprop=&#34;name&#34;][1]/text()&#39;</span>)<span style="color:#f92672">.</span>extract())
    <span style="color:#75715e"># similarly for others</span></code></pre></div>
<p><code>scrapy crawl</code></p>

<blockquote>
<p><code>self.log()</code> output DEBUG: sessions for inspecting correctness</p>
</blockquote>

<p><code>scrapy parse</code></p>

<p>This allows to use <strong>most suitable</strong> spider to parse any URL as ARG. BUT best specify.</p>

<p><code>scrapy parse --spider=crawl &lt;URL&gt;</code></p>

<p>This outputs similar info as above and often used for DEBUGGING.</p>

<h3 id="populating-item">Populating ITEM</h3>

<p>Slight mod yet &ldquo;unlocking&rdquo; tons of functionalities.</p>

<p>INIT and return one. Adding to <code>parse()</code> function process.</p>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-python" data-lang="python"><span style="color:#f92672">from</span> properties.items <span style="color:#f92672">import</span> PropertiesItem

<span style="color:#75715e"># inside parse()</span>
item <span style="color:#f92672">=</span> PropertiesItem()
item[<span style="color:#e6db74">&#39;title&#39;</span>] <span style="color:#f92672">=</span> response<span style="color:#f92672">.</span>xpath(<span style="color:#e6db74">&#39;//*[@itemprop=&#34;name&#34;][1]/text()&#39;</span>)<span style="color:#f92672">.</span>extract()
<span style="color:#75715e"># et les restes </span>
<span style="color:#66d9ef">return</span> item</code></pre></div>
<p><a name="pipeline"></a></p>

<p>Now <code>scrape crawl basic</code> returns not LOG but DICT of the item. Scrapy is built around the ITEMs to be used by PIPELINEs for more functionalities.</p>

<h3 id="saving-to-files">Saving to Files</h3>

<p><code>scrape crawl basic -o items.json .jl .csv .xml</code></p>

<p>CSV and XML popular for excel apps. JSON for expressiveness and link to JavaScript. <code>.jl</code> files have one JSON object per line, read more efficiently.</p>

<p>To save on cloud:</p>

<p><code>scrape crawl basic -o &quot;ftp://user:pass@ftp.scrapybook.com/item-s.json&quot;</code></p>

<p><code>scrapy crawl basic -o &quot;s3://aws_key:aws_secret@scrapy-book/items.json&quot;</code></p>

<p>Scrapy parse now adjusted to the new setting. You&rsquo;ll appreciate it even more while DEBUG URLs that give unexpected results.</p>

<h3 id="clean-up-item-loader-and-housekeeping-fields">Clean Up - ITEM LOADER and housekeeping fields</h3>

<p><code>ItemLoader</code> class replaces all messy looking <code>extract()</code> and <code>xpath()</code> operations.</p>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-python" data-lang="python"><span style="color:#f92672">from</span> scrapy.loader <span style="color:#f92672">import</span> ItemLoader

<span style="color:#66d9ef">def</span> <span style="color:#a6e22e">parse</span>(self, response):
    l <span style="color:#f92672">=</span> ItemLoader(item <span style="color:#f92672">=</span> PropertiesItem(), response <span style="color:#f92672">=</span> response)
    l<span style="color:#f92672">.</span>add_xpath(<span style="color:#e6db74">&#39;title&#39;</span>, <span style="color:#e6db74">&#39;//*[@itemprop=&#34;name&#34;][1]/text()&#39;</span>)
    l<span style="color:#f92672">.</span>add_xpath(<span style="color:#e6db74">&#39;price&#39;</span>, <span style="color:#e6db74">&#39;.//[@itemprop=&#34;price&#34;][1]/text()&#39;</span>, re <span style="color:#f92672">=</span> <span style="color:#e6db74">&#39;[,.0-9]+&#39;</span>)
    <span style="color:#66d9ef">return</span> l<span style="color:#f92672">.</span>load_item()</code></pre></div>
<p>More than clean, it declares very clearly intention of action. <code>ItemLoader</code> provie many cool ways of mixing data, formatting, cleaning up. Note they are actively developed so keep abreast <a href="http://doc.scrapy.org/en/latest/topics/loaders.html">here</a></p>

<p><strong>Processors</strong> are fast and neat functions manipulating multiple selectors.</p>

<table>
<thead>
<tr>
<th>Processor</th>
<th>Functionality</th>
</tr>
</thead>

<tbody>
<tr>
<td><code>Join()</code></td>
<td>Concatenates multiple results into one</td>
</tr>

<tr>
<td><code>MapCompose(unicode.strip)</code></td>
<td>Chaining python func: i.e. Removes leading and trailling whitespace chars</td>
</tr>

<tr>
<td><code>MapCompose(unicode.title)</code></td>
<td>Also gives title cased results</td>
</tr>

<tr>
<td><code>MapCompose(float)</code></td>
<td>converts strings to integers</td>
</tr>

<tr>
<td><code>MapCompose(lambda i: i.replace(',', ''), float)</code></td>
<td>WOW, inheriting all power of lambda function&hellip;.</td>
</tr>

<tr>
<td><code>MapCompose(lambda i: urlparse.urljoin(response.url, i))</code></td>
<td>converts relative URLs to absolute using <code>response.url</code> as base</td>
</tr>
</tbody>
</table>

<p>Possible use of any Python expression as a processor. These are simply functions embedded in Scrapy, such that possible to try in SHELL</p>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell"><span style="color:#75715e"># scrapy shell someweb</span>
from scrapy.loader.processors import MapCompose, Join

Join<span style="color:#f92672">()([</span><span style="color:#e6db74">&#39;hi&#39;</span>, <span style="color:#e6db74">&#39;John&#39;</span><span style="color:#f92672">])</span>
&gt;&gt;&gt; u<span style="color:#e6db74">&#39;hi John&#39;</span></code></pre></div>
<p>Let&rsquo;s see how to add them inside <code>parse()</code></p>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-python" data-lang="python"><span style="color:#75715e"># RECALL to import relevant modules</span>
<span style="color:#f92672">import</span> datetime<span style="color:#f92672">,</span> socket<span style="color:#f92672">,</span> urlparse

<span style="color:#75715e"># for processing items</span>
l<span style="color:#f92672">.</span>add_xpath(<span style="color:#e6db74">&#39;title&#39;</span>, <span style="color:#e6db74">&#39;XPATH&#39;</span>, MapCompose())

<span style="color:#75715e"># for easy, housekeeping fields</span>
l<span style="color:#f92672">.</span>add_value(<span style="color:#e6db74">&#39;url&#39;</span>, response<span style="color:#f92672">.</span>url)
l<span style="color:#f92672">.</span>add_value(<span style="color:#e6db74">&#39;project&#39;</span>, self<span style="color:#f92672">.</span>settings<span style="color:#f92672">.</span>get(<span style="color:#e6db74">&#34;BOT_NAME&#34;</span>))
l<span style="color:#f92672">.</span>add_value(<span style="color:#e6db74">&#39;spider&#39;</span>, self<span style="color:#f92672">.</span>name)
l<span style="color:#f92672">.</span>add_value(<span style="color:#e6db74">&#39;server&#39;</span>, socket<span style="color:#f92672">.</span>gethosename())
l<span style="color:#f92672">.</span>add_value(<span style="color:#e6db74">&#39;date&#39;</span>, datetime<span style="color:#f92672">.</span>datetime<span style="color:#f92672">.</span>now())</code></pre></div>
<blockquote>
<p>Perfectly looking <code>Items</code> and might at first glance seems complex. BUT it&rsquo;s worth it, especially considering the similar power requires tons more codes in other langues, here only 25-line of codes.</p>

<p>Another feeling stems from all those processors and <code>ItemLoaders</code> These xeno-python codes are worth the effort for serious web scraping journey.</p>
</blockquote>

<h3 id="creating-contracts">Creating Contracts</h3>

<p>Contracts like unit tests for spiders, for quickly checking broke code. For instance, checking old spiders if working now. Contracts are included in the comments just after the name of function (docstring) starting with <code>@</code></p>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-python" data-lang="python"><span style="color:#66d9ef">def</span> <span style="color:#a6e22e">parse</span>(self, response):
    <span style="color:#e6db74">&#34;&#34;&#34; This function parses a property page.
</span><span style="color:#e6db74">    
</span><span style="color:#e6db74">    @url http://web:9312/proeprties/property_000000.html
</span><span style="color:#e6db74">    @returns items 1
</span><span style="color:#e6db74">    @scrapes title price description address image_urls
</span><span style="color:#e6db74">    @scrapes url project spider server date
</span><span style="color:#e6db74">    &#34;&#34;&#34;</span></code></pre></div>
<p>It means &lsquo;checking this URL and you should fine one item with values on those fields enlisted here&rsquo;.</p>

<p><code>scrapy check</code>  will go and check whether the contracts are valid : <code>scrapy check basic</code> In case of error</p>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">FAIL: <span style="color:#f92672">[</span>basic<span style="color:#f92672">]</span> parse <span style="color:#f92672">(</span>@scrapes post-hook<span style="color:#f92672">)</span>
------------------------------------------------------------------------------------
ContractFail: <span style="color:#e6db74">&#39;url&#39;</span> field is missing</code></pre></div>
<p>Fail either in code or selector. Good first line of check.</p>

<h4 id="recap-code">RECAP CODE</h4>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-python" data-lang="python"><span style="color:#f92672">import</span> datetime
<span style="color:#f92672">import</span> urlparse
<span style="color:#f92672">import</span> socket
<span style="color:#f92672">import</span> scrapy

<span style="color:#f92672">from</span> scrapy.loader.processors <span style="color:#f92672">import</span> MapCompose, Join
<span style="color:#f92672">from</span> scrapy.loader <span style="color:#f92672">import</span> ItemLoader
<span style="color:#f92672">from</span> scrapy.http <span style="color:#f92672">import</span> Request

<span style="color:#f92672">from</span> properties.items <span style="color:#f92672">import</span> PropertiesItem


<span style="color:#66d9ef">class</span> <span style="color:#a6e22e">BasicSpider</span>(scrapy<span style="color:#f92672">.</span>Spider):
    name <span style="color:#f92672">=</span> <span style="color:#e6db74">&#34;manual&#34;</span>
    allowed_domains <span style="color:#f92672">=</span> [<span style="color:#e6db74">&#34;web&#34;</span>]

    <span style="color:#75715e"># Start on the first index page</span>
    start_urls <span style="color:#f92672">=</span> (
        <span style="color:#e6db74">&#39;http://web:9312/properties/index_00000.html&#39;</span>,
    )

    <span style="color:#66d9ef">def</span> <span style="color:#a6e22e">parse</span>(self, response):
        <span style="color:#75715e"># Get the next index URLs and yield Requests</span>
        next_selector <span style="color:#f92672">=</span> response<span style="color:#f92672">.</span>xpath(<span style="color:#e6db74">&#39;//*[contains(@class,&#34;next&#34;)]//@href&#39;</span>)
        <span style="color:#66d9ef">for</span> url <span style="color:#f92672">in</span> next_selector<span style="color:#f92672">.</span>extract():
            <span style="color:#66d9ef">yield</span> Request(urlparse<span style="color:#f92672">.</span>urljoin(response<span style="color:#f92672">.</span>url, url))

        <span style="color:#75715e"># Get item URLs and yield Requests</span>
        item_selector <span style="color:#f92672">=</span> response<span style="color:#f92672">.</span>xpath(<span style="color:#e6db74">&#39;//*[@itemprop=&#34;url&#34;]/@href&#39;</span>)
        <span style="color:#66d9ef">for</span> url <span style="color:#f92672">in</span> item_selector<span style="color:#f92672">.</span>extract():
            <span style="color:#66d9ef">yield</span> Request(urlparse<span style="color:#f92672">.</span>urljoin(response<span style="color:#f92672">.</span>url, url),
                          callback<span style="color:#f92672">=</span>self<span style="color:#f92672">.</span>parse_item)

    <span style="color:#66d9ef">def</span> <span style="color:#a6e22e">parse_item</span>(self, response):
        <span style="color:#e6db74">&#34;&#34;&#34; This function parses a property page.
</span><span style="color:#e6db74">
</span><span style="color:#e6db74">        @url http://web:9312/properties/property_000000.html
</span><span style="color:#e6db74">        @returns items 1
</span><span style="color:#e6db74">        @scrapes title price description address image_urls
</span><span style="color:#e6db74">        @scrapes url project spider server date
</span><span style="color:#e6db74">        &#34;&#34;&#34;</span>

        <span style="color:#75715e"># Create the loader using the response</span>
        l <span style="color:#f92672">=</span> ItemLoader(item<span style="color:#f92672">=</span>PropertiesItem(), response<span style="color:#f92672">=</span>response)

        <span style="color:#75715e"># Load fields using XPath expressions</span>
        l<span style="color:#f92672">.</span>add_xpath(<span style="color:#e6db74">&#39;title&#39;</span>, <span style="color:#e6db74">&#39;//*[@itemprop=&#34;name&#34;][1]/text()&#39;</span>,
                    MapCompose(unicode<span style="color:#f92672">.</span>strip, unicode<span style="color:#f92672">.</span>title))
        l<span style="color:#f92672">.</span>add_xpath(<span style="color:#e6db74">&#39;price&#39;</span>, <span style="color:#e6db74">&#39;.//*[@itemprop=&#34;price&#34;][1]/text()&#39;</span>,
                    MapCompose(<span style="color:#66d9ef">lambda</span> i: i<span style="color:#f92672">.</span>replace(<span style="color:#e6db74">&#39;,&#39;</span>, <span style="color:#e6db74">&#39;&#39;</span>), float),
                    re<span style="color:#f92672">=</span><span style="color:#e6db74">&#39;[,.0-9]+&#39;</span>)
        l<span style="color:#f92672">.</span>add_xpath(<span style="color:#e6db74">&#39;description&#39;</span>, <span style="color:#e6db74">&#39;//*[@itemprop=&#34;description&#34;][1]/text()&#39;</span>,
                    MapCompose(unicode<span style="color:#f92672">.</span>strip), Join())
        l<span style="color:#f92672">.</span>add_xpath(<span style="color:#e6db74">&#39;address&#39;</span>,
                    <span style="color:#e6db74">&#39;//*[@itemtype=&#34;http://schema.org/Place&#34;][1]/text()&#39;</span>,
                    MapCompose(unicode<span style="color:#f92672">.</span>strip))
        l<span style="color:#f92672">.</span>add_xpath(<span style="color:#e6db74">&#39;image_urls&#39;</span>, <span style="color:#e6db74">&#39;//*[@itemprop=&#34;image&#34;][1]/@src&#39;</span>,
                    MapCompose(<span style="color:#66d9ef">lambda</span> i: urlparse<span style="color:#f92672">.</span>urljoin(response<span style="color:#f92672">.</span>url, i)))

        <span style="color:#75715e"># Housekeeping fields</span>
        l<span style="color:#f92672">.</span>add_value(<span style="color:#e6db74">&#39;url&#39;</span>, response<span style="color:#f92672">.</span>url)
        l<span style="color:#f92672">.</span>add_value(<span style="color:#e6db74">&#39;project&#39;</span>, self<span style="color:#f92672">.</span>settings<span style="color:#f92672">.</span>get(<span style="color:#e6db74">&#39;BOT_NAME&#39;</span>))
        l<span style="color:#f92672">.</span>add_value(<span style="color:#e6db74">&#39;spider&#39;</span>, self<span style="color:#f92672">.</span>name)
        l<span style="color:#f92672">.</span>add_value(<span style="color:#e6db74">&#39;server&#39;</span>, socket<span style="color:#f92672">.</span>gethostname())
        l<span style="color:#f92672">.</span>add_value(<span style="color:#e6db74">&#39;date&#39;</span>, datetime<span style="color:#f92672">.</span>datetime<span style="color:#f92672">.</span>now())

        <span style="color:#66d9ef">return</span> l<span style="color:#f92672">.</span>load_item()</code></pre></div>
<p><strong>With CrawlSpider</strong></p>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-python" data-lang="python"><span style="color:#f92672">import</span> datetime
<span style="color:#f92672">import</span> urlparse
<span style="color:#f92672">import</span> socket

<span style="color:#f92672">from</span> scrapy.loader.processors <span style="color:#f92672">import</span> MapCompose, Join
<span style="color:#f92672">from</span> scrapy.linkextractors <span style="color:#f92672">import</span> LinkExtractor
<span style="color:#f92672">from</span> scrapy.spiders <span style="color:#f92672">import</span> CrawlSpider, Rule
<span style="color:#f92672">from</span> scrapy.loader <span style="color:#f92672">import</span> ItemLoader

<span style="color:#f92672">from</span> properties.items <span style="color:#f92672">import</span> PropertiesItem


<span style="color:#66d9ef">class</span> <span style="color:#a6e22e">EasySpider</span>(CrawlSpider):
    name <span style="color:#f92672">=</span> <span style="color:#e6db74">&#39;crawl&#39;</span>
    allowed_domains <span style="color:#f92672">=</span> [<span style="color:#e6db74">&#34;web&#34;</span>]

    <span style="color:#75715e"># Start on the first index page</span>
    start_urls <span style="color:#f92672">=</span> (
        <span style="color:#e6db74">&#39;http://web:9312/properties/index_00000.html&#39;</span>,
    )

    <span style="color:#75715e"># Rules for horizontal and vertical crawling</span>
    rules <span style="color:#f92672">=</span> (
        Rule(LinkExtractor(restrict_xpaths<span style="color:#f92672">=</span><span style="color:#e6db74">&#39;//*[contains(@class,&#34;next&#34;)]&#39;</span>)),
        Rule(LinkExtractor(restrict_xpaths<span style="color:#f92672">=</span><span style="color:#e6db74">&#39;//*[@itemprop=&#34;url&#34;]&#39;</span>),
             callback<span style="color:#f92672">=</span><span style="color:#e6db74">&#39;parse_item&#39;</span>)
    )

    <span style="color:#66d9ef">def</span> <span style="color:#a6e22e">parse_item</span>(self, response):
        <span style="color:#e6db74">&#34;&#34;&#34; This function parses a property page.
</span><span style="color:#e6db74">
</span><span style="color:#e6db74">        @url http://web:9312/properties/property_000000.html
</span><span style="color:#e6db74">        @returns items 1
</span><span style="color:#e6db74">        @scrapes title price description address image_urls
</span><span style="color:#e6db74">        @scrapes url project spider server date
</span><span style="color:#e6db74">        &#34;&#34;&#34;</span>

        <span style="color:#75715e"># Create the loader using the response</span>
        l <span style="color:#f92672">=</span> ItemLoader(item<span style="color:#f92672">=</span>PropertiesItem(), response<span style="color:#f92672">=</span>response)

        <span style="color:#75715e"># Load fields using XPath expressions</span>
        l<span style="color:#f92672">.</span>add_xpath(<span style="color:#e6db74">&#39;title&#39;</span>, <span style="color:#e6db74">&#39;//*[@itemprop=&#34;name&#34;][1]/text()&#39;</span>,
                    MapCompose(unicode<span style="color:#f92672">.</span>strip, unicode<span style="color:#f92672">.</span>title))
        l<span style="color:#f92672">.</span>add_xpath(<span style="color:#e6db74">&#39;price&#39;</span>, <span style="color:#e6db74">&#39;.//*[@itemprop=&#34;price&#34;][1]/text()&#39;</span>,
                    MapCompose(<span style="color:#66d9ef">lambda</span> i: i<span style="color:#f92672">.</span>replace(<span style="color:#e6db74">&#39;,&#39;</span>, <span style="color:#e6db74">&#39;&#39;</span>), float),
                    re<span style="color:#f92672">=</span><span style="color:#e6db74">&#39;[,.0-9]+&#39;</span>)
        l<span style="color:#f92672">.</span>add_xpath(<span style="color:#e6db74">&#39;description&#39;</span>, <span style="color:#e6db74">&#39;//*[@itemprop=&#34;description&#34;][1]/text()&#39;</span>,
                    MapCompose(unicode<span style="color:#f92672">.</span>strip), Join())
        l<span style="color:#f92672">.</span>add_xpath(<span style="color:#e6db74">&#39;address&#39;</span>,
                    <span style="color:#e6db74">&#39;//*[@itemtype=&#34;http://schema.org/Place&#34;][1]/text()&#39;</span>,
                    MapCompose(unicode<span style="color:#f92672">.</span>strip))
        l<span style="color:#f92672">.</span>add_xpath(<span style="color:#e6db74">&#39;image_urls&#39;</span>, <span style="color:#e6db74">&#39;//*[@itemprop=&#34;image&#34;][1]/@src&#39;</span>,
                    MapCompose(<span style="color:#66d9ef">lambda</span> i: urlparse<span style="color:#f92672">.</span>urljoin(response<span style="color:#f92672">.</span>url, i)))

        <span style="color:#75715e"># Housekeeping fields</span>
        l<span style="color:#f92672">.</span>add_value(<span style="color:#e6db74">&#39;url&#39;</span>, response<span style="color:#f92672">.</span>url)
        l<span style="color:#f92672">.</span>add_value(<span style="color:#e6db74">&#39;project&#39;</span>, self<span style="color:#f92672">.</span>settings<span style="color:#f92672">.</span>get(<span style="color:#e6db74">&#39;BOT_NAME&#39;</span>))
        l<span style="color:#f92672">.</span>add_value(<span style="color:#e6db74">&#39;spider&#39;</span>, self<span style="color:#f92672">.</span>name)
        l<span style="color:#f92672">.</span>add_value(<span style="color:#e6db74">&#39;server&#39;</span>, socket<span style="color:#f92672">.</span>gethostname())
        l<span style="color:#f92672">.</span>add_value(<span style="color:#e6db74">&#39;date&#39;</span>, datetime<span style="color:#f92672">.</span>datetime<span style="color:#f92672">.</span>now())

        <span style="color:#66d9ef">return</span> l<span style="color:#f92672">.</span>load_item()</code></pre></div>
<h3 id="more-urls">MORE URLS</h3>

<p>First kind, hardcode LIST of URLs in <code>start_urls = ()</code></p>

<p>Up a notch would be <code>start_urls = [i.strip() for i in open('todo.urls.txt').readlines()]</code></p>

<p><strong>Crawling Direction</strong></p>

<ul>
<li>Horizontal - from index to another

<ul>
<li>Find <strong>Next Page</strong> icon, then parse <code>//*[contains(@class, &quot;next&quot;)]//@href</code></li>
</ul></li>
<li>Vertical - from index page to the listing pages to extract <code>Items</code>

<ul>
<li>Find <strong>Listing</strong> page URL via similar method</li>
</ul></li>
</ul>

<p><em>Check via Shell first</em></p>

<h3 id="two-direction-crawling">Two-Direction Crawling</h3>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-python" data-lang="python"><span style="color:#f92672">from</span> scrapy.http <span style="color:#f92672">import</span> Request
<span style="color:#66d9ef">def</span> <span style="color:#a6e22e">parse</span>(self, response):
    <span style="color:#75715e"># Get next index and yield Requests</span>
    next_selector <span style="color:#f92672">=</span> response<span style="color:#f92672">.</span>xpath(<span style="color:#e6db74">&#39;//@[contains(@class, &#39;</span>next<span style="color:#e6db74">&#39;)]//@href&#39;</span>)
    
    <span style="color:#66d9ef">for</span> url <span style="color:#f92672">in</span> next_selector<span style="color:#f92672">.</span>extract():
        <span style="color:#66d9ef">yield</span> Request(urlparse<span style="color:#f92672">.</span>urljoin(response<span style="color:#f92672">.</span>url, url))
    
    <span style="color:#75715e"># Get item URLs and yield Requests</span>
    item_selector <span style="color:#f92672">=</span> response<span style="color:#f92672">.</span>xpath(<span style="color:#e6db74">&#39;//*[itemprop=&#39;</span>url<span style="color:#e6db74">&#39;]/@href&#39;</span>)
    
    <span style="color:#66d9ef">for</span> url <span style="color:#f92672">in</span> item_selector<span style="color:#f92672">.</span>extract():
        <span style="color:#66d9ef">yield</span> Requests(urlparse<span style="color:#f92672">.</span>urljoin(response<span style="color:#f92672">.</span>url, url), 
                      callback <span style="color:#f92672">=</span> self<span style="color:#f92672">.</span>parse_item)</code></pre></div>
<blockquote>
<p><code>yield</code> DOES NOT EXIT function, but continues with the <code>for</code> loop. PYTHON MAGIC.</p>
</blockquote>

<p>For testing purpose, stop at certain items quantity</p>

<p><code>scrapy crawl manual -s CLOSESPIDER_ITEMCOUNT=90</code></p>

<blockquote>
<p>It first read index, then spawns many Requests, executed. Scrapy uses LIFO strategy to process requests (depth first crawl). Last request submitted will be processed first. Convenient for most cases. E.g. processing each listing page before moving to the next index page, or else fill a huge queue of pending listing pages.</p>

<p>Modifiable in setting PRIORITY argument greater than 0 (higher than default) or less than 0. In general, scrapy scheduler will execute higher priority requests first, but don&rsquo;t spend much time thinking about the exact request should be executed first. Highly likely that not use more than one or two request priority levels in most applicaitons. Note also that URLs are subject to duplication filtering, most often desired. If wishing to perform a request to the same URL more than once, <code>dont_filter = true</code> inside <code>Request()</code></p>
</blockquote>

<h3 id="two-direction-crawling-with-crawlspider">Two-Direction Crawling with CrawlSpider</h3>

<p>Seemingly tedious code in basic spider, CrawlSpider class offers simpler methods.  Once genspider, the extra code inherited on the surface are:</p>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-python" data-lang="python">rules <span style="color:#f92672">=</span> ( 
	Rule(LinkExtractor(allow<span style="color:#f92672">=</span><span style="color:#e6db74">r</span><span style="color:#e6db74">&#39;Items/&#39;</span>), callback <span style="color:#f92672">=</span> <span style="color:#e6db74">&#39;parse_item&#39;</span>, follow <span style="color:#f92672">=</span> true)
)

<span style="color:#66d9ef">def</span> <span style="color:#a6e22e">parse_item</span>(self, resposne):
    <span style="color:#66d9ef">pass</span></code></pre></div>
<p>TIP Why learn manual as above? <code>yield</code> + <code>Requests</code> with <code>callback</code> is such as USEFUL and CORE technique that will use repeated later, worth knowing.</p>

<p>Now mode rules one for horizontal and one for vertical crawlling</p>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-python" data-lang="python">rules <span style="color:#f92672">=</span> (
	Rule(LinkExtractor(restrict_xpaths <span style="color:#f92672">=</span> <span style="color:#e6db74">&#39;//*[contains(@class, &#39;</span>next<span style="color:#e6db74">&#39;)]&#39;</span>)),
    Rule(LinkExtractor(restrict_xpaths <span style="color:#f92672">=</span> <span style="color:#e6db74">&#39;//*[@itemprop=&#39;</span>url<span style="color:#e6db74">&#39;]&#39;</span>), callback <span style="color:#f92672">=</span> <span style="color:#e6db74">&#39;parse_item&#39;</span>)
)</code></pre></div>
<blockquote>
<p>What differ are missing <code>a</code> and <code>href</code> constraints, for LinkExtractor by default looks for those two <code>elemetns</code>. Also note taht callbacks are now strings not method references in <code>Requests(self.parse_item)</code>. Unless <code>callback</code> is set, a <code>Rule</code> will follow the extracted URLs, which means that it will scan target pages for extra links and follow them. If a <code>callback</code>, <code>Rule</code> will NOT follow the links from target pages. If need to follow links, either <code>return/yield</code> them from <code>callback</code> method, or set <code>follow=true</code> in <code>Rule()</code>. This might be useful when listing pages contain both ITEMs and extra useful navigation links!!</p>
</blockquote>

<h1 id="from-scrapy-to-a-mobile-app">From Scrapy to a MOBILE APP</h1>

<h3 id="choosing-a-mobile-app-framework">Choosing a Mobile App Framework</h3>

<p>Feeding data scraped to app is easy if using appropriate tools. Many frameworks such as PhoneGap, Appcelerator, jQuery Mobile, Sencha Touch.</p>

<p>This tutorial uses Appery.io for its iOS, Android, Windows Phone and HTML5 compatibility and ease of use using PhoneGap and jQuery Mobile. Its paid service bundles both mobile and backend services, meaning no need to configure DB, write REST APIs or use perhaps other langues to write them.</p>

<p>Detail see <strong>Learning Scrapy</strong> on GitHub source code.</p>

<h1 id="quick-spider-recipes">Quick Spider Recipes</h1>

<p>Previously on extracting info from pages and stored as Items. This is the 80% of the use case, and this section covers special usage to focus on 2 two important classes <code>Request</code> and <code>Response</code>, the two Rs in the process model.</p>

<h3 id="a-spider-that-logs-in">A spider that logs in</h3>

<p>When website having login mechanism, the two Rs are key to extract data via inspecting Network traffic in dev tool.</p>

<p>Once login correctly, <strong>Request Method: POST</strong> appears in network request. Inspect data including <strong>Form Data</strong>, <strong>Cookie</strong> stores the login detail set under <strong>Request Headers</strong>. Thus a single operation, such as login, may involve several server round-trips, including POST, HTTP redirects 302, etc. Scrapy handles most of these operations automatically, with simple code needed. Inherit from CrawlSpider, define a new spider: ``<code>class LoginSpider(CrawlSpider): name='login'</code></p>

<p><code>FormRequest</code> class send initial request that logs in by performing POST request, similar to <code>Request</code> with extra <code>formadata</code> argument to pass data (i.e. <code>user</code> and <code>pass</code>)</p>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-python" data-lang="python"><span style="color:#f92672">import</span> datetime
<span style="color:#f92672">import</span> urlparse
<span style="color:#f92672">import</span> socket

<span style="color:#f92672">from</span> scrapy.loader.processors <span style="color:#f92672">import</span> MapCompose, Join
<span style="color:#f92672">from</span> scrapy.linkextractors <span style="color:#f92672">import</span> LinkExtractor
<span style="color:#f92672">from</span> scrapy.spiders <span style="color:#f92672">import</span> CrawlSpider, Rule
<span style="color:#f92672">from</span> scrapy.loader <span style="color:#f92672">import</span> ItemLoader
<span style="color:#f92672">from</span> scrapy.http <span style="color:#f92672">import</span> FormRequest

<span style="color:#f92672">from</span> properties.items <span style="color:#f92672">import</span> PropertiesItem


<span style="color:#66d9ef">class</span> <span style="color:#a6e22e">LoginSpider</span>(CrawlSpider):
    name <span style="color:#f92672">=</span> <span style="color:#e6db74">&#39;login&#39;</span>
    allowed_domains <span style="color:#f92672">=</span> [<span style="color:#e6db74">&#34;web&#34;</span>]

    <span style="color:#75715e"># Start with a login request</span>
    <span style="color:#66d9ef">def</span> <span style="color:#a6e22e">start_requests</span>(self):
        <span style="color:#66d9ef">return</span> [
            FormRequest(
                <span style="color:#e6db74">&#34;http://web:9312/dynamic/login&#34;</span>,
                formdata<span style="color:#f92672">=</span>{<span style="color:#e6db74">&#34;user&#34;</span>: <span style="color:#e6db74">&#34;user&#34;</span>, <span style="color:#e6db74">&#34;pass&#34;</span>: <span style="color:#e6db74">&#34;pass&#34;</span>}
            )]

    <span style="color:#75715e"># Rules for horizontal and vertical crawling</span>
    rules <span style="color:#f92672">=</span> (
        Rule(LinkExtractor(restrict_xpaths<span style="color:#f92672">=</span><span style="color:#e6db74">&#39;//*[contains(@class,&#34;next&#34;)]&#39;</span>)),
        Rule(LinkExtractor(restrict_xpaths<span style="color:#f92672">=</span><span style="color:#e6db74">&#39;//*[@itemprop=&#34;url&#34;]&#39;</span>),
             callback<span style="color:#f92672">=</span><span style="color:#e6db74">&#39;parse_item&#39;</span>)
    )

    <span style="color:#66d9ef">def</span> <span style="color:#a6e22e">parse_item</span>(self, response):
        <span style="color:#75715e"># stay the same</span></code></pre></div>
<p>That&rsquo;s it really. The default <code>parse()</code> of <code>CrawlSpider</code> handles <code>Response</code> and uses <code>Rules</code> exactly as previously. So little code since Scrapy handles cookies transparently for us, as soon as login, it passes them on to subsequent requests in exactly the same manner as a browser.</p>

<p>Naturally some login mechanism is more complex, such as a <strong>HIDDEN</strong> value, which need be POST together. This means two requests! Visit the form page and then the login page, then pass through some data. A new spider now in <code>start_requests()</code> return a simple <code>Request</code> to our form page, and will manually handle the ersponse by setting its <code>callback</code> to hour handler method named <code>parse_welcome()</code> below. In it, use the helper <code>from_response()</code> method of <code>FormRequest</code> object to create <code>FormRequest</code> that is pre-populated with all the fields and values from the original form. <code>FormRequest.from_response()</code> roughly emulates a submit click on the first form on the page with all the fields left blank.</p>

<p>TIP: worth familiarise with documentaion of <code>from_response()</code> for its many features like <code>formname</code> and <code>formnumber</code> designed to help select the form desired if multiple occur.</p>

<p>This effortless feature use <code>formdata</code> argument to fill in the user and pass fields and return <code>FormRequest</code>:</p>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-python" data-lang="python"><span style="color:#f92672">import</span> datetime
<span style="color:#f92672">import</span> urlparse
<span style="color:#f92672">import</span> socket

<span style="color:#f92672">from</span> scrapy.loader.processors <span style="color:#f92672">import</span> MapCompose, Join
<span style="color:#f92672">from</span> scrapy.linkextractors <span style="color:#f92672">import</span> LinkExtractor
<span style="color:#f92672">from</span> scrapy.spiders <span style="color:#f92672">import</span> CrawlSpider, Rule
<span style="color:#f92672">from</span> scrapy.loader <span style="color:#f92672">import</span> ItemLoader
<span style="color:#f92672">from</span> scrapy.http <span style="color:#f92672">import</span> Request, FormRequest

<span style="color:#f92672">from</span> properties.items <span style="color:#f92672">import</span> PropertiesItem


<span style="color:#66d9ef">class</span> <span style="color:#a6e22e">NonceLoginSpider</span>(CrawlSpider):
    name <span style="color:#f92672">=</span> <span style="color:#e6db74">&#39;noncelogin&#39;</span>
    allowed_domains <span style="color:#f92672">=</span> [<span style="color:#e6db74">&#34;web&#34;</span>]

    <span style="color:#75715e"># Start on the welcome page</span>
    <span style="color:#66d9ef">def</span> <span style="color:#a6e22e">start_requests</span>(self):
        <span style="color:#66d9ef">return</span> [
            Request(
                <span style="color:#e6db74">&#34;http://web:9312/dynamic/nonce&#34;</span>,
                callback<span style="color:#f92672">=</span>self<span style="color:#f92672">.</span>parse_welcome)
        ]

    <span style="color:#75715e"># Post welcome page&#39;s first form with the given user/pass</span>
    <span style="color:#66d9ef">def</span> <span style="color:#a6e22e">parse_welcome</span>(self, response):
        <span style="color:#66d9ef">return</span> FormRequest<span style="color:#f92672">.</span>from_response(
            response,
            formdata<span style="color:#f92672">=</span>{<span style="color:#e6db74">&#34;user&#34;</span>: <span style="color:#e6db74">&#34;user&#34;</span>, <span style="color:#e6db74">&#34;pass&#34;</span>: <span style="color:#e6db74">&#34;pass&#34;</span>}
        )</code></pre></div>
<p>When run in Shell, observes first GET to /hidden_login page, and then POST, folllowed by redirection on to /hidden_login_success page that leads to /gated as before.</p>

<h3 id="a-spider-that-uses-json-apis-and-ajax-pages">A spider that uses JSON APIs and AJAX pages</h3>

<p>Hidden elements managed by JSON objects dynamically. Similarly, inspect via Network traffic, often in the form of <strong>api.json</strong>. More complex APIs may require login, POST, or return more interesting data structures. At any rate, JSON is one of the easiest formats to parse as no need to write any XPATH to extract.</p>

<p>Python provides a great JSON parsing module - <code>import json</code> -&gt; <code>json.loads(response.body)</code> to parse JSON and convert it to an equal object consisting of Python primitives, lists, and dicts.</p>

<p>Once found, make spider that works just on it.</p>

<p><code>start_urls = ('http://someurl/api.json')</code></p>

<p>More complex need can be done using previous mechanism. At this point, Scrapy will open this URL and call <code>parse()</code> with <code>Response</code> as argument.</p>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-python" data-lang="python"><span style="color:#f92672">import</span> urlparse
<span style="color:#f92672">import</span> socket
<span style="color:#f92672">import</span> scrapy
<span style="color:#f92672">import</span> json

<span style="color:#66d9ef">class</span> <span style="color:#a6e22e">ApiSpider</span>(scrapy<span style="color:#f92672">.</span>Spider):
    name <span style="color:#f92672">=</span> <span style="color:#e6db74">&#39;api&#39;</span>
    allowed_domains <span style="color:#f92672">=</span> [<span style="color:#e6db74">&#34;web&#34;</span>]

    <span style="color:#75715e"># Start on the first index page</span>
    start_urls <span style="color:#f92672">=</span> (
        <span style="color:#e6db74">&#39;http://web:9312/properties/api.json&#39;</span>,
    )

    <span style="color:#75715e"># Format the URLs based on the API call response</span>
    <span style="color:#66d9ef">def</span> <span style="color:#a6e22e">parse</span>(self, response):
        base_url <span style="color:#f92672">=</span> <span style="color:#e6db74">&#34;http://web:9312/properties/&#34;</span>
        js <span style="color:#f92672">=</span> json<span style="color:#f92672">.</span>loads(response<span style="color:#f92672">.</span>body)
        <span style="color:#66d9ef">for</span> item <span style="color:#f92672">in</span> js:
            id <span style="color:#f92672">=</span> item[<span style="color:#e6db74">&#34;id&#34;</span>]
            title <span style="color:#f92672">=</span> item[<span style="color:#e6db74">&#34;title&#34;</span>]
            url <span style="color:#f92672">=</span> base_url <span style="color:#f92672">+</span> <span style="color:#e6db74">&#34;property_</span><span style="color:#e6db74">%06d</span><span style="color:#e6db74">.html&#34;</span> <span style="color:#f92672">%</span> id
            <span style="color:#66d9ef">yield</span> Request(url, meta<span style="color:#f92672">=</span>{<span style="color:#e6db74">&#34;title&#34;</span>: title}, callback<span style="color:#f92672">=</span>self<span style="color:#f92672">.</span>parse_item)</code></pre></div>
<p><code>%06d</code> is a very useful piece of Python syntax for creating new strings by combining Python variables. <code>%d</code> means treat as digit and extends to 6 characters by prepending 0s if necessary. If <code>id</code> has the value 5, it will be repalced with 000005, else if 34322 then 034322. <code>yield</code> new <code>Request</code> of correctly joined URL with callback.</p>

<h3 id="passing-arguments-between-responses">Passing arguments between responses</h3>

<p>If info on JSON APIs need be stored to ITEM, how to pass it from <code>parse()</code> to <code>parse_item()</code> method?</p>

<p><strong>meta</strong> data as dict inside <code>Request()</code> used for this purpose, and the index page info. For example, let&rsquo;s set a title value on this dict to store the title from JSON object: <code>title = item[&quot;title&quot;]</code></p>

<p><code>yield Request(url, meta = {&quot;title&quot;: title}, callback = self.parse_item)</code></p>

<p>Inside <code>parse_item()</code>, we can use this value instead of XPath expression before:</p>

<p><code>l.add_value('title', response.meta['title'], MapCompose(unicode.strip, unicode.title))</code></p>

<p>Notice the switch from calling <code>add_xpath()</code> to <code>add_value()</code> when using <strong>meta</strong></p>

<h3 id="30x-faster-spider">30x Faster Spider</h3>

<p>Avoid scraping every single listing page if able to extract about the same info from index page !!</p>

<p>TIP: If a website gives 10, 50 or 100 listing pages per index page by tuning a param, such as <code>&amp;show=50</code> on URL, set to maximum before horizontal crawling.</p>

<p>A programming design decision here, since most website set <strong>throttle</strong> requests.</p>

<p>Demo of such mechanism:</p>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-python" data-lang="python"><span style="color:#66d9ef">def</span> <span style="color:#a6e22e">parse</span>(self, response):
    <span style="color:#75715e"># Get next index URLs and yield Requests</span>
    <span style="color:#75715e"># same as before</span>
    
    <span style="color:#75715e"># Iterate through products and create PropertiesItems</span>
    selectors <span style="color:#f92672">=</span> response<span style="color:#f92672">.</span>xpath(
    	<span style="color:#e6db74">&#39;//*[@itemtype = &#34;http://scheme.org/Product&#34;]&#39;</span>)
    <span style="color:#75715e"># differs in yielding each of 30 product from selectors and parse_item them</span>
    <span style="color:#66d9ef">for</span> selector <span style="color:#f92672">in</span> selectors:
        <span style="color:#66d9ef">yield</span> self<span style="color:#f92672">.</span>parse_item(selector, response)

<span style="color:#66d9ef">def</span> <span style="color:#a6e22e">parse_item</span>(self, selector, response):
    <span style="color:#75715e"># Create the laoder using the selector</span>
    l <span style="color:#f92672">=</span> ItemLoader(item <span style="color:#f92672">=</span> PropertiesItem(), selector <span style="color:#f92672">=</span> selector)
    
    <span style="color:#75715e"># Load fields using XPath</span>
    <span style="color:#75715e"># NOTE!! relative CURRENT XPath selector &#39;.&#39; for each expression!</span>
    l<span style="color:#f92672">.</span>add_xpath(<span style="color:#e6db74">&#39;title&#39;</span>, <span style="color:#e6db74">&#39;.//*[@itemprop=&#34;name&#34;][1]/text()&#39;</span>,
    	MapCompose(unicode<span style="color:#f92672">.</span>strip, unicode<span style="color:#f92672">.</span>title))
    <span style="color:#75715e"># etc</span>
    make_url <span style="color:#f92672">=</span> <span style="color:#66d9ef">lambda</span> i : urlparse<span style="color:#f92672">.</span>urljoin(response<span style="color:#f92672">.</span>url, i)
    l<span style="color:#f92672">.</span>add_xpath(<span style="color:#e6db74">&#39;image_urls&#39;</span>, <span style="color:#e6db74">&#39;.//*[@itemprop=&#34;image&#34;][1]/@src&#39;</span>,
               MapCompose(make_url))
    <span style="color:#75715e"># Housekeeping mostly the same</span>
    l<span style="color:#f92672">.</span>add_xpath(<span style="color:#e6db74">&#39;image_urls&#39;</span>, <span style="color:#e6db74">&#39;.//*[itemprop=&#34;url&#34;][1]/@href&#39;</span>,
               MapCompose(make_url))
    
    <span style="color:#66d9ef">return</span> l<span style="color:#f92672">.</span>load_item()</code></pre></div>
<p>Slight changes made:</p>

<ul>
<li>ItemLoader now uses selector as source rather than Response. This is a convenient feature of <code>ItemLoader</code> API allowing to extract from currently selected segment instead of entire page.</li>
<li>Path turned to relative by prepending dot (.) TIP: so happens in this case, XPath identical in detail and index pages, but not always the case</li>
<li>Need to compile URL of ITEM manually. Before <code>response.url</code> was giving URL for listing page, now gives URL of index page since this was page crawled. Need to extract URL of listing using XPath and convert it to absolute URL with MapCompose processor</li>
</ul>

<h3 id="spider-crawling-based-on-excel-file">Spider crawling based on Excel file</h3>

<p>In case where scrape data from many sites with ONLY XPath changes, overkill to have a spider for every site. How to use a single spider?</p>

<p>Create a new project <code>generic</code> name spider <code>fromcsv</code>. Create a CSV with fields containing relevant URL, items to extract (XPATH), save in project root directory.</p>

<p>Read the CSV into Dict:</p>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-python" data-lang="python"><span style="color:#f92672">import</span> csv

<span style="color:#66d9ef">with</span> open(<span style="color:#e6db74">&#34;data.csv&#34;</span>, <span style="color:#e6db74">&#34;rU&#34;</span>) <span style="color:#66d9ef">as</span> f:
    reader <span style="color:#f92672">=</span> csv<span style="color:#f92672">.</span>DictReader(f)
    <span style="color:#66d9ef">for</span> line <span style="color:#f92672">in</span> reader:
        <span style="color:#66d9ef">print</span>(line)</code></pre></div>
<p>Modification to spider:</p>

<ul>
<li>remove <code>start_urls</code> and <code>allows_domains</code></li>
<li>use <code>start_requests()</code> and <code>Request</code> each row of data</li>

<li><p>Store field names and XPATH from CSV in <code>request.meta</code> to use in <code>parse()</code></p>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-python" data-lang="python"><span style="color:#f92672">import</span> csv

<span style="color:#f92672">import</span> scrapy
<span style="color:#f92672">from</span> scrapy.http <span style="color:#f92672">import</span> Request
<span style="color:#f92672">from</span> scrapy.loader <span style="color:#f92672">import</span> ItemLoader
<span style="color:#f92672">from</span> scrapy.item <span style="color:#f92672">import</span> Item, Field


<span style="color:#66d9ef">class</span> <span style="color:#a6e22e">FromcsvSpider</span>(scrapy<span style="color:#f92672">.</span>Spider):
name <span style="color:#f92672">=</span> <span style="color:#e6db74">&#34;fromcsv&#34;</span>

<span style="color:#66d9ef">def</span> <span style="color:#a6e22e">start_requests</span>(self):
    <span style="color:#66d9ef">with</span> open(getattr(self, <span style="color:#e6db74">&#34;file&#34;</span>, <span style="color:#e6db74">&#34;todo.csv&#34;</span>), <span style="color:#e6db74">&#34;rU&#34;</span>) <span style="color:#66d9ef">as</span> f:
        reader <span style="color:#f92672">=</span> csv<span style="color:#f92672">.</span>DictReader(f)
        <span style="color:#66d9ef">for</span> line <span style="color:#f92672">in</span> reader:
            request <span style="color:#f92672">=</span> Request(line<span style="color:#f92672">.</span>pop(<span style="color:#e6db74">&#39;url&#39;</span>))
            request<span style="color:#f92672">.</span>meta[<span style="color:#e6db74">&#39;fields&#39;</span>] <span style="color:#f92672">=</span> line
            <span style="color:#66d9ef">yield</span> request

<span style="color:#66d9ef">def</span> <span style="color:#a6e22e">parse</span>(self, response):
    item <span style="color:#f92672">=</span> Item()
    l <span style="color:#f92672">=</span> ItemLoader(item<span style="color:#f92672">=</span>item, response<span style="color:#f92672">=</span>response)
    <span style="color:#66d9ef">for</span> name, xpath <span style="color:#f92672">in</span> response<span style="color:#f92672">.</span>meta[<span style="color:#e6db74">&#39;fields&#39;</span>]<span style="color:#f92672">.</span>iteritems():
        <span style="color:#66d9ef">if</span> xpath:
            item<span style="color:#f92672">.</span>fields[name] <span style="color:#f92672">=</span> Field()
            l<span style="color:#f92672">.</span>add_xpath(name, xpath)

    <span style="color:#66d9ef">return</span> l<span style="color:#f92672">.</span>load_item()</code></pre></div></li>
</ul>

<p>Observation:</p>

<ul>
<li>Since no project-wide ITEM, need to provide one to ItemLoader manually inside <code>parse()</code></li>
<li>Fields added dynamically using <code>fields</code> member variable of Item.</li>
<li>Hardcoding data.csv is not good practice, Scrapy gives easy way to pass arguments to spiders.

<ul>
<li><code>-a variable=value</code></li>
<li>a spider property is set and able to retrieve it with <code>self.variable</code></li>
<li>to check for variable and use a default if it isn&rsquo;t provided, use <code>getattr()</code> Python method</li>
<li><code>getattr(self, 'variable', 'default')</code></li>
<li>In sum, replace <code>with open</code> :</li>
<li><code>with open(getattr(self, 'file', 'data.csv'), &quot;rU&quot;) as f:</code></li>
<li>Now CSV is the default value unless overridden by setting a source file explicitly with <code>-a variable=value</code></li>
<li>Given a second file, <code>another_data.csv</code> :</li>
<li><code>scrapy crawl fromcsv -a file=another_data.csv -o output.csv</code></li>
</ul></li>
</ul>

<h1 id="deploying-to-scrapinghub">Deploying to Scrapinghub</h1>

<ol>
<li>+ Service</li>
<li>Scrapy Cloud -&gt; Project Naming -&gt; Create</li>
<li>Open Project -&gt; menu on the left [JOBS, SPIDERS, COLLECTIONS, USAGE, REPORTS, ACTIVITY, PERIODIC JOBS, SETTINGS]</li>
<li>Setting -&gt; Scrapy Deploy -&gt; COPY data into project&rsquo;s <code>scrapy.cfg</code></li>
<li>pip install shub</li>
<li>shub login (with API keys)</li>
<li>shub deploy -&gt; <code>Run your spiders at https://dash.scrapinghub.com/p/28814/</code></li>
<li>SPIDERS -&gt; spiders uploaded</li>
<li>Schedule -&gt; view all info or Stop</li>
</ol>

<h3 id="programmatic-access-to-scrapinghub-jobs-data">Programmatic Access to Scrapinghub Jobs/Data</h3>

<p>Inspecting URL of jobs and spiders to understand entry points.</p>

<p><code>curl -u &lt;API&gt;: https://storage.scrapinghub.com/items/&lt;project id&gt;/&lt;spider id&gt;/&lt;job id&gt;</code></p>

<p>Leave blank if prompt pass. This allows writing applications/services using Scrapinghub as data storage backend. Mindful of time limit in cloud plan.</p>

<h3 id="scheduling-recurring-crawls">Scheduling Recurring Crawls</h3>

<ol>
<li>PERIODIC JOBS -&gt; Add -&gt; set-up</li>
</ol>

<h1 id="configuration-and-management">Configuration and Management</h1>

<h3 id="settings">Settings</h3>

<p>Source code information on DEFAULT PRIORITY <code>scrapy/settings/default_settings.py</code></p>

<p>Project-level setting tuning is most practical.</p>

<p>Spider-level settings via <code>custom_settings</code> attribute in spider definitions per spider.</p>

<p>Last-minute mod pass Shell cmd <code>-s CLOSESPIDER_PAGECOUNT=3</code></p>

<p>TESTING</p>

<p><code>scrapy settings --get CONCURRENT_REQUESTS -s CONCURRENT_REQUESTS=19</code></p>

<p><code>scrapy shell -s CONCURRENT_REQUESTS=19</code></p>

<h3 id="essential-settings">Essential Settings</h3>

<p>ANALYSIS</p>

<table>
<thead>
<tr>
<th>CODE</th>
<th>DETAIL</th>
</tr>
</thead>

<tbody>
<tr>
<td>(Logging) <code>LOG_LEVEL</code></td>
<td>Various levels of logs based on severity: <code>DEBUG</code> -&gt; <code>INFO</code> -&gt; <code>WARNING</code> -&gt; <code>ERROR</code> -&gt; <code>CRITICAL</code> , this controls threshold of level to display. Often <code>INFO</code> as <code>DEBUG</code> can be verbose.</td>
</tr>

<tr>
<td>(Logging) <code>LOGSTATS_INTERVAL</code></td>
<td>Prints number of times and pages scraped per minute. It sets logging frequency default = 60 seconds. This may be too infrequent, often 5 seconds if short run.</td>
</tr>

<tr>
<td>(Logging) <code>LOG_ENABLED</code></td>
<td></td>
</tr>

<tr>
<td>(Logging) <code>LOG_FILE</code></td>
<td>Where logs are written, unless set, it go to STDERR except if logging gets disabled to False above.</td>
</tr>

<tr>
<td>(Logging) <code>LOG_STDOUT</code></td>
<td>Record all of its STDOUT (e.g. &ldquo;print&rdquo; msg) to log by set True.</td>
</tr>

<tr>
<td>(Stats) <code>STATS_DUMP</code></td>
<td>Enabled as default, it dumps values from Stats Collector to log once spider done.</td>
</tr>

<tr>
<td>(Stats) <code>DOWNLOADER_STATS</code></td>
<td>Control wheter stats are recorded for the downloader.</td>
</tr>

<tr>
<td>(Stats) <code>DEPTH_STATS</code></td>
<td>Control whether stats are collected for site depth.</td>
</tr>

<tr>
<td>(Stats) <code>DEPTH_STATS_VERBOSE</code></td>
<td>Verbose log of above.</td>
</tr>

<tr>
<td>(Stats) <code>STATSMAILER_RCPTS</code></td>
<td>A list (e.g. set to &ldquo;my@gmail.com&rdquo;) of e-mails to send stats to when crawl done.</td>
</tr>

<tr>
<td>(Telnet) <code>TELNETCONSOLE_ENABLED</code></td>
<td>Python shell running process enabled as default</td>
</tr>

<tr>
<td>(Telnet) <code>TELNETCONSOLE_PORT</code></td>
<td>Determines ports used to connect to console. EX1: In case wanting to look on internal  status of Scrapy while running. <code>DEBUG: Telnet console listening on 127.0.0.1:6023:6023</code> means telnet is on and listening in port 6023. Now on another terminal, use telnet command to connect to it: <code>telnet localhost 6023</code> giving a Python console inside Scrapy, for inspecting components like engine using <code>engine</code> variable or <code>est()</code> for quick overview. Very useful when using remote machine: <code>engine.pause() .unpause() .stop()</code></td>
</tr>
</tbody>
</table>

<p>PERFORMANCE</p>

<table>
<thead>
<tr>
<th>Code</th>
<th>Detail</th>
</tr>
</thead>

<tbody>
<tr>
<td><code>CONCURRENT_REQUESTS</code></td>
<td>Maximum number of requests concurrently, mostly protects server&rsquo;s outbound cap.</td>
</tr>

<tr>
<td><code>CONCURRENT_REQUESTS_PER_DOMAIN</code></td>
<td>More restrictive, protects remote servers by limiting numer of concurrent req per unique domain or IP</td>
</tr>

<tr>
<td><code>CONCURRENT_REQUESTS_PER_IP</code></td>
<td>If true, the above is ignored. NOT per second, if 16 and avg req 0.25 a second then limit is <sup>16</sup>&frasl;<sub>0</sub>.25 = 64 req per second.</td>
</tr>

<tr>
<td><code>CONCURRENT_ITEMS</code></td>
<td>Max number of items per response concurrently, per request. if 16 CONCURRENT_REQUESTS and this 100 =&gt; 1600 items concurrently wriing to DB, etc.</td>
</tr>

<tr>
<td><code>DOWNLOAD_TIMEOUT</code></td>
<td>Time waited before canceling request. 180 seconds as default, seemingly excessive, advised reduction to 10 seconds.</td>
</tr>

<tr>
<td><code>DOWNLOAD_DELAY</code></td>
<td>Default to 0, mod to apply conservative download speed using this. A site might use FREQUENCY REQUEST` detect bot.</td>
</tr>

<tr>
<td><code>RANDOMIZE_DOWNLOAD_DELAY</code></td>
<td>If above true, this enabled to +- 50% on delay</td>
</tr>

<tr>
<td><code>DNSCAHCE_ENABLED</code></td>
<td>For faster DNS lookups, an in-memory DNS cache is enabled by default.</td>
</tr>
</tbody>
</table>

<p>CLOSING</p>

<table>
<thead>
<tr>
<th>Code</th>
<th>Detail</th>
</tr>
</thead>

<tbody>
<tr>
<td><code>CLOSESPIDER_ERRORCOUNT</code></td>
<td>Auto-stop when conditions met. Often set while running spider in SHELL for testing</td>
</tr>

<tr>
<td><code>CLOSESPIDER_ITEMCOUNT</code></td>
<td></td>
</tr>

<tr>
<td><code>CLOSESPIDER_PAGECOUNT</code></td>
<td></td>
</tr>

<tr>
<td><code>CLOSESPIDER_TIMEOUT</code></td>
<td>In seconds</td>
</tr>
</tbody>
</table>

<p>HTTP CACHE</p>

<table>
<thead>
<tr>
<th>Code</th>
<th>Detail</th>
</tr>
</thead>

<tbody>
<tr>
<td><code>HTTPCACHE_ENABLED</code></td>
<td>The <code>HttpCacheMiddleware</code> deactivated by default gives low-level cache for HTTP req/res</td>
</tr>

<tr>
<td><code>HTTPCACHE_DIR</code></td>
<td>Relative path to project root</td>
</tr>

<tr>
<td><code>HTTPCACHE_POLICY</code></td>
<td>If = <code>scrapy.contrib.httpcache.RFC2616Policy</code> enables way more sophy caching policy respecting sits hints according to RFC2616. (above two also True)</td>
</tr>

<tr>
<td><code>HTTPCACHE_STORAGE</code></td>
<td><code>scrapy.contrib.httpcache.DbmCacheStorage</code></td>
</tr>

<tr>
<td><code>HTTPCACHE_DBM_MODULE</code></td>
<td>Adjusting (defaults to anydbm)</td>
</tr>

<tr>
<td><code>HTTPCACHE_EXPIRATION_SECS</code></td>
<td></td>
</tr>

<tr>
<td><code>HTTPCACHE_IGNORE_HTTP_CODES</code></td>
<td></td>
</tr>

<tr>
<td><code>HTTPCACHE_IGNORE_MISSING</code></td>
<td></td>
</tr>

<tr>
<td><code>HTTPCACHE_IGNORE_SCHEMES</code></td>
<td></td>
</tr>

<tr>
<td><code>HTTPCACHE_GZIP</code></td>
<td></td>
</tr>
</tbody>
</table>

<p>CRAWLING STYLE</p>

<table>
<thead>
<tr>
<th>Code</th>
<th>Detail</th>
</tr>
</thead>

<tbody>
<tr>
<td><code>DEPTH_LIMIT</code></td>
<td>Max depth 0 meaning no limit.</td>
</tr>

<tr>
<td><code>DEPTH_PRIORITY</code></td>
<td>This alows BREADTH FIRST Crawl by setting this to positive number changing from LIFO to FIFO: <code>DEPTH_PRIORITY = 1</code> (useful for news site where recent data best use FIFO method with <code>DEPTH_LIMIT = 3</code> might allow quick scan latest news on portal)</td>
</tr>

<tr>
<td><code>SCHEDULER_DISK_QUEUE</code></td>
<td>Following above example: <code>= scrapy.squeue.PickleFileDiskQueue</code></td>
</tr>

<tr>
<td><code>SCHEDULER_MEMORY_QUEUE</code></td>
<td>Following above example: <code>= scrapy.squeue.FifoMemoryQueue</code></td>
</tr>

<tr>
<td><code>ROBOTSTXT_OBEY</code></td>
<td></td>
</tr>

<tr>
<td><code>COOKIES_ENABLED</code></td>
<td><code>CookiesMiddleware</code> takes care of all cookie-wise operations, enabling others to log in etc. If prefer more &lsquo;stealth&rsquo; crawling, disable this.</td>
</tr>

<tr>
<td><code>REFERER_ENABLED</code></td>
<td>Default to True enabling populating Referer headers, defined with <code>DEFAULT_REQUEST_HEADERS</code> useful for weird sites banning unless showing particular request headers !!</td>
</tr>

<tr>
<td><code>USER_AGENT</code></td>
<td></td>
</tr>

<tr>
<td><code>DEFAULT_REQUEST_HEADERS</code></td>
<td>Set along with Referer Headers above.</td>
</tr>
</tbody>
</table>

<p>FEEDS</p>

<table>
<thead>
<tr>
<th>CODE</th>
<th>DETAIL</th>
</tr>
</thead>

<tbody>
<tr>
<td><code>FEED_URI</code></td>
<td><code>scrapy crawl fast -o &quot;%(name)s_%(time)s.jl&quot;</code> will auto-name output file. Custom variable defined in spider also allowed <code>%(foo)s</code> if foo defined. This is also set for S3, FTP. (e.g. <code>=s3://mybucket/file.json</code> along with AWS settings below)</td>
</tr>

<tr>
<td><code>FEED_FORMAT</code></td>
<td>Auto-assigned based on URI extension, or set here.</td>
</tr>

<tr>
<td><code>FEED_STORE_EMPTY</code></td>
<td>Bool for empty feed.</td>
</tr>

<tr>
<td><code>FEED_EXPORT_FILEDS</code></td>
<td>Filter esp. CSV with fixed header columns if need.</td>
</tr>

<tr>
<td><code>FEED_URI_PARAMS</code></td>
<td>Define function to postprocess any parmas to URI</td>
</tr>
</tbody>
</table>

<p>MEDIA DOWNLOAD</p>

<table>
<thead>
<tr>
<th>CODE</th>
<th>DETAIL</th>
</tr>
</thead>

<tbody>
<tr>
<td><code>IMAGES_STORE</code></td>
<td>Directory stored (project root relative path) URLs for images for each ITEM should be in its <code>image_url</code> FIELD (can be overridden by <code>IMAGES_URLS_FIELD</code>)</td>
</tr>

<tr>
<td><code>IMAGES_EXPIRES</code></td>
<td></td>
</tr>

<tr>
<td><code>IMAGES_THUMBS</code></td>
<td>E.g. one icon-sized and one medium size per image</td>
</tr>

<tr>
<td><code>IMAGES_URLS_FIELD</code></td>
<td></td>
</tr>

<tr>
<td><code>IMAGES_RESULT_FIELD</code></td>
<td>Overrides <code>image</code> FIELD filenames</td>
</tr>

<tr>
<td><code>IMAGES_MIN_HEIGHT</code></td>
<td></td>
</tr>

<tr>
<td><code>IMAGES_MIN_WIDTH</code></td>
<td></td>
</tr>

<tr>
<td><code>FILES_STORE</code></td>
<td>Other media, same style as Image. Both can be set at once.</td>
</tr>

<tr>
<td><code>FILES_EXPIRES</code></td>
<td></td>
</tr>

<tr>
<td><code>FILES_URLS_FIELD</code></td>
<td></td>
</tr>

<tr>
<td><code>FILES_RESULT_FIELD</code></td>
<td></td>
</tr>
</tbody>
</table>

<p>Example - downloading images</p>

<p>To use image functions - <code>pip install image</code> ; to enable IMAGE PIPELINE, edit projects&rsquo; <code>settings.py</code> add below.</p>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-python" data-lang="python">ITEM_PIPELINES <span style="color:#f92672">=</span> {
    <span style="color:#f92672">...</span>
    <span style="color:#e6db74">&#39;scrapy.pipelines.images.ImagesPipeline&#39;</span>: <span style="color:#ae81ff">1</span>,
}
IMAGES_STORE <span style="color:#f92672">=</span> <span style="color:#e6db74">&#39;images&#39;</span>
IMAGES_THUMBS <span style="color:#f92672">=</span> { <span style="color:#e6db74">&#39;small&#39;</span>: (<span style="color:#ae81ff">30</span>, <span style="color:#ae81ff">30</span>) }</code></pre></div>
<p>Already have an <code>image_urls</code> field set to <code>Item</code>, so run :</p>

<p><code>scrapy crawl fast -s CLOSESPIDER_ITEMCOUNT=90</code></p>

<p>AWS</p>

<table>
<thead>
<tr>
<th>CODE</th>
<th>DETAL</th>
</tr>
</thead>

<tbody>
<tr>
<td><code>AWS_ACCESS_KEY_ID</code></td>
<td>Used as: When download URL start with s3:// instead of http:// etc, or s3:// path to store files with media pipelines and store output Item feeds onto s3:// directory</td>
</tr>

<tr>
<td><code>AWS_SECRET_ACCESS_KEY</code></td>
<td></td>
</tr>
</tbody>
</table>

<p>PROXY</p>

<table>
<thead>
<tr>
<th>CODE</th>
<th>DETAIL</th>
</tr>
</thead>

<tbody>
<tr>
<td><code>http_proxy</code></td>
<td><code>HttpProxyMiddleware</code> uses these settings in accordance with Unix&rsquo;s convention, enabled as default.</td>
</tr>

<tr>
<td><code>https_proxy</code></td>
<td></td>
</tr>

<tr>
<td><code>no_proxy</code></td>
<td></td>
</tr>
</tbody>
</table>

<p>Example - Using proxies and Crawlera&rsquo;s clever proxy</p>

<p>DynDNS (or similar service) provides a free online tool to check your current IP, using Shell making a request to checkip.dyndns.org to see:</p>

<p><code>scrapy shell http://checkip.dyndns.org</code></p>

<p>Inside <code>response.body</code> see Current IP Address:</p>

<p>To start proxying requests, exit shell and use <code>export</code> command to set new proxy. Test free proxy by search through HMA&rsquo;s public proxy list (<a href="http://proxylist.hidemyass.com/">http://proxylist.hidemyass.com/</a>), e.g. assuming from lsit a 10.10.1.1 and port 80</p>

<p><code>env | grep http_proxy</code></p>

<p>Should have nothing set, then</p>

<p><code>export http_proxy=http://10.10.1.1:80</code></p>

<p>Rerun Shell will see new IP.  Crawlera is Scrapy official service augmented by smart configurations</p>

<p><code>export http_proxy=myusername:password@proxy.crawlera.com:8010</code></p>

<h3 id="further-settings">Further settings</h3>

<ul>
<li><p><strong>Project</strong></p>

<ul>
<li>housekeeping for specific project, <code>BOT_NAME</code>, <code>SPIDER_MODULES</code>, etc. Might be useful for project productivity. There are also two ENV variables, <code>SCRAPY_SETTINGS_MODULE</code> and <code>SCRAY_PROJECT</code> alowing fine tune like Django project integration. <code>scrapy.cfg</code> also allows adjusting name of settings module.</li>
</ul></li>

<li><p><strong>Extending</strong></p>

<ul>
<li><p>Allowing mod almost every aspect of Scrapy, the KING of them is <code>ITEM_PIPELINES</code> which allows use of Item Processing PIpelines. <code>COMMANDS_MODULE</code> allows adding custom commands, e.g. assuming added a <code>properties/hi.py</code> with</p></li>

<li><div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-python" data-lang="python"><span style="color:#f92672">from</span> scrapy.commands <span style="color:#f92672">import</span> ScrapyCommand
      
<span style="color:#66d9ef">class</span> <span style="color:#a6e22e">Command</span>(ScrapyCommand):
  default_settings <span style="color:#f92672">=</span> {<span style="color:#e6db74">&#39;LOG_ENABLED&#39;</span>: False}
  <span style="color:#66d9ef">def</span> <span style="color:#a6e22e">run</span>(self, args, opts):
      <span style="color:#66d9ef">print</span>(<span style="color:#e6db74">&#34;hello&#34;</span>)
              
<span style="color:#75715e"># Inside settings.py</span>
COMMANDS_MODULE <span style="color:#f92672">=</span> <span style="color:#e6db74">&#39;properties.hi&#39;</span></code></pre></div></li>

<li><p>Soon as adding <code>COMMANDS_MODULE='properties.hi'</code> in <code>settings.py</code>, it activates this command showing up in help and run with <code>scrapy hi</code>. The settings defined in <code>default_settings</code> get merged into a project&rsquo;s settings overriding defaults but with lower priority to that defined in <code>settings.py</code> or in Shell CLI line.</p></li>

<li><p>Scrapy uses <code>_BASE</code> dictionaries (e.g. <code>FEED_EXPORTERD_BASE</code>) to store default values for various framework extensions and then allows customising in <code>settings.py</code> and CLI by setting their non-<code>_BASE</code> version (e.g. <code>FEED_EXPORTERS</code>)</p></li>

<li><p><code>DOWNLOADERS</code> or <code>SCHEULER</code> which hold package/class names for essential parts of system, potentially to be inherited from default downloader (<code>scrapy.core.downloader.Downloader</code>), overload a few methods, then set custom class on <code>DOWNLOADER</code> setting - allowing experimenting features and eases automated testing, if good comprehension.</p></li>

<li><p><strong>Downloading</strong></p></li>

<li><p><code>RETRY_*, REDIRECT_*, METAREFRESH_*</code> configure the Retry, Redirect and Meta-Refresh middlewares. E.g. <code>REDIRECT_PRIORITY_ADJUST = 2</code> MEANS PER REDIRECT, NEW REQUEST WILL BE SCHEDULED AFTER all non-redirected requests get served, and <code>REDIRECT_MAX_TIMES = 20</code> means after 20 redirects the downloader will give up and return whatever done. Be aware of these settings in case crawling some ill-cased sites, but default values will serve mostly. Same applies to <code>HTTPERROR_ALLOWED_CODES, URLLENGTH_LIMIT</code></p></li>

<li><p><strong>Autothrottle</strong></p></li>

<li><p><code>AUTOTHROTTLE_*</code> configures itself. But in practice, it tends to be somewhat conservative and difficult to tune, as it suses download latencies to gauge how loaded and target server are and adjusts delay accordinly. If having hard finding best value for <code>DOWNLOAD_DELAY</code> (default = 0), should find this module useful.</p></li>

<li><p><strong>Memory</strong></p></li>

<li><p><code>MEMUSAGE_*</code> enables and configures memory which shuts down spider if exceeding limit, useful in shared ENV where processes obey resources; more often, it&rsquo;s useful to receive just its warning e-mail by disabling the shut down by <code>MENUSAGE_LIMIT_MB = 0</code>. (ONLY APPLICABLE IN UNIX-LIKE OS)</p></li>

<li><p><code>MEMDEBUG_ENABLED, MEMDEBUG_NOTIFY</code> configures debugger printing number of live references on spider close, overall, chasing memory leaks is NOT fun or easy, most importantly keeping crawls relatively short, batched, and accords with server&rsquo;s capacity. (E.g. no good reason to run batches of more than a few thousands pages or more than a few minutes long)</p></li>

<li><p><strong>Logging and Debugging</strong></p></li>

<li><p><code>LOG_ENCODING, LOG_DATEFORMAT, LOG_FORMAT</code> fine tune logging and useful in using log-management solutions, like Splunk, Logstash. <code>DUPEFILTER_DEBUG, COOKIE_DEBUG</code> help debug relatively complex cases where exit unsual requests or lost sessions.</p></li>
</ul>

<h1 id="programming-scrapy">Programming Scrapy</h1>

<p>Up to here, spiders wrote have main task in defining ways crawling and extracting. Beyond Spiders, Scrapy gives mechanisms allowing fine-tune most aspects of its functionalities, such as facing:</p>

<ol>
<li>Copy and paste lots of code among spiders of same project. Repeated code is more related to data (performing calculations on fields) rather than data sources</li>
<li>Having to write scripts postprocessing ITEM to drop duplicate entries or calculating values</li>
<li>Having repeated code across projects to deal with infrastructur. E.g. need to log in and transfer files to proprietary repositories, add ITEM to DB, or trigger postprocessing operations when crawls complete</li>
</ol>

<p>Scrapy developers designed its architecture in ways allowing customisation, such as engine powering Scrapy <strong>TWISTED</strong></p>

<h3 id="scrapy-is-a-twisted-application">SCRAPY IS A TWISTED APPLICATION</h3>

<p>Twisted Python Framework is unusual becuase it&rsquo;s event-driven and encourages writing asynchronous code.</p>

<p>DO NOT write BLOCK code:</p>

<ul>
<li>Code that accesses files, databases or the Web</li>
<li>Code that spawns new processes and consumes their output, like running shell CLI</li>
<li>Code that performs hacky system-level operations, like waiting for system queues</li>
</ul>

<p>Twisted gives methods allowing performing all these and more without blocking code execution.</p>

<blockquote>
<p>Imaging a typical synchronous scrapping application having 4 threads and, at any moment, 3 of them are blocked waiting for responses, 1 of them blocked performing a database write access to persist and ITEM. At any moment, it&rsquo;s quite unlikely to find a general-purpose thread of a scrapping app doing anything else but waiting for some blocking to pass. When blocking passes, some computations may take place for a few microseconds and then threads block again on other blocking ops likely lasting a few ms. Overall the server is not idle as it runs tens of apps utilising thousands of threads, thus after some careful tuning, CPUs remain reasonably utilised.</p>

<p>MULTI-THREADING (4 threads)</p>

<ul>
<li>Thread 1: blocked on web request #330</li>
<li>Thread 2: blocked on database access #79</li>
<li>Thread 3: blocked on web request #330</li>
<li>Thread 4: blocked on web request #312</li>
</ul>

<p>TWISTED (1 thread)</p>

<ul>
<li>Thread 1: blocked waiting for any of the resources to free up

<ul>
<li>Hanging : R329, D79, R330, R312, F32, &hellip; 1000&rsquo;s more&hellip;</li>
</ul></li>
</ul>

<p>Twisted approach favours using a single thread as possible, using modern OS I/O multiplexing functions (<code>select(), poll(), epoll()</code>) as HANGER, returns at once. BUT not the actual value but a hook, i.e. <code>deferred = i_dont_block()</code>, where hang whatever functionality wishign to run whenever value becomes free. Twisted application is made of chains of such deferred ops. Since single-threaded, no suffering costs of context switches and save resources (like memory) that extra threads require. Autrement dit, using this nonblocking infrastructure, gets similar performance if having thousands of threads.</p>

<p>OS developers have been optimising thread ops for decades to make fast. The performance arguments is not as strong, but certainly writing correct thread-safe code for complex apps very hard. Mind framework change in thinking in deferred/callback, Twisted code significantly simpler than threaded code. <code>inlineCallbacks</code> generator utility makes code even simpler.</p>

<p>NOTE: arguably, the most successful nonblocking I/O system until now is Node.js, mainly for its high performance/concurrency. Every Node.js app uses just nonblocking APIs.</p>
</blockquote>

<h3 id="deferreds-and-deferred-chains">DEFERREDS AND DEFERRED CHAINS</h3>

<p>Deferreds are most essential mechanism Twisted offers to help write asynchronous code. APIs use deferreds to allow definig sequences of actions to be called when certain evens occur.</p>

<p>```python</p></li>
</ul>

<p>from twisted.internet import defer</p>

<p>d = defer.Deferred()
d.called</p>

<h1 id="false">False</h1>

<p>d.callback(3)
d.called</p>

<h1 id="true">True</h1>

<p>d.result</p>

<h1 id="3">3</h1>

<pre><code>
See that Deferred is at core a thing representing a value that hangs, when fire d called it's called state becomes True, result attribute is set to value set on callback.

</code></pre>

<p>python
d = defer.Deferred()
def foo(v):
    print(&ldquo;foo called&rdquo;)
    return v+1</p>

<p>d.addCallback(foo)
d.called</p>

<h1 id="flase">Flase</h1>

<p>d.callback(3)</p>

<h1 id="foo-called">foo called</h1>

<p>d.called</p>

<h1 id="true-1">True</h1>

<p>d.result</p>

<h1 id="4">4</h1>

<pre><code>
The most powerful feature of deferred is that we can chain other ops to be called when a value is set. Add a `foo()` func as callabck of d.

### Understanding Twisted and nonblocking I/O

</code></pre>

<h1 id="twisted-a-python-tale">~<em>~ Twisted - A Python tale ~</em>~</h1>

<p>from time import sleep</p>

<h1 id="hello-i-m-a-developer-and-i-mainly-setup-wordpress">Hello, I&rsquo;m a developer and I mainly setup Wordpress.</h1>

<p>def install_wordpress(customer):
	# Our hosting company Threads Ltd. is bad. I start installation and &hellip;
	print(&ldquo;Start installation for&rdquo;, customer)
	# &hellip;then wait till the installation finishes successfully. It is
	# boring and I&rsquo;m speding most of my time waiting while consuming
	# resources (RAM and CPU cycles). It&rsquo;s because the process is BLOCKING
	sleep(3)
	print(&ldquo;All done for&rdquo;, customer)</p>

<h1 id="i-do-this-all-day-for-our-customers">I do this all day for our customers</h1>

<p>def developer_day(customers):
	for customer in customers:
		install_wordpress(customer)</p>

<p>developer_day( [&ldquo;Bill&rdquo;, &ldquo;Elon&rdquo;, &ldquo;Steve&rdquo;, &ldquo;Mark&rdquo;])</p>

<h1 id="let-s-run-it">Let&rsquo;s run it</h1>

<p>$ ./deferreds.py 1
&hellip;
* Elasped time: 12.03 seconds</p>

<pre><code>
What gotten is a sequential execution. 4 customers with 3 seconds processing each means 12 overall. Doesn't scale well.

</code></pre>

<p>python
from twisted.internet import reactor, defer, task</p>

<h1 id="twisted-has-a-slightly-different-approach">Twisted has a slightly different approach</h1>

<p>def schedule_install(customer):
    def schedule_install_wordpress():
        def on_done():
            print(&ldquo;Callback: Finished installation for&rdquo;, customer)
        print(&ldquo;Scheduling: Installation for&rdquo;, customer)
        return task.deferLater(reactor, 3, on_done)
	def all<em>done(</em>):
        print(&ldquo;All done for &ldquo;, customer)</p>

<pre><code># For each customer, schedule these processes on the CRM and that is all has to do
d = schedule_install_wordpress()
d.addCallback(all_done)
return d
</code></pre>

<p>def twisted_developer_day(customers):
    work = [schedule_install(customer) for customer in customers]
    join = defer.DeferredList(work)
    join.addCallback(lambda _ : reactor.stop())</p>

<p>twisted_developer_day( [&ldquo;Customer %d&rdquo; % i for i in xrange(15)])</p>

<p>reactor.run()</p>

<pre><code>
This processes all 15 customers in parallel, 45 seconds computation in just three seconds! The trick is replacing all blocking calls to sleep() with its Twisted counterpart `task.deferLater()` and callback. 

Guide to programming scrapy:

| Problem                                                      | Solution                      |
| ------------------------------------------------------------ | ----------------------------- |
| Specific to website crawled                                  | Mod Spider                    |
| Mod or storing ITEM - domain-specific, may be reused across projects | Write an Item Pipeline        |
| Mod or dropping Request/Reponse - domain-specific, mmay be reused across projects | Write a spider middleware     |
| Executing Requests/Responses - generic, like to support some custom login scheme or a special way to handle cookies | Write a downloader middleware |
| All other problems                                           | Write an extension            |



### Example 1 - a very simple pipeline

Problem: Lots of spiders, but database need string format for indexing, changing individual spiders too much code.

Write a postprocess item pipeline:

</code></pre>

<p>python
from datetime import datetime</p>

<p>class TidyUp(object):
    def process_item(self, item, spider):
        item[&lsquo;date&rsquo;] = map(datetime.isoformat, item[&lsquo;date&rsquo;])
        return item</p>

<pre><code>
Simple class with `process_item()` method. Add it in `tidyup.py` insdie `pipelines` directory.

NOTE: The placement of code is free, but a separate directory is a good idea.

Now edit `settings.py` and set

</code></pre>

<p>ITEM_PIPELINES = { &lsquo;properties.pipelines.tidyup.TidyUp&rsquo; : 100 }`</p>

<p>The number 100 on dict defines the order in which pipelines are connected. If another pipeline has a smaller number, it will process ITEM prior to this pipeline.</p>

<p>The resulting <code>date</code> data will be <code>['2015-11-08T14:47:04.148232']</code> as ISO string.</p>

<h3 id="signals">Signals</h3>

<p>Mechanism to add callbacks to events happening in system, such as when a spider opens, or when an item gets scraped. Hook to them using <code>crawler.signals.connect()</code> (see below example). There&rsquo;re just 11 of them and maybe the easiest way to understand in action. Below is a project having an extension hooking to all signals. Plus a Item Pipeline, one Downloader and one spider middleware, logging every method invocation.</p>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-python" data-lang="python"><span style="color:#66d9ef">def</span> <span style="color:#a6e22e">parse</span>(self, response):
    <span style="color:#66d9ef">for</span> i <span style="color:#f92672">in</span> range(<span style="color:#ae81ff">2</span>):
        item <span style="color:#f92672">=</span> HooksasyncItem()
        item[<span style="color:#e6db74">&#39;name&#39;</span>] <span style="color:#f92672">=</span> <span style="color:#e6db74">&#34;Hello </span><span style="color:#e6db74">%d</span><span style="color:#e6db74">&#34;</span> <span style="color:#f92672">%</span> i
        <span style="color:#66d9ef">yield</span> item
    <span style="color:#66d9ef">raise</span> <span style="color:#a6e22e">Exception</span>(<span style="color:#e6db74">&#34;dead&#34;</span>)</code></pre></div>
<p>On the second ITEM, configured the Item Pipeline to raise a <code>DropItem</code> exception.</p>

<p>This illustrates when certain signals get sent via logs:</p>

<pre><code>$ scrapy crawl test
...many lines....

# First we get those two signals...
INFO: Extension, signals.spider_opened fired
INFO: Extension, signals.engine_started fired

# Then for each URL get a request_scheduled signal
INFO: Extension, singals.request_scheduled fired

# when downlad compltes we get
INFO: Extension, signals.response_downloaded fired
INFO: DownloaderMiddlewareprocess_response called for example.com

# Work between 
INFO: Extension, singals.response_received fired
INFO: SpiderMiddlewareprocess_spider_input called for..

# here our parse() gets called then SpiderMiddleware use
INFO: SpiderMiddlewareprocess_spider_output called for url

# For every Item going through pipelines successfullly...
INFO: Extension, signals.item_scraped fired

# For every item gets dropped using DroptItem exception
INFO: Extension, signals.item_dropped fired

# If your spider throws sth lese..
INFO: Extension, signals.spider_error fired

# ... the above process repeats for each URL
# ... till we run out of them. then..
INFO: Extension, signals.spider_idle fired

# by hooking spider_idle you can shcedule further Requests. if you dont the spider ends
INFO: Closing spider (finished)
INFO: Extension, signals.spider_closed fire

# ....stats get printed and finally engines get stopped
INFO: Extension, singal.sengine_stopped fired

</code></pre>

<p>Only 11 signals, but every scrapy default middleware is implemented using just them, so they must be sufficient. Note every signal except spider_idle, error, request, you can also return deferreds instead of actual values.</p>

<h3 id="example-2-an-extension-measuring-thorugput-and-latencies">Example 2 - an extension measuring thorugput and latencies</h3>

<p>Built-in extension for this the Log Stats extenson (<code>scrapy/extensions/logstats.py</code>) in source code as starting point. To measure latencies, hook the <code>request_scheduled, response_received, item_scraped</code> signals.</p>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-python" data-lang="python"><span style="color:#66d9ef">class</span> <span style="color:#a6e22e">Latencies</span>(object):
    <span style="color:#a6e22e">@classmethod</span>
    <span style="color:#66d9ef">def</span> <span style="color:#a6e22e">from_crawler</span>(cls, crawler):
        <span style="color:#66d9ef">return</span> cls(crawler)
    <span style="color:#66d9ef">def</span> __init__(self, crawler):
        self<span style="color:#f92672">.</span>crawler <span style="color:#f92672">=</span> crawler
        self<span style="color:#f92672">.</span>interval <span style="color:#f92672">=</span> crawler<span style="color:#f92672">.</span>settings<span style="color:#f92672">.</span>getfloat(<span style="color:#e6db74">&#39;LATENCIeS_INTERVAL&#39;</span>)
            <span style="color:#66d9ef">if</span> <span style="color:#f92672">not</span> self<span style="color:#f92672">.</span>interval:
                <span style="color:#66d9ef">raise</span> NotConfigured
        cs <span style="color:#f92672">=</span> crawler<span style="color:#f92672">.</span>signals
        cs<span style="color:#f92672">.</span>connect(self<span style="color:#f92672">.</span>_spider_opened, signal<span style="color:#f92672">=</span>signals<span style="color:#f92672">.</span>spider_opened)
        cs<span style="color:#f92672">.</span>connect(self<span style="color:#f92672">.</span>_spider_closed, signal<span style="color:#f92672">=</span>signals<span style="color:#f92672">.</span>spider_closed)
        cs<span style="color:#f92672">.</span>connect(self<span style="color:#f92672">.</span>_request_scheduled, signal<span style="color:#f92672">=</span>signals<span style="color:#f92672">.</span>request_scheduled)
        cs<span style="color:#f92672">.</span>connect(self<span style="color:#f92672">.</span>_request_received, signal<span style="color:#f92672">=</span>signals<span style="color:#f92672">.</span>request_received)
        cs<span style="color:#f92672">.</span>connect(self<span style="color:#f92672">.</span>_item_scraped, signal<span style="color:#f92672">=</span>signals<span style="color:#f92672">.</span>item_scraped)
        self<span style="color:#f92672">.</span>latency, self<span style="color:#f92672">.</span>proc_latency, self<span style="color:#f92672">.</span>items <span style="color:#f92672">=</span> <span style="color:#ae81ff">0</span>, <span style="color:#ae81ff">0</span>, <span style="color:#ae81ff">0</span>
    <span style="color:#66d9ef">def</span> <span style="color:#a6e22e">_spider_opened</span>(self, spider):
        self<span style="color:#f92672">.</span>task <span style="color:#f92672">=</span> task<span style="color:#f92672">.</span>LoopingCall(self<span style="color:#f92672">.</span>_log, spider)
        self<span style="color:#f92672">.</span>task<span style="color:#f92672">.</span>start(self<span style="color:#f92672">.</span>interval)
        
    <span style="color:#66d9ef">def</span> <span style="color:#a6e22e">_spider_closed</span>(self, spider, reason):
        <span style="color:#66d9ef">if</span> self<span style="color:#f92672">.</span>task<span style="color:#f92672">.</span>running:
            self<span style="color:#f92672">.</span>task<span style="color:#f92672">.</span>stop()
    <span style="color:#66d9ef">def</span> <span style="color:#a6e22e">_request_scheduled</span>(self, request, spider):
        request<span style="color:#f92672">.</span>meta[<span style="color:#e6db74">&#39;schedule_time&#39;</span>] <span style="color:#f92672">=</span> time()
    <span style="color:#66d9ef">def</span> <span style="color:#a6e22e">_reponse_received</span>(self, response, request, spider):
        request<span style="color:#f92672">.</span>meta[<span style="color:#e6db74">&#39;received_time&#39;</span>] <span style="color:#f92672">=</span> time()
    <span style="color:#66d9ef">def</span> <span style="color:#a6e22e">_item_scraped</span>(self, item, response, spider):
        self<span style="color:#f92672">.</span>latency <span style="color:#f92672">+=</span> time() <span style="color:#f92672">-</span> response<span style="color:#f92672">.</span>meta[<span style="color:#e6db74">&#39;schedule_time&#39;</span>]
        self<span style="color:#f92672">.</span>proc_latency <span style="color:#f92672">+=</span> time() <span style="color:#f92672">-</span> response<span style="color:#f92672">.</span>meta[<span style="color:#e6db74">&#39;received_tieme&#39;</span>]
        self<span style="color:#f92672">.</span>items <span style="color:#f92672">+=</span> <span style="color:#ae81ff">1</span>
    <span style="color:#66d9ef">def</span> <span style="color:#a6e22e">_log</span>(self, spider):
        irate <span style="color:#f92672">=</span> float(self<span style="color:#f92672">.</span>items) <span style="color:#f92672">/</span> self<span style="color:#f92672">.</span>interval
        latency <span style="color:#f92672">=</span> self<span style="color:#f92672">.</span>latency <span style="color:#f92672">/</span> slef<span style="color:#f92672">.</span>items <span style="color:#66d9ef">if</span> self<span style="color:#f92672">.</span>items <span style="color:#66d9ef">else</span> <span style="color:#ae81ff">0</span>
        proc_latency <span style="color:#f92672">=</span> self<span style="color:#f92672">.</span>proc_latency <span style="color:#f92672">/</span> self<span style="color:#f92672">.</span>items <span style="color:#66d9ef">if</span> self<span style="color:#f92672">.</span>items <span style="color:#66d9ef">else</span> <span style="color:#ae81ff">0</span>
        spider<span style="color:#f92672">.</span>logger<span style="color:#f92672">.</span>info((<span style="color:#e6db74">&#34;Scraped </span><span style="color:#e6db74">%d</span><span style="color:#e6db74"> items at </span><span style="color:#e6db74">%.1f</span><span style="color:#e6db74"> items/s, avg latencys: &#34;</span>
                           <span style="color:#e6db74">&#34;</span><span style="color:#e6db74">%.2f</span><span style="color:#e6db74"> s and avg time in pipelines: </span><span style="color:#e6db74">%.2f</span><span style="color:#e6db74"> s&#34;</span>) <span style="color:#f92672">%</span>
                           (self<span style="color:#f92672">.</span>itesm, irate, latency, proc_latency))
        self<span style="color:#f92672">.</span>latency, self<span style="color:#f92672">.</span>proc_latency, self<span style="color:#f92672">.</span>items <span style="color:#f92672">=</span> <span style="color:#ae81ff">0</span>, <span style="color:#ae81ff">0</span>, <span style="color:#ae81ff">0</span></code></pre></div>
<p>The first wo methods key as they are typical. INIT middleware using Crawler object. They&rsquo;re every nontrivial middleware. <code>from_crawler(cis, craler)</code> is way of grabbing the crawler object. Then notice in <strong>init</strong>() accesing crawler.settings and raise exception if not set. Many FooBar extensions checking the corresponding FOOBAR_ENABLED setting and raise if not set or Flase. This is very common pattern allowing middleware to be included for ease in matching settings.py settings (ITEM__PIPELINES, for example) but being disabled by default, unless enableld by flag settings. Many default middleware (AutoThrottle or HttpCache) use this pattern. In this case, the extension remains disabled unless LATENCIES_INTERVAL is est</p>

<p>Then <code>__init__()</code> register callbacks for all signals interested in using crawler.signals.connect(), and INIT afew member vars, the rest of class deploys singals handlers.</p>

<p>NOTE: by analogy to multithreaded context code this absence of mutexes in code will see single-threaded is eaiser and scales well in more complex scenarios.</p>

<p>Add this extension in <code>latencies.py</code> module at the same level as <code>settings.py</code>. Enable it by adding in <code>settings.py</code></p>

<p><code>EXTENSIONS = { 'properties.latencies.Latencies' : 500, }</code></p>

<p><code>LATENCIES_INTERVAL = 5</code></p>

<p>Now the running log will print INFO as desinged.</p>

<h3 id="extending-beyond-middlewares">EXTENDING BEYOND MIDDLEWARES</h3>

<p>Inspecting source code in <code>default_settings.py</code> will see a few class names among it. Scrapy extensively sues a dependency-injection-like mechanism allowing customisation and extension of its internal obejcts. E.g. one may want to supplort more protocls for URLs beyond files, HTTP, HTTPS, S3, and FTP that are defined in <code>downlaod_handlers_base</code> SETTING. MOST DIFFICULT PART IS TO DISCOVER WHAT THE INETERFACE FOR YOUR CUSTOM CLASSES MUST BE (I.E. WHICH MEHTODS TO IMPLEMENT) AS MOST INTERFACES ARE NOT EXPLICIT. One has to read source code and see how these classes get used. You best bet is starting with an existing implementation and altering it to your need. That said, these interfaces become more and more stable with recent versions.</p>

<h1 id="pipeline-recipes">PIPELINE Recipes</h1>

<p>Previous on middlewares, now on pipelines by showcasing consuming REST APIs, interfaciing with DB, performing CPU-intensive tasks, and interfacing with legacy services.</p>

<h3 id="using-rest-apis">Using REST APIs</h3>

<p>REST is a set of techs used to create modern web services. Its main pro is simpler more lightweight htan SOAP or else. Software designers see a similarity between CRUD (Create, Read, Update, Delete) that web services often provide and basic HTTP ops (GET POST PUT DELETE). Also seeing much of info required for typical web-serivce call could be compacted on a resoruce URL. e.g. <a href="http://api.mysite.com/customer/john">http://api.mysite.com/customer/john</a> is a resource URL alowing to identify target server (api.mysite.com), the fact that to performing ops related to customers table in taht server, and more specifically somehting taht has to do with somethe named johb (row-primary key). This plus other web oncepts like secure AUTH, being stateless, caching, XML, JSON as payload, etc provides a powerful yet simple, familiar and effortlessly cross-platform way to provide and consume web services. Quite common some of the fucntioanlity needed to use in Scrapy pipelien to be provided in the form of REST API.</p>

<p>USING treq</p>

<p><code>treq</code> is a Pyton pkg trying to equate Python <code>requests</code> pkg for Twisted-based apps. One would prefer <code>treq</code> over Scrapy&rsquo;s <code>Request/crawler.engine.download()</code> API for it&rsquo;s equally simple, but it has perforance pros.</p>

<p>PIPELINE WRITING TO <strong>ELASTICSEARCH</strong></p>

<p>Start by writing ITEM on an ES server. Perhaps begin with ES (even before MySQL) as persistence mechanism a bit unusual, but it&rsquo;s actually the easiest thing one ca ndo. ES can be schema-less, meaning using it without any configuration. <code>treq</code> is also enough for this case.</p>

<p><code>curl http://es:9200</code> returning JSON =like data. To DELETE: <code>curl -XDELETE http://es:9200/proeprties</code></p>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-python" data-lang="python"><span style="color:#a6e22e">@defer.inlineCallbacks</span>
<span style="color:#66d9ef">def</span> <span style="color:#a6e22e">process_item</span>(self, item, spider):
    data <span style="color:#f92672">=</span> json<span style="color:#f92672">.</span>dumps(dict(item), ensure_ascii<span style="color:#f92672">=</span>False)<span style="color:#f92672">.</span>encode(<span style="color:#e6db74">&#34;utf8&#34;</span>)
    <span style="color:#66d9ef">yield</span> treq<span style="color:#f92672">.</span>post(self<span style="color:#f92672">.</span>es_url, data)</code></pre></div>
<p>The first two lines define a standard <code>process_item()</code> able to <code>yield Deferred</code> as illustrated before.</p>

<p>Third line prepares data for insertion. First convert ITEM to dicts, with encoding etc. Last line uses <code>post()</code> of <code>treq</code> to perform POST request inserting doc in ES.</p>

<p>To enable pipeline, need to add it on <code>ITEM_PIPELIENS</code> setting insdie settings.py and INIT <code>ES_PIPELINE_URL</code> settings:</p>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-python" data-lang="python">ITEM_PIPELINES <span style="color:#f92672">=</span> {
    <span style="color:#e6db74">&#39;properties.pipelines.tidyip.TidyUp&#39;</span>: <span style="color:#ae81ff">100</span>,
    <span style="color:#e6db74">&#39;properties.pipelines.es.EsWriter&#39;</span>: <span style="color:#ae81ff">800</span>,
}
ES_PIPELINE_URL <span style="color:#f92672">=</span> <span style="color:#e6db74">&#39;http://es:9200/proeprties/property&#39;</span></code></pre></div>
<p>NOTE: is it a good idea to use pipelines to insert ITEM in DB? NO, often DB provide orders of magnitude more efficient ways to bulk insert entries, and we shuld definietley use them instead. This would mean bulking ITEMS and batching inserting them or performing inserts as post-processing step at ned of crawl.</p>

<p>PIPELINE GEOCODES USING GOOGLE GEOCODING API</p>

<p>Say area names for our properties, like to geocode them, finding respoective coordinates. Google Geocoding API saves the effort of complex DB, sophisticated text mathcing and spatial computations.</p>

<p><code>curl &quot;https://maps.googleapis.com/maps/api/geocode/json?sensor=false&amp;address=london&quot;</code> will return JSON of info.</p>

<p>Google API is accessible using same techniques as <code>treq</code> saving as <code>geo.py</code> inside <code>pipelines</code> directory:</p>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-python" data-lang="python"><span style="color:#a6e22e">@defer.inlineCallbacks</span>
<span style="color:#66d9ef">def</span> <span style="color:#a6e22e">geocode</span>(self, addresss):
    endpoint <span style="color:#f92672">=</span> <span style="color:#e6db74">&#39;http://web:9312/maps/api/geocode/json&#39;</span>
    
    params <span style="color:#f92672">=</span> [(<span style="color:#e6db74">&#39;address&#39;</span>, address), (<span style="color:#e6db74">&#39;sensor&#39;</span>, <span style="color:#e6db74">&#39;false&#39;</span>)]
    response <span style="color:#f92672">=</span> <span style="color:#66d9ef">yield</span> treq<span style="color:#f92672">.</span>get(endpoint, params<span style="color:#f92672">=</span>params)
    content <span style="color:#f92672">=</span> <span style="color:#66d9ef">yield</span> response<span style="color:#f92672">.</span>json()
    
    geo <span style="color:#f92672">=</span> content[<span style="color:#e6db74">&#39;results&#39;</span>][<span style="color:#ae81ff">0</span>][<span style="color:#e6db74">&#34;geometry&#34;</span>][<span style="color:#e6db74">&#34;location&#34;</span>]
    defer<span style="color:#f92672">.</span>returnValue( {<span style="color:#e6db74">&#34;lat&#34;</span>: geo[<span style="color:#e6db74">&#34;lat&#34;</span>], <span style="color:#e6db74">&#34;lon&#34;</span>: geo[<span style="color:#e6db74">&#34;lng&#34;</span>]})</code></pre></div>
<p>The endpoint is for faking for faster execution, less intrusive, available offline, more predictable. You can use endpoint = actual google api URL to hi Google&rsquo;s servers, but keep in mind STRICT LIMIT ON REQUESTS.</p>

<p>Now <code>process_item()</code> becomes a single line <code>item['location'] = yield self.geocode(item[&quot;address&quot;][0])</code></p>

<p>Enables: <code>ITEM_PIPELINES = { ... properties.pipelines.geo.geoPipeline': 400, ...}</code></p>

<h1 id="official-scrapy-tutorial">Official Scrapy Tutorial</h1>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-python" data-lang="python"><span style="color:#f92672">import</span> scrapy

<span style="color:#66d9ef">class</span> <span style="color:#a6e22e">QuotesSpider</span>(scrapy<span style="color:#f92672">.</span>Spider):
    name <span style="color:#f92672">=</span> <span style="color:#e6db74">&#34;quotes&#34;</span>
    allowed_domains <span style="color:#f92672">=</span> [<span style="color:#e6db74">&#34;toscrape.com&#34;</span>]
    start_urls <span style="color:#f92672">=</span> [<span style="color:#e6db74">&#39;http://quotes.toscrape.com&#39;</span>]

    <span style="color:#75715e"># Version1: Scraping by pages</span>
    <span style="color:#66d9ef">def</span> <span style="color:#a6e22e">parse</span>(self, response):
        self<span style="color:#f92672">.</span>log(f<span style="color:#e6db74">&#39;I just visited: {response.url}&#39;</span>)
        <span style="color:#66d9ef">for</span> quote <span style="color:#f92672">in</span> response<span style="color:#f92672">.</span>css(<span style="color:#e6db74">&#39;div.quote&#39;</span>):
            item <span style="color:#f92672">=</span> {
                <span style="color:#e6db74">&#39;author_name&#39;</span>: quote<span style="color:#f92672">.</span>css(<span style="color:#e6db74">&#39;small.author::text&#39;</span>)<span style="color:#f92672">.</span>extract_first(),
                <span style="color:#e6db74">&#39;text&#39;</span>: quote<span style="color:#f92672">.</span>css(<span style="color:#e6db74">&#39;span.text::text&#39;</span>)<span style="color:#f92672">.</span>extract_first(),
                <span style="color:#e6db74">&#39;tags&#39;</span>: quote<span style="color:#f92672">.</span>css(<span style="color:#e6db74">&#39;a.tag::text&#39;</span>)<span style="color:#f92672">.</span>extract(),
            }
            <span style="color:#66d9ef">yield</span> item
        <span style="color:#75715e"># follow pagination link</span>
        next_page_url <span style="color:#f92672">=</span> response<span style="color:#f92672">.</span>css(<span style="color:#e6db74">&#39;li.next &gt; a::attr(href)&#39;</span>)<span style="color:#f92672">.</span>extract_first()
        <span style="color:#75715e"># stop at null page link</span>
        <span style="color:#66d9ef">if</span> next_page_url:
            <span style="color:#75715e"># response.urljoin(&#39;relative path&#39;) joins with response.url(&#39;abs path&#39;)</span>
            next_page_url <span style="color:#f92672">=</span> response<span style="color:#f92672">.</span>urljoin(next_page_url)
            <span style="color:#66d9ef">yield</span> scrapy<span style="color:#f92672">.</span>Request(url<span style="color:#f92672">=</span>next_page_url, callback<span style="color:#f92672">=</span>self<span style="color:#f92672">.</span>parse)

    <span style="color:#75715e"># Version2: Scraping through links</span>
    <span style="color:#66d9ef">def</span> <span style="color:#a6e22e">parse</span>(self, response):
        <span style="color:#75715e"># collecting all links wishing to click on a page</span>
        <span style="color:#75715e"># DEFAULT DUPEFILTER set to ignore duplicate pages</span>
        urls <span style="color:#f92672">=</span> response<span style="color:#f92672">.</span>css(<span style="color:#e6db74">&#39;div.quote &gt; span &gt; a::attr(href)&#39;</span>)<span style="color:#f92672">.</span>extract()
        <span style="color:#66d9ef">for</span> url <span style="color:#f92672">in</span> urls:
            url <span style="color:#f92672">=</span> response<span style="color:#f92672">.</span>urljoin(url)
            <span style="color:#66d9ef">yield</span> scrapy<span style="color:#f92672">.</span>Request(url<span style="color:#f92672">=</span>url, callback<span style="color:#f92672">=</span>self<span style="color:#f92672">.</span>parse_details) <span style="color:#75715e"># callback to be defined</span>

        <span style="color:#75715e"># follow pagination links</span>
        <span style="color:#75715e"># same as above</span>

    <span style="color:#66d9ef">def</span> <span style="color:#a6e22e">parse_details</span>(self, response):
        <span style="color:#66d9ef">yield</span> {
            <span style="color:#e6db74">&#39;name&#39;</span>: response<span style="color:#f92672">.</span>css(<span style="color:#e6db74">&#39;h3.author-title::text&#39;</span>)<span style="color:#f92672">.</span>extract_first()
            <span style="color:#e6db74">&#39;birth&#39;</span>: response<span style="color:#f92672">.</span>css(<span style="color:#e6db74">&#39;span.author-born-date::text&#39;</span>)<span style="color:#f92672">.</span>extract_first()
        }

<span style="color:#75715e"># Version3: Scraping Infinite Scrolling Pages; finding APIs powering AJAX-based inf-scroll</span>
<span style="color:#e6db74">&#34;&#34;&#34;Concept
</span><span style="color:#e6db74">Using DevTool to inspect network as scrolling happens returning AJAX powered, mostly, JSON files;
</span><span style="color:#e6db74">explorable inside DevTool;
</span><span style="color:#e6db74">Preview by json library tools:
</span><span style="color:#e6db74">response.text   :revealing JSON format
</span><span style="color:#e6db74">print(response.text)    :readable format
</span><span style="color:#e6db74">import json
</span><span style="color:#e6db74">data = json.loads(response.text)
</span><span style="color:#e6db74">data.keys()     :prints keys
</span><span style="color:#e6db74">data[&#39;quotes&#39;][0]   :first element of quotes, a dict
</span><span style="color:#e6db74">data[&#39;quotes&#39;][0][&#39;author&#39;][&#39;name&#39;]     :lowest-level data
</span><span style="color:#e6db74">&#34;&#34;&#34;</span>
<span style="color:#f92672">import</span> json

<span style="color:#66d9ef">class</span> <span style="color:#a6e22e">QuoteScrollSpider</span>(scrapy<span style="color:#f92672">.</span>Spider):
    name <span style="color:#f92672">=</span> <span style="color:#e6db74">&#34;quotes-scroll&#34;</span>
    api_url <span style="color:#f92672">=</span> <span style="color:#e6db74">&#39;http://quotes.toscrape.com/api/quotes?page={}&#39;</span>
    start_urls <span style="color:#f92672">=</span> [api_url<span style="color:#f92672">.</span>format(<span style="color:#ae81ff">1</span>)] <span style="color:#75715e"># KEY step</span>

    <span style="color:#66d9ef">def</span> <span style="color:#a6e22e">parse</span>(self, response):
        data <span style="color:#f92672">=</span> json<span style="color:#f92672">.</span>loads(response<span style="color:#f92672">.</span>text)
        <span style="color:#66d9ef">for</span> quote <span style="color:#f92672">in</span> data[<span style="color:#e6db74">&#39;quotes&#39;</span>]:
            <span style="color:#66d9ef">yield</span> {
            <span style="color:#e6db74">&#39;author&#39;</span>: quote[<span style="color:#e6db74">&#39;author&#39;</span>][<span style="color:#e6db74">&#39;name&#39;</span>],
            <span style="color:#e6db74">&#39;text&#39;</span>: quote[<span style="color:#e6db74">&#39;text&#39;</span>]
            <span style="color:#e6db74">&#39;tags&#39;</span>: quote[<span style="color:#e6db74">&#39;tags&#39;</span>]
            }
        <span style="color:#66d9ef">if</span> data[<span style="color:#e6db74">&#39;has_next&#39;</span>]:
            next_page <span style="color:#f92672">=</span> data[<span style="color:#e6db74">&#39;page&#39;</span>] <span style="color:#f92672">+</span> <span style="color:#ae81ff">1</span>
            <span style="color:#66d9ef">yield</span> scrapy<span style="color:#f92672">.</span>Request(url<span style="color:#f92672">=</span>self<span style="color:#f92672">.</span>api_url<span style="color:#f92672">.</span>format(next_page), callback<span style="color:#f92672">=</span>self<span style="color:#f92672">.</span>parse)

<span style="color:#75715e"># Version4: Submitting Forms - POST requests such as logins</span>
<span style="color:#e6db74">&#34;&#34;&#34;Concept
</span><span style="color:#e6db74">Network-inspect requests at login reveals POST request such as login with value like username, password;
</span><span style="color:#e6db74">In example case, there&#39;s a hidden input &#39;type=&#34;hidden&#34; name=&#34;carf_token&#34;&#39;, inspect via page-source-code;
</span><span style="color:#e6db74">Its value is often HASHed of something, which in case is randomised per page load;
</span><span style="color:#e6db74">Solution: submit form with user/pass + carf_token scraped at page-load
</span><span style="color:#e6db74">&#34;&#34;&#34;</span>
<span style="color:#66d9ef">class</span> <span style="color:#a6e22e">LoginSpider</span>(scrapy<span style="color:#f92672">.</span>Spider):
    name <span style="color:#f92672">=</span> <span style="color:#e6db74">&#39;loging-spider&#39;</span>
    login_url <span style="color:#f92672">=</span> <span style="color:#e6db74">&#39;http://quotes.toscrape.com/login&#39;</span>
    start_urls <span style="color:#f92672">=</span> [login_url]

    <span style="color:#66d9ef">def</span> <span style="color:#a6e22e">parse</span>(self, response):
        <span style="color:#75715e"># extract the CSRF token (selector depending on context)</span>
        token <span style="color:#f92672">=</span> response<span style="color:#f92672">.</span>css(<span style="color:#e6db74">&#39;input[name=&#39;</span>csrf_token<span style="color:#e6db74">&#39;]::attr(value)&#39;</span>)<span style="color:#f92672">.</span>extract_first()
        <span style="color:#75715e"># create a python dict with form values</span>
        data <span style="color:#f92672">=</span> {
            <span style="color:#e6db74">&#39;csrf_token&#39;</span>: token,
            <span style="color:#e6db74">&#39;username&#39;</span>: <span style="color:#e6db74">&#39;whatever&#39;</span>,
            <span style="color:#e6db74">&#39;password&#39;</span>: <span style="color:#e6db74">&#39;whatever&#39;</span>,
        }
        <span style="color:#75715e"># submit a POST request to web (url may differ from login page)</span>
        <span style="color:#66d9ef">yield</span> scrapy<span style="color:#f92672">.</span>FormRequest(url<span style="color:#f92672">=</span>self<span style="color:#f92672">.</span>login_url, formdata<span style="color:#f92672">=</span>data, callback<span style="color:#f92672">=</span>self<span style="color:#f92672">.</span>parse<span style="color:#f92672">.</span>quotes)


    <span style="color:#66d9ef">def</span> <span style="color:#a6e22e">parse_quotes</span>(self, response):
        <span style="color:#e6db74">&#34;&#34;&#34;Parse the main page after the spider logged in&#34;&#34;&#34;</span>
        <span style="color:#66d9ef">for</span> q <span style="color:#f92672">in</span> response<span style="color:#f92672">.</span>css(<span style="color:#e6db74">&#39;div.quote&#39;</span>):
            <span style="color:#66d9ef">yield</span> {
                <span style="color:#e6db74">&#39;author_name&#39;</span>: q<span style="color:#f92672">.</span>css(<span style="color:#e6db74">&#39;small.author::text&#39;</span>)<span style="color:#f92672">.</span>extract_first(),
                <span style="color:#e6db74">&#39;author_url&#39;</span>: q<span style="color:#f92672">.</span>css(<span style="color:#e6db74">&#39;small.author ~ a[href*=&#34;goodreads.com&#34;]::attr(href)&#39;</span>)<span style="color:#f92672">.</span>extract_first()
            }

<span style="color:#75715e"># Even simpler way is to use FormRequest.from_request() directly parsing hidden field!!</span>
<span style="color:#e6db74">&#34;&#34;&#34;
</span><span style="color:#e6db74">This method reads the response object and creates a FormRequest that automatically includes all the pre-filled values from the form, along with the hidden ones. 
</span><span style="color:#e6db74">This is how our spider&#39;s parse_tags() method looks:
</span><span style="color:#e6db74">So, whenever you are dealing with forms containing some hidden fields and pre-filled values, use the from_response method because your code will look much cleaner.
</span><span style="color:#e6db74">&#34;&#34;&#34;</span>
    <span style="color:#66d9ef">def</span> <span style="color:#a6e22e">parse_tags</span>(self, response):
        <span style="color:#66d9ef">for</span> tag <span style="color:#f92672">in</span> response<span style="color:#f92672">.</span>css(<span style="color:#e6db74">&#39;select#tag &gt; option ::attr(value)&#39;</span>)<span style="color:#f92672">.</span>extract():
            <span style="color:#66d9ef">yield</span> scrapy<span style="color:#f92672">.</span>FormRequest<span style="color:#f92672">.</span>from_response(
                response,
                formdata<span style="color:#f92672">=</span>{<span style="color:#e6db74">&#39;tag&#39;</span>: tag},
                callback<span style="color:#f92672">=</span>self<span style="color:#f92672">.</span>parse_results,
            )


<span style="color:#75715e"># Version5: Scraping JS pages with Splash: scraping JS-based webs using Scrapy + Splash</span>
<span style="color:#e6db74">&#34;&#34;&#34;Concept
</span><span style="color:#e6db74">JS-based pages returns only static HTML when scraped by Scrapy, or whatever present inspected via PAGE SOURCE;
</span><span style="color:#e6db74">Active inspection will show JS called content, which is not scraped by Scrapy;
</span><span style="color:#e6db74">
</span><span style="color:#e6db74">Splash - JS Engine
</span><span style="color:#e6db74">    docker pull scrapinghub/splash
</span><span style="color:#e6db74">    docker run -p 8050:8050 scrapinghub/splash
</span><span style="color:#e6db74">        # now splash is LISTENING to the local 8050 port
</span><span style="color:#e6db74">        # so Spider can REQUEST to it, Splash fetches the page, execute JS code on it, then returning rendered pages to spider
</span><span style="color:#e6db74">    pip install scrapy-splash
</span><span style="color:#e6db74">
</span><span style="color:#e6db74">Need to config SETTING:
</span><span style="color:#e6db74">
</span><span style="color:#e6db74">    DOWNLOADER_MIDDLEWARES = {
</span><span style="color:#e6db74">        &#39;scrapy_splash.SplashCookiesMiddleware&#39;: 723,
</span><span style="color:#e6db74">        &#39;scrapy_splash.SplashMiddleware&#39;: 725,
</span><span style="color:#e6db74">        &#39;scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware&#39;: 810,
</span><span style="color:#e6db74">    }
</span><span style="color:#e6db74">
</span><span style="color:#e6db74">The middleware needs to take precedence over HttpProxyMiddleware,
</span><span style="color:#e6db74">which by default is at position 750, so we set the middleware positions to numbers below 750.
</span><span style="color:#e6db74">
</span><span style="color:#e6db74">You then need to set the SPLASH_URL setting in your project&#39;s settings.py:
</span><span style="color:#e6db74">    SPLASH_URL = &#39;http://localhost:8050/&#39;
</span><span style="color:#e6db74">
</span><span style="color:#e6db74">Don’t forget, if you’re using a Docker Machine on OS X or Windows, you will need to set this to the IP address of Docker’s virtual machine, e.g.:
</span><span style="color:#e6db74">    SPLASH_URL = &#39;http://192.168.59.103:8050/&#39;
</span><span style="color:#e6db74">
</span><span style="color:#e6db74">Enable SplashDeduplicateArgsMiddleware to support cache_args feature: it allows to save disk space by not storing duplicate Splash arguments multiple times in a disk request queue. If Splash 2.1+ is used the middleware also allows to save network traffic by not sending these duplicate arguments to Splash server multiple times.
</span><span style="color:#e6db74">    SPIDER_MIDDLEWARES = {
</span><span style="color:#e6db74">    &#39;scrapy_splash.SplashDeduplicateArgsMiddleware&#39;: 100,
</span><span style="color:#e6db74">    }
</span><span style="color:#e6db74">
</span><span style="color:#e6db74">Scrapy currently doesn’t provide a way to override request fingerprints calculation globally, so you will also have to set a custom DUPEFILTER_CLASS and a custom cache storage backend:
</span><span style="color:#e6db74">    DUPEFILTER_CLASS = &#39;scrapy_splash.SplashAwareDupeFilter&#39;
</span><span style="color:#e6db74">    HTTPCACHE_STORAGE = &#39;scrapy_splash.SplashAwareFSCacheStorage&#39;
</span><span style="color:#e6db74">
</span><span style="color:#e6db74">If you already use another cache storage backend, you will need to subclass it and replace all calls to scrapy.util.request.request_fingerprint with scrapy_splash.splash_request_fingerprint.
</span><span style="color:#e6db74">
</span><span style="color:#e6db74">Now that the Splash middleware is enabled, you can use SplashRequest in place of scrapy.Request to render pages with Splash.
</span><span style="color:#e6db74">
</span><span style="color:#e6db74">For full list of arguments in HTTP API doc: http://splash.readthedocs.org/en/latest/api.html
</span><span style="color:#e6db74">By default the endpoint is set to &#39;render.json&#39; but here overridden and set to &#39;render.html&#39; for HTML response
</span><span style="color:#e6db74">&#34;&#34;&#34;</span>

<span style="color:#f92672">import</span> scrapy
<span style="color:#f92672">from</span> scrapy_splash <span style="color:#f92672">import</span> SplashRequest

<span style="color:#66d9ef">class</span> <span style="color:#a6e22e">QuotesJSSpider</span>(scrapy<span style="color:#f92672">.</span>Spider):
    name <span style="color:#f92672">=</span> <span style="color:#e6db74">&#39;quotesjs&#39;</span>

    <span style="color:#66d9ef">def</span> <span style="color:#a6e22e">start_request</span>(self):
        <span style="color:#66d9ef">yield</span> SplashRequest(
            url<span style="color:#f92672">=</span><span style="color:#e6db74">&#39;http://quotes.toscrape.com/js&#39;</span>,
            callback<span style="color:#f92672">=</span>self<span style="color:#f92672">.</span>parse,
            )
    <span style="color:#66d9ef">def</span> <span style="color:#a6e22e">parse</span>(self, response):
        <span style="color:#66d9ef">for</span> quote <span style="color:#f92672">in</span> response<span style="color:#f92672">.</span>css(<span style="color:#f92672">...</span>)
        <span style="color:#66d9ef">yield</span> <span style="color:#f92672">...</span>

        

<span style="color:#e6db74">&#34;&#34;&#34;
</span><span style="color:#e6db74">**Running Custom JS**
</span><span style="color:#e6db74">Sometimes you need to press a buttom or close a modal to view pape properly.
</span><span style="color:#e6db74">Splash lets run custom JS code within the context of web page:
</span><span style="color:#e6db74">
</span><span style="color:#e6db74">1) Using js_source Parameter
</span><span style="color:#e6db74">Code is run after page loaded but before page rendered, allowing use of JS code
</span><span style="color:#e6db74">to modify page being rendered:
</span><span style="color:#e6db74">
</span><span style="color:#e6db74">EX: render page and mod its title dynamically
</span><span style="color:#e6db74">    yield SplashRequest(
</span><span style="color:#e6db74">        &#39;http://example.com&#39;,
</span><span style="color:#e6db74">        endpoint=&#39;render.html&#39;,
</span><span style="color:#e6db74">        args={&#39;js_source&#39;: &#39;document.title=&#34;My Title&#34;;&#39;},
</span><span style="color:#e6db74">    )
</span><span style="color:#e6db74">
</span><span style="color:#e6db74">2) Splash Scripts
</span><span style="color:#e6db74">Splash supports LUA scripts via execute endpoint. Preferred way for preload libraries
</span><span style="color:#e6db74">choosing when to execute JS and retrieve output.
</span><span style="color:#e6db74">
</span><span style="color:#e6db74">Sample script:
</span><span style="color:#e6db74">    function main(splash)
</span><span style="color:#e6db74">        assert(splash:go(splash.args.url))
</span><span style="color:#e6db74">        splash:wait(0.5)
</span><span style="color:#e6db74">        local title = splash:evaljs(&#34;document.title&#34;)
</span><span style="color:#e6db74">        return {title=title}
</span><span style="color:#e6db74">    end
</span><span style="color:#e6db74">
</span><span style="color:#e6db74">Need to send script to execute endpoint, in lua_source arguments, returning a JSON object having title:
</span><span style="color:#e6db74">    {
</span><span style="color:#e6db74">        &#34;title&#34;: &#34;Some title&#34;
</span><span style="color:#e6db74">    }
</span><span style="color:#e6db74">
</span><span style="color:#e6db74">Every script needs a main func to act as entry point. Able to return lua table be rendred as JSON, as here.
</span><span style="color:#e6db74">Using splash:go function to tell Splash to visit the URL, splash:evaljs function lets run JS within page context,
</span><span style="color:#e6db74">but if no need result then use splash:runjs instead
</span><span style="color:#e6db74">
</span><span style="color:#e6db74">Test Splash scripts in browser at instance&#39;s index page set above, 
</span><span style="color:#e6db74">For mouse-click function: using splash:mouse_click
</span><span style="color:#e6db74">
</span><span style="color:#e6db74">    function main(splash)
</span><span style="color:#e6db74">        assert(splash:go(splash.args.url))
</span><span style="color:#e6db74">        local get_dimensions = splash:jsfunc([[
</span><span style="color:#e6db74">            function () {
</span><span style="color:#e6db74">                var rect = document.getElementById(&#39;button&#39;).getClientRects()[0];
</span><span style="color:#e6db74">                return {&#34;x&#34;: rect.left, &#34;y&#34;: rect.top}
</span><span style="color:#e6db74">            }
</span><span style="color:#e6db74">        ]])
</span><span style="color:#e6db74">        splash:set_viewport_full()
</span><span style="color:#e6db74">        splash:wait(0.1)
</span><span style="color:#e6db74">        local dimensions = get_dimensions()
</span><span style="color:#e6db74">        splash:mouse_click(dimensions.x, dimensions.y)
</span><span style="color:#e6db74">        -- Wait split second to allow event to propagate.
</span><span style="color:#e6db74">        splash:wait(0.1)
</span><span style="color:#e6db74">        return splash:html()
</span><span style="color:#e6db74">    end
</span><span style="color:#e6db74">
</span><span style="color:#e6db74">Here splash:jsfunc defined to return element coordinates, visible by splash:set_viewport_full, click element and return HTML
</span><span style="color:#e6db74">&#34;&#34;&#34;</span>


<span style="color:#e6db74">&#34;&#34;&#34;
</span><span style="color:#e6db74">Run Spider on Cloud: deploy, run and manage crawlers in cloud
</span><span style="color:#e6db74">Above are single spider.py, now build a project
</span><span style="color:#e6db74">    scrapy startproject quotes_crawler
</span><span style="color:#e6db74">Example, move one of above spiders to project and run
</span><span style="color:#e6db74">    scrapy crawl spiderName
</span><span style="color:#e6db74">
</span><span style="color:#e6db74">Scraping Hub as Cloud
</span><span style="color:#e6db74">    pip install shub
</span><span style="color:#e6db74">
</span><span style="color:#e6db74">    shub login # then API key
</span><span style="color:#e6db74">    shub deploy # requring Project ID, which is digits on URL of project page
</span><span style="color:#e6db74">        # review under Code &amp; Deploys
</span><span style="color:#e6db74">Run -&gt; Spider -&gt; Job Units etc
</span><span style="color:#e6db74">Inspect result under Job with downloadable formats
</span><span style="color:#e6db74">Schedule features
</span><span style="color:#e6db74">Also CLI
</span><span style="color:#e6db74">    shub schedule quotes # run the spider
</span><span style="color:#e6db74">&#34;&#34;&#34;</span></code></pre></div>
<h2 id="udemy-video-summary">Udemy Video Summary</h2>

<h3 id="1-scrapy-architecture">(1) Scrapy Architecture</h3>

<p>Root/scrapy.pyc [settings] path and [deploy] url and project folder</p>

<p>Under ProjectFolder:
 - <strong>init</strong>.py 		:directories
 - items.py 		:item classes which be imported inside spider (via scrapy.loader.ItemLoader)
 - pipelines.py 	:process_item and related processing as pipelines (init in settings.py -&gt; pipeline class inserted)</p>

<h3 id="2-avoiding-ban">(2) Avoiding Ban</h3>

<p>	a) DOWNLOAD_DELAY or via time.sleep(random.randrange(1,3)) at end of code
	b) USER_AGENT
	c) Proxies scrapy-proxies package or use VPN
	d) Professional work using ScrapingHub
	e) Be mindful of regulation and rights</p>

<h3 id="3-runspider-for-standalone-scripting">(3) Runspider for standalone scripting</h3>

<p>	- Without use of ITEM/PIPELINE etc
	- Print out or yield result</p>

<h3 id="4-scrapy-spiders-crawlspider-has-more-functions-such-as-rule-which-need-importing-itself">(4) scrapy.spiders.CrawlSpider has more functions such as RULE, which need importing itself</h3>

<p>	rules = (Rule(LinkExtractor(allow=(&lsquo;music&rsquo;), deny_domains=(&lsquo;google.com&rsquo;)), callback=&lsquo;parse_be_defined&rsquo;, follow=False),)</p>

<h3 id="5-scrapy-http-request-method-used-under-ordinary-parse-self-response-without-callback-to-loop-through-new-request-url-to-parse">(5) scrapy.http.Request method used under ordinary parse(self,response) WITHOUT callback to loop through new Request(url) to parse() !!</h3>

<h3 id="6-relative-url-fixing-e-g-images">(6) relative URL fixing, e.g. images</h3>

<p>	- Inspect HTML //img/@src to see relative path, e.g. ../../path/to/image.jpg
	- Replace ../../ with actual image URL
		image_url = image_url.replace(&lsquo;../..&rsquo;, &lsquo;<a href="http://missingPath'">http://missingPath'</a>)</p>

<h3 id="7-define-functions-to-extract-well-formated-datapoints-e-g-tables-of-data">(7) Define functions to extract well-formated datapoints, e.g. tables of data</h3>

<p>	def product_info(response, value)
		return response.xpath(&lsquo;//th[text()=&ldquo;&rsquo; + value + &lsquo;&rdquo;]/following-sibling::td/text()&lsquo;).extract_first()
	then use it to extract and save into ITEM key-value pairs</p>

<h3 id="example-code">EXAMPLE CODE</h3>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-python" data-lang="python"><span style="color:#f92672">from</span> scrapy <span style="color:#f92672">import</span> Spider
<span style="color:#f92672">from</span> scrapy.http <span style="color:#f92672">import</span> Request

<span style="color:#66d9ef">def</span> <span style="color:#a6e22e">product_info</span>(response, value):
    <span style="color:#f92672">...</span>

<span style="color:#66d9ef">class</span> <span style="color:#a6e22e">BooksSpider</span>(Spider):
    name
    allowed_domains
    start_urls

    <span style="color:#66d9ef">def</span> <span style="color:#a6e22e">parse</span>(self, resonse):
        books <span style="color:#f92672">=</span> response<span style="color:#f92672">.</span>xpath(<span style="color:#e6db74">&#39;//h3/a/@href&#39;</span>)<span style="color:#f92672">.</span>extract()
        <span style="color:#66d9ef">for</span> book <span style="color:#f92672">in</span> books:
            abs_url <span style="color:#f92672">=</span> response<span style="color:#f92672">.</span>urljoin(book)
            <span style="color:#66d9ef">yield</span> Request(abs_url, callback<span style="color:#f92672">=</span>self<span style="color:#f92672">.</span>parse_book)
        <span style="color:#75715e"># process next page</span>
        next_url <span style="color:#f92672">=</span> response<span style="color:#f92672">.</span>xpath(<span style="color:#e6db74">&#39;//a[text()=&#34;next&#34;]/@href&#39;</span>)<span style="color:#f92672">.</span>extract_first()
        abs_next_url <span style="color:#f92672">=</span> response<span style="color:#f92672">.</span>urljoin(next_url)
        <span style="color:#66d9ef">yield</span> Request(abs_next_url)
    
    <span style="color:#66d9ef">def</span> <span style="color:#a6e22e">parse_book</span>(self, response):
        title <span style="color:#f92672">=</span> response<span style="color:#f92672">.</span>css(<span style="color:#e6db74">&#39;h1::text&#39;</span>)<span style="color:#f92672">.</span>extract_first()
        price <span style="color:#f92672">=</span> response<span style="color:#f92672">.</span>xpath(<span style="color:#e6db74">&#39;//*[@class=&#34;price_color&#34;]/text()&#39;</span>)<span style="color:#f92672">.</span>extract_first()
        image_url (<span style="color:#66d9ef">as</span> above)
        rating <span style="color:#f92672">=</span> response<span style="color:#f92672">.</span>xpath(<span style="color:#e6db74">&#39;//*[contains(@class, &#34;star-rating&#34;)]/@class&#39;</span>)<span style="color:#f92672">.</span>extract_first()
        rating <span style="color:#f92672">=</span> rating<span style="color:#f92672">.</span>replace(<span style="color:#e6db74">&#39;star-rating &#39;</span>, <span style="color:#e6db74">&#39;&#39;</span>)
    
        description <span style="color:#f92672">=</span> response<span style="color:#f92672">.</span>xpath(<span style="color:#e6db74">&#39;//*[@id=&#34;product_description&#34;]/following-sibling::p/text()&#39;</span>)<span style="color:#f92672">.</span>extract_first()
    
        <span style="color:#75715e"># product table as above</span>
        upc <span style="color:#f92672">=</span> product_info(response, <span style="color:#e6db74">&#39;UPC&#39;</span>)</code></pre></div>
<hr />

<h3 id="8-arguments-e-g-isolating-book-categories">(8) Arguments: e.g. isolating &lsquo;book categories&rsquo;</h3>

<p>Update above code
​	REMOVE start_url WITH:</p>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-python" data-lang="python"><span style="color:#66d9ef">def</span> __init__(self, category): <span style="color:#75715e"># constructor!</span>
self<span style="color:#f92672">.</span>start_urls <span style="color:#f92672">=</span> [category] 

THIS CREATES A ARGUMENT<span style="color:#f92672">-</span>ABLE FOR __INIT__
used <span style="color:#f92672">in</span> Shell: 
scrapy crawl bookspider <span style="color:#f92672">-</span>a category<span style="color:#f92672">=</span><span style="color:#e6db74">&#34;category_specific_URL&#34;</span></code></pre></div>
<h3 id="9-scrapy-functions-executed-at-end-of-crawling">(9) Scrapy Functions: executed at end of crawling</h3>

<p>Anything needed to run, cleaning, sending, etc. Defined inside Spider.py</p>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-python" data-lang="python"><span style="color:#75715e"># EX overriding output filename.csv a function postprocessing </span>

<span style="color:#f92672">import</span> os
<span style="color:#f92672">import</span> glob

    <span style="color:#66d9ef">def</span> <span style="color:#a6e22e">close</span>(self, reason):
        csv_file <span style="color:#f92672">=</span> max(glob<span style="color:#f92672">.</span>iglob(<span style="color:#e6db74">&#39;*.csv&#39;</span>), keys<span style="color:#f92672">=</span>os<span style="color:#f92672">.</span>path<span style="color:#f92672">.</span>getctime)
        os<span style="color:#f92672">.</span>rename(csv_file, <span style="color:#e6db74">&#39;foobar.csv&#39;</span>)

<span style="color:#f92672">&gt;&gt;</span> <span style="color:#f92672">...</span> <span style="color:#f92672">-</span>o item<span style="color:#f92672">.</span>csv</code></pre></div>
<h3 id="10-feeding">(10) Feeding</h3>

<p>&hellip; -o items.csv/json/xml # items can be whatever</p>

<h3 id="11-image-download-via-built-in-imagespipeline">(11) Image Download via built-in ImagesPipeline</h3>

<p>Best first define Item class in items.py with all required datapoints + image
Then change settings.py</p>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-python" data-lang="python">ITEM_PIPELINES <span style="color:#f92672">=</span> {
    <span style="color:#e6db74">&#39;scrapy.pipelines.images.ImagesPipeline&#39;</span>: <span style="color:#ae81ff">1</span>,
}
IMAGES_STORE <span style="color:#f92672">=</span> <span style="color:#e6db74">&#39;local/folder&#39;</span></code></pre></div>
<p>Then add ItemLoader and items.definedClass in spider.py</p>
</div>

        
        <div class="section bottom-menu">
<hr />
<p>


    

    
        
            <a href="/code">
                code
            </a>
        
    
    
        
            &#183; 
            <a href="https://tiny.cc/oceannotebook">
                notebook
            </a>
        
            &#183; 
            <a href="https://github.com/Oceanbao/KEN">
                KEN
            </a>
        
            &#183; 
            <a href="/prose">
                prose
            </a>
        
            &#183; 
            <a href="/gallery">
                gallery
            </a>
        
            &#183; 
            <a href="/qui">
                qui et quoi?
            </a>
        
    
    &#183; 
    <a href="https://oceanbao.github.io/">
        main
    </a>

</p></div>
        

        <div class="section footer">Ocean Ode

<script src="//yihui.name/js/math-code.js"></script>
<script async src="//mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML"></script>

<script async src="//yihui.name/js/center-img.js"></script></div>
    </div>
</body>

</html>