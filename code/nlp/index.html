<!DOCTYPE html>
<html>

<head>
    
    <meta charset="utf-8">
<meta http-equiv="X-UA-Compatible" content="chrome=1">
<meta name="HandheldFriendly" content="True">
<meta name="MobileOptimized" content="320">
<meta name="viewport" content="width=device-width, initial-scale=1.0">
<meta name="referrer" content="no-referrer">

<meta name="description" content="Ocean on GitHub.">

<meta name="twitter:card" content="summary">
<meta name="twitter:domain" content="/">

<meta name="twitter:image" content="/tn.png">
<meta name="twitter:title" property="og:title" itemprop="title name" content="Ocean Ode">
<meta name="twitter:description" property="og:description" itemprop="description" content="Ocean on GitHub.">
<meta name="og:type" content="website">
<meta name="og:url" content="/">
<meta name="og:image" itemprop="image primaryImageOfPage" content="/tn.png">

<link rel="shortcut icon" href="/oceanicon.jpg" id="favicon">
<link rel="stylesheet" href="/css/style.css">

<link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Didact+Gothic">


    

    
    
    
    <title>
        
        NLP
        
    </title>
</head>

<body>
    <div class="wrap">
        <div class="section" id="title">NLP</div>

        <div class="section" id="content">

<h1 id="nlp-primer">NLP - Primer</h1>

<ul>
<li>Text, unstructured particularly, is as aboundant as important to understanding!</li>
<li><a href="https://devblogs.nvidia.com/parallelforall/introduction-neural-machine-translation-gpus-part2/">Introduction to NN Translation with GPUs</a></li>
<li>Sources

<ul>
<li><a href="http://www.anc.org">Open American Natioanl Corpus</a></li>
<li><a href="http://www.natcorp.ox.ac.uk">British Natioanl Corpus</a></li>
<li><a href="https://en.wikipedia.org/wiki/List_of_text_corpora">List of Text Corpora</a></li>
<li><a href="https://en.wikipedia.org/wiki/Wikipedia:Database_download">Wikiepedia Dataset</a></li>
<li>Twitter (see Text)</li>
</ul></li>
<li>Topic spotting</li>
<li>Text classification</li>
<li>Application

<ol>
<li>chatbot</li>
<li>translation</li>
<li>sentiment analysis</li>
</ol></li>
</ul>

<h1 id="string-primer">String Primer</h1>

<h3 id="unicode">Unicode</h3>

<ul>
<li>Remember to include <strong>U</strong> before string to ensure Unicode String</li>
</ul>

<h2 id="regular-expression-strings-with-special-syntax">Regular Expression - strings with special syntax</h2>

<ul>
<li>allows to match <strong>patterns</strong> in other strings</li>
<li>Regex, <code>import re</code></li>
<li>Matching pattern with string</li>
</ul>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-python" data-lang="python">re<span style="color:#f92672">.</span>match(<span style="color:#e6db74">&#39;abc&#39;</span>, <span style="color:#e6db74">&#39;abcdef&#39;</span>)
<span style="color:#75715e"># word phrase</span>
word_regex <span style="color:#f92672">=</span> <span style="color:#e6db74">&#39;\w+&#39;</span>
re<span style="color:#f92672">.</span>match(word_regex, <span style="color:#e6db74">&#39;hi there!&#39;</span>)</code></pre></div>
<h3 id="common-regex-patterns">Common Regex Patterns</h3>

<ul>
<li><code>\w+</code> [word, &lsquo;Magic&rsquo;]</li>
<li><code>\d</code> [digit, 9]</li>
<li><code>\s</code> [space, &ldquo;]</li>
<li><code>.*</code> [wildcard, &lsquo;usename74&rsquo;]</li>
<li><code>+ or *</code> [greedy, &lsquo;aaaaa&rsquo;]</li>
<li>capitalised = Negation, <code>\S</code> [Not space, &lsquo;no_spaces&rsquo;]</li>
<li><code>[a-z]</code> [lowercase group, &lsquo;abcedfg&rsquo;]</li>
</ul>

<blockquote>
<p>return depends, iter, string, or match object</p>

<p>useful for preprocessing before <strong>TOKENISATION</strong></p>
</blockquote>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-python" data-lang="python">my_string <span style="color:#f92672">=</span> <span style="color:#e6db74">&#34;Let&#39;s write RegEx!  Won&#39;t that be fun?  I sure think so.  Can you find 4 sentences?  Or perhaps, all 19 words?&#34;</span>


<span style="color:#f92672">import</span> re

sentence_endings <span style="color:#f92672">=</span> <span style="color:#e6db74">r</span><span style="color:#e6db74">&#34;[.?!]&#34;</span>

<span style="color:#66d9ef">print</span>(<span style="color:#e6db74">&#34;</span><span style="color:#ae81ff">\n</span><span style="color:#e6db74">&#34;</span>,re<span style="color:#f92672">.</span>split(sentence_endings, my_string))

capitalized_words <span style="color:#f92672">=</span> <span style="color:#e6db74">r</span><span style="color:#e6db74">&#34;[A-Z]\w+&#34;</span>
<span style="color:#66d9ef">print</span>(re<span style="color:#f92672">.</span>findall(capitalized_words, my_string))


spaces <span style="color:#f92672">=</span> <span style="color:#e6db74">r</span><span style="color:#e6db74">&#34;\s+&#34;</span>
<span style="color:#66d9ef">print</span>(<span style="color:#e6db74">&#34;</span><span style="color:#ae81ff">\n</span><span style="color:#e6db74">&#34;</span>, re<span style="color:#f92672">.</span>split(spaces, my_string))


digits <span style="color:#f92672">=</span> <span style="color:#e6db74">r</span><span style="color:#e6db74">&#34;\d+&#34;</span>
<span style="color:#66d9ef">print</span>(<span style="color:#e6db74">&#34;</span><span style="color:#ae81ff">\n</span><span style="color:#e6db74">&#34;</span>,re<span style="color:#f92672">.</span>findall(digits, my_string))</code></pre></div>
<pre><code> [&quot;Let's write RegEx&quot;, &quot;  Won't that be fun&quot;, '  I sure think so', '  Can you find 4 sentences', '  Or perhaps, all 19 words', '']
['Let', 'RegEx', 'Won', 'Can', 'Or']

 [&quot;Let's&quot;, 'write', 'RegEx!', &quot;Won't&quot;, 'that', 'be', 'fun?', 'I', 'sure', 'think', 'so.', 'Can', 'you', 'find', '4', 'sentences?', 'Or', 'perhaps,', 'all', '19', 'words?']

 ['4', '19']
</code></pre>

<h2 id="tokenisation">Tokenisation</h2>

<ul>
<li>preparing text for NLP</li>
<li>N-gram, punctuation, hashtages, etc</li>
<li>NLTK module: <code>from nltk.tokenize import word_tokenize</code></li>
<li><strong>WHY</strong>

<ol>
<li>easier to map SPEECH PART</li>
<li>matching common words</li>
<li>removing unwanted tokens</li>
<li>e.g. &lsquo;I don&rsquo;t like Sam&rsquo;s shoes&rsquo; -&gt; &ldquo;I&rdquo;, &ldquo;do&rdquo;, &ldquo;n&rsquo;t&rdquo;&hellip;</li>
</ol></li>
<li>Other NLTK class:

<ol>
<li>`<code>sent_tokenize</code> tokenise document into sentenses</li>
<li><code>regexp_tokenize</code> tokenise string or doc on regex pattern</li>
<li><code>TweetTokenizer</code> special class for tweet, hashtags, mentions, lots of exclamation points !!!</li>
</ol></li>
</ul>

<h4 id="diff-search-match">Diff(search, match)</h4>

<ul>
<li>Search matches all on everywhere, unlike match only onset BUT <strong>useful for entire pattern or beginning</strong></li>
</ul>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-python" data-lang="python"><span style="color:#66d9ef">with</span> open(<span style="color:#e6db74">&#39;Data_Folder/TxT/grail_abridged.txt&#39;</span>, mode<span style="color:#f92672">=</span><span style="color:#e6db74">&#39;r&#39;</span>) <span style="color:#66d9ef">as</span> file:
    scene_one <span style="color:#f92672">=</span> file<span style="color:#f92672">.</span>read()

<span style="color:#f92672">from</span> nltk.tokenize <span style="color:#f92672">import</span> sent_tokenize, word_tokenize

<span style="color:#75715e"># Split as sentences</span>
sentences <span style="color:#f92672">=</span> sent_tokenize(scene_one)

<span style="color:#66d9ef">print</span>(<span style="color:#e6db74">&#39;</span><span style="color:#ae81ff">\n</span><span style="color:#e6db74">&#39;</span>,sentences[:<span style="color:#ae81ff">3</span>])

<span style="color:#75715e"># tokenise the 4th sentence</span>
tokenize_sent <span style="color:#f92672">=</span> word_tokenize(sentences[<span style="color:#ae81ff">3</span>])

<span style="color:#66d9ef">print</span>(<span style="color:#e6db74">&#39;</span><span style="color:#ae81ff">\n</span><span style="color:#e6db74">&#39;</span>,tokenize_sent)

unique_tokens <span style="color:#f92672">=</span> set(word_tokenize(scene_one))

<span style="color:#66d9ef">print</span>(<span style="color:#e6db74">&#39;</span><span style="color:#ae81ff">\n</span><span style="color:#e6db74">&#39;</span>,unique_tokens)</code></pre></div>
<pre><code> ['SCENE 1: [wind] [clop clop clop] \nKING ARTHUR: Whoa there!', '[clop clop clop] \nSOLDIER #1: Halt!', 'Who goes there?']

 ['ARTHUR', ':', 'It', 'is', 'I', ',', 'Arthur', ',', 'son', 'of', 'Uther', 'Pendragon', ',', 'from', 'the', 'castle', 'of', 'Camelot', '.']

 {'they', 'Not', 'husk', 'using', 'fly', 'under', 'tropical', 'That', 'lord', 'point', 'use', 'sovereign', 'winter', 'wants', 'to', 'second', 'Uther', 'ounce', 'together', 'five', 'by', '!', '1', 'coconut', ']', 'and', 'Patsy', 'Saxons', '2', 'join', 'or', 'pound', 'Halt', 'there', 'goes', 'is', 'Yes', 'seek', 'go', 'master', 'do', 'that', 'martin', &quot;'re&quot;, 'warmer', 'son', 'ridden', 'SOLDIER', 'with', 'could', ',', &quot;'em&quot;, 'Mercea', 'agree', 'mean', 'A', 'KING', 'clop', 'one', 'court', 'You', 'It', 'two', '--', 'get', 'our', 'must', 'They', 'Pendragon', 'forty-three', 'south', 'back', 'bird', 'ratios', '?', 'migrate', 'here', 'breadth', 'will', 'he', 'empty', 'other', 'creeper', &quot;'d&quot;, 'the', 'So', 'Will', 'Wait', 'Arthur', 'why', 'got', 'interested', 'have', 'house', 'defeator', 'where', 'Britons', 'dorsal', 'In', 'carried', 'maintain', 'your', 'halves', 'Listen', 'Supposing', 'my', 'length', 'We', 'found', 'temperate', 'Court', 'strangers', 'tell', 'anyway', '[', 'on', 'through', 'Whoa', 'you', 'be', 'England', 'horse', 'yeah', 'am', 'just', &quot;'s&quot;, &quot;n't&quot;, '#', 'all', 'may', 'African', 'Are', 'land', 'in', 'line', 'Well', 'needs', 'this', 'European', 'Where', 'zone', 'ARTHUR', 'minute', 'castle', 'from', 'Please', 'trusty', 'swallows', &quot;'m&quot;, 'then', 'these', 'every', 'simple', 'grip', 'held', 'strand', 'guiding', 'maybe', '...', &quot;'&quot;, 'swallow', 'speak', 'weight', 'Found', 'beat', 'me', 'bangin', 'but', 'King', 'What', 'SCENE', 'are', 'kingdom', 'grips', 'velocity', 'suggesting', 'times', 'Pull', 'of', 'Camelot', 'But', 'The', 'question', 'an', 'Ridden', 'snows', '.', &quot;'ve&quot;, 'No', 'matter', 'at', 'covered', 'servant', 'who', 'order', 'non-migratory', 'them', 'climes', 'Oh', 'feathers', 'it', 'Am', 'air-speed', 'its', 'plover', 'carrying', 'wings', 'Who', 'wind', 'ask', 'knights', 'I', 'bring', 'if', 'not', 'a', 'does', 'yet', 'course', 'coconuts', 'carry', ':', 'right', 'search', 'sun', 'since'}
</code></pre>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-python" data-lang="python"><span style="color:#75715e"># Search for the first occurrence of &#34;coconuts&#34; in scene_one: match</span>
match <span style="color:#f92672">=</span> re<span style="color:#f92672">.</span>search(<span style="color:#e6db74">&#34;coconuts&#34;</span>, scene_one)

<span style="color:#75715e"># Print the start and end indexes of match</span>
<span style="color:#66d9ef">print</span>(match<span style="color:#f92672">.</span>start(), match<span style="color:#f92672">.</span>end())

<span style="color:#75715e"># Write a regular expression to search for anything in square brackets: pattern1</span>
pattern1 <span style="color:#f92672">=</span> <span style="color:#e6db74">r</span><span style="color:#e6db74">&#34;\[.*\]&#34;</span>

<span style="color:#75715e"># Use re.search to find the first text in square brackets</span>
<span style="color:#66d9ef">print</span>(re<span style="color:#f92672">.</span>search(pattern1, scene_one))

<span style="color:#75715e"># Find the script notation at the beginning of the fourth sentence and print it</span>
pattern2 <span style="color:#f92672">=</span> <span style="color:#e6db74">r</span><span style="color:#e6db74">&#34;[\w\s]+:&#34;</span>
<span style="color:#66d9ef">print</span>(re<span style="color:#f92672">.</span>match(pattern2, sentences[<span style="color:#ae81ff">3</span>]))</code></pre></div>
<pre><code>580 588
&lt;_sre.SRE_Match object; span=(9, 32), match='[wind] [clop clop clop]'&gt;
&lt;_sre.SRE_Match object; span=(0, 7), match='ARTHUR:'&gt;
</code></pre>

<h2 id="advanced-tokenisation">Advanced Tokenisation</h2>

<ul>
<li><strong>union</strong> |</li>
<li>grouping ()</li>
<li>explicit char range []</li>
<li>ex:

<ol>
<li>digit-word = <code>('(\d+|\w+)')</code></li>
</ol></li>
<li><strong>range and group</strong>

<ul>
<li><strong>special character</strong> &ldquo;\&rdquo;</li>
<li>[A-Za-z]+ &ldquo;upper/lower alphabet&rdquo;</li>
<li>[0-9] &ldquo;numeric 0-9&rdquo;</li>
<li>[A-Za-z-.]+ &ldquo;upper/lower, - and .&rdquo;</li>
<li>(a-z) &ldquo;a - and z&rdquo; <strong>GROUP</strong></li>
<li>(\s+|,) &ldquo;spaces or comma&rdquo;</li>
</ul></li>
<li>e.g. &lsquo;[a-z0-9 ]+&rsquo; meaning lower, digits, SPACE, greedily -&gt; using .match() will <strong>stop at any punctuation, as not specified</strong></li>
</ul>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-python" data-lang="python">my_string <span style="color:#f92672">=</span> <span style="color:#e6db74">&#34;SOLDIER #1: Found them? In Mercea? The coconut&#39;s tropical!&#34;</span>

<span style="color:#75715e"># to tokenise by words &amp; punctuation &amp; retain #1</span>

<span style="color:#f92672">from</span> nltk.tokenize <span style="color:#f92672">import</span> regexp_tokenize

regexp_tokenize(my_string, <span style="color:#e6db74">&#39;(\w+|#\d|\?|!)&#39;</span>)</code></pre></div>
<pre><code>['SOLDIER',
 '#1',
 'Found',
 'them',
 '?',
 'In',
 'Mercea',
 '?',
 'The',
 'coconut',
 's',
 'tropical',
 '!']
</code></pre>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-python" data-lang="python"><span style="color:#75715e"># Tweet tokenisation</span>
tweets <span style="color:#f92672">=</span> [<span style="color:#e6db74">&#39;This is the best #nlp exercise ive found online! #python&#39;</span>,
 <span style="color:#e6db74">&#39;#NLP is super fun! &lt;3 #learning&#39;</span>,
 <span style="color:#e6db74">&#39;Thanks @datacamp :) #nlp #python&#39;</span>]

<span style="color:#f92672">from</span> nltk.tokenize <span style="color:#f92672">import</span> TweetTokenizer

<span style="color:#75715e"># first use regexp via key symbols</span>

<span style="color:#66d9ef">print</span>(regexp_tokenize(str(tweets), <span style="color:#e6db74">&#39;[@#]\w+&#39;</span>))

<span style="color:#75715e"># then specialised class</span>

tweetTK <span style="color:#f92672">=</span> TweetTokenizer()

all_tokens <span style="color:#f92672">=</span> [tweetTK<span style="color:#f92672">.</span>tokenize(t) <span style="color:#66d9ef">for</span> t <span style="color:#f92672">in</span> tweets]

<span style="color:#66d9ef">print</span>(<span style="color:#e6db74">&#39;</span><span style="color:#ae81ff">\n</span><span style="color:#e6db74">&#39;</span>, all_tokens)</code></pre></div>
<pre><code>['#nlp', '#python', '#NLP', '#learning', '@datacamp', '#nlp', '#python']

 [['This', 'is', 'the', 'best', '#nlp', 'exercise', 'ive', 'found', 'online', '!', '#python'], ['#NLP', 'is', 'super', 'fun', '!', '&lt;3', '#learning'], ['Thanks', '@datacamp', ':)', '#nlp', '#python']]
</code></pre>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-python" data-lang="python"><span style="color:#75715e"># Non-ASCII tokenisation</span>

german_text <span style="color:#f92672">=</span> <span style="color:#e6db74">&#39;Wann gehen wir Pizza essen? 🍕 Und fährst du mit Über? 🚕&#39;</span>

<span style="color:#f92672">from</span> nltk.tokenize <span style="color:#f92672">import</span> word_tokenize

word_tokenize</code></pre></div>
<pre><code>&lt;function nltk.tokenize.word_tokenize(text, language='english', preserve_line=False)&gt;
</code></pre>

<h4 id="unicode-ranges-for-emoji-are">Unicode ranges for emoji are:</h4>

<ul>
<li>(&rsquo;\U0001F300&rsquo;-&rsquo;\U0001F5FF&rsquo;)</li>
<li>(&rsquo;\U0001F600-\U0001F64F&rsquo;)</li>
<li>(&rsquo;\U0001F680-\U0001F6FF&rsquo;)</li>
<li>(&rsquo;\u2600&rsquo;-\u26FF-\u2700-\u27BF&rsquo;)</li>
</ul>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-python" data-lang="python">all_words <span style="color:#f92672">=</span> word_tokenize(german_text)
<span style="color:#66d9ef">print</span>(all_words)

capital_german <span style="color:#f92672">=</span> <span style="color:#e6db74">r</span><span style="color:#e6db74">&#34;[A-ZÜ]\w+&#34;</span>
<span style="color:#66d9ef">print</span>(<span style="color:#e6db74">&#39;</span><span style="color:#ae81ff">\n</span><span style="color:#e6db74">&#39;</span>, regexp_tokenize(german_text, capital_german))

emoji <span style="color:#f92672">=</span> <span style="color:#e6db74">&#34;[&#39;</span><span style="color:#ae81ff">\U0001F300</span><span style="color:#e6db74">-</span><span style="color:#ae81ff">\U0001F5FF</span><span style="color:#e6db74">&#39;|&#39;</span><span style="color:#ae81ff">\U0001F600</span><span style="color:#e6db74">-</span><span style="color:#ae81ff">\U0001F64F</span><span style="color:#e6db74">&#39;|&#39;</span><span style="color:#ae81ff">\U0001F680</span><span style="color:#e6db74">-</span><span style="color:#ae81ff">\U0001F6FF</span><span style="color:#e6db74">&#39;|&#39;</span><span style="color:#ae81ff">\u2600</span><span style="color:#e6db74">-</span><span style="color:#ae81ff">\u26FF\u2700</span><span style="color:#e6db74">-</span><span style="color:#ae81ff">\u27BF</span><span style="color:#e6db74">&#39;]&#34;</span>

<span style="color:#66d9ef">print</span>(<span style="color:#e6db74">&#39;</span><span style="color:#ae81ff">\n</span><span style="color:#e6db74">&#39;</span>, regexp_tokenize(german_text, emoji))</code></pre></div>
<pre><code>['Wann', 'gehen', 'wir', 'Pizza', 'essen', '?', '🍕', 'Und', 'fährst', 'du', 'mit', 'Über', '?', '🚕']

 ['Wann', 'Pizza', 'Und', 'Über']

 ['🍕', '🚕']
</code></pre>

<h2 id="charting-word-length">Charting Word Length</h2>

<ul>
<li>Charting and graphs and animations</li>
<li>Visualise</li>
</ul>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-python" data-lang="python"><span style="color:#66d9ef">with</span> open(<span style="color:#e6db74">&#39;Data_Folder/TxT/grail.txt&#39;</span>, mode<span style="color:#f92672">=</span><span style="color:#e6db74">&#39;r&#39;</span>) <span style="color:#66d9ef">as</span> file:
    grail_text <span style="color:#f92672">=</span> file<span style="color:#f92672">.</span>read()

grail_lines <span style="color:#f92672">=</span> grail_text<span style="color:#f92672">.</span>split(sep<span style="color:#f92672">=</span><span style="color:#e6db74">&#39;</span><span style="color:#ae81ff">\n</span><span style="color:#e6db74">&#39;</span>)

<span style="color:#75715e"># replacing all script lines for speaker or NAME: instances</span>
pattern_speaker <span style="color:#f92672">=</span> <span style="color:#e6db74">&#34;[A-Z]{2,}(\s)?(#\d)?([A-Z]{2,})?:&#34;</span>

grail_lines <span style="color:#f92672">=</span> [re<span style="color:#f92672">.</span>sub(pattern_speaker, <span style="color:#e6db74">&#39;&#39;</span>, line) <span style="color:#66d9ef">for</span> line <span style="color:#f92672">in</span> grail_lines]

grail_tokenised <span style="color:#f92672">=</span> [regexp_tokenize(s, <span style="color:#e6db74">&#34;\w+&#34;</span>) <span style="color:#66d9ef">for</span> s <span style="color:#f92672">in</span> grail_lines]

line_num_words <span style="color:#f92672">=</span> [len(t_line) <span style="color:#66d9ef">for</span> t_line <span style="color:#f92672">in</span> grail_tokenised]

plt<span style="color:#f92672">.</span>hist(line_num_words)
plt<span style="color:#f92672">.</span>show()</code></pre></div>
<pre><code>(array([916., 177.,  52.,  22.,   9.,   4.,   4.,   5.,   1.,   2.]),
 array([  0. ,  10.3,  20.6,  30.9,  41.2,  51.5,  61.8,  72.1,  82.4,
         92.7, 103. ]),
 &lt;a list of 10 Patch objects&gt;)
</code></pre>

<p><img src="output_14_1.png" alt="png" /></p>

<h1 id="bag-of-word-counting">Bag-of-word Counting</h1>

<ul>
<li>Tokenise-Count flow</li>
<li>frequency is a common statistics (max or min)</li>
<li>e.g. lower(all text) to evade duplication</li>
<li>e.g. preprocess by expurgating articles and frivolous words</li>
</ul>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-python" data-lang="python"><span style="color:#66d9ef">with</span> open(<span style="color:#e6db74">&#39;Data_Folder/TxT/wiki_txt2/wiki_text_debugging.txt&#39;</span>, <span style="color:#e6db74">&#39;r&#39;</span>) <span style="color:#66d9ef">as</span> file:
    wiki_debugging <span style="color:#f92672">=</span> file<span style="color:#f92672">.</span>read()

<span style="color:#f92672">from</span> collections <span style="color:#f92672">import</span> Counter

wiki_debugging_TK <span style="color:#f92672">=</span> word_tokenize(wiki_debugging)

wiki_debugging_lower <span style="color:#f92672">=</span> [t<span style="color:#f92672">.</span>lower() <span style="color:#66d9ef">for</span> t <span style="color:#f92672">in</span> wiki_debugging_TK]

bow_wiki_debugging <span style="color:#f92672">=</span> Counter(wiki_debugging_lower)

<span style="color:#66d9ef">print</span>(bow_wiki_debugging<span style="color:#f92672">.</span>most_common(<span style="color:#ae81ff">10</span>))

type(bow_wiki_debugging)</code></pre></div>
<pre><code>[(',', 151), ('the', 150), ('.', 89), ('of', 81), (&quot;''&quot;, 66), ('to', 63), ('a', 60), ('``', 47), ('in', 44), ('and', 41)]





collections.Counter
</code></pre>

<h2 id="text-preprocessing">Text Preprocessing</h2>

<ul>
<li>preparing for ML or analysis</li>
<li>e.g. token, bow, lowercasing, etc.</li>

<li><p><strong>Lemmatisation/Stemming</strong> shortening to root stems</p>

<ul>
<li><p><code>.isalpha()</code></p></li>

<li><p><code>from ntlk.corpus import stopwords</code>
<code>[... if t not in stopwords.words('english')]</code></p></li>
</ul></li>

<li><p>try and error method to context</p></li>
</ul>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-python" data-lang="python"><span style="color:#f92672">from</span> nltk.stem <span style="color:#f92672">import</span> WordNetLemmatizer
<span style="color:#f92672">from</span> nltk.corpus <span style="color:#f92672">import</span> stopwords

alpha_only <span style="color:#f92672">=</span> [t <span style="color:#66d9ef">for</span> t <span style="color:#f92672">in</span> wiki_debugging_lower <span style="color:#66d9ef">if</span> t<span style="color:#f92672">.</span>isalpha()]

english_stops <span style="color:#f92672">=</span> stopwords<span style="color:#f92672">.</span>words(<span style="color:#e6db74">&#39;english&#39;</span>)

no_stops <span style="color:#f92672">=</span> [t <span style="color:#66d9ef">for</span> t <span style="color:#f92672">in</span> alpha_only <span style="color:#66d9ef">if</span> t <span style="color:#f92672">not</span> <span style="color:#f92672">in</span> english_stops]

wordnet_lemmatizer <span style="color:#f92672">=</span> WordNetLemmatizer()

lemmatized <span style="color:#f92672">=</span> [wordnet_lemmatizer<span style="color:#f92672">.</span>lemmatize(t) <span style="color:#66d9ef">for</span> t <span style="color:#f92672">in</span> no_stops]

bow <span style="color:#f92672">=</span> Counter(lemmatized)

<span style="color:#66d9ef">print</span>(bow<span style="color:#f92672">.</span>most_common(<span style="color:#ae81ff">10</span>))</code></pre></div>
<pre><code>[('debugging', 40), ('system', 25), ('bug', 17), ('software', 16), ('problem', 15), ('tool', 15), ('computer', 14), ('process', 13), ('term', 13), ('debugger', 13)]
</code></pre>

<h2 id="gensim">GENSIM</h2>

<ul>
<li>open NLP lib using top academic models to action complex tasks

<ul>
<li>Vectorising doc or word</li>
<li>Topic spotting and comparison</li>
</ul></li>
<li><strong>vector space</strong>, <strong>distance</strong>, <strong>similarity</strong> analysis for semantics and relation</li>
<li>Typically SparseMatrix format, [muse]</li>
<li>Example

<ul>
<li>length(Male-Female) ~= length(King-Queen)</li>
<li>length-diff(verb tenses)</li>
<li>Node and Edge of capital-city pair</li>
</ul></li>

<li><p>In a nutshell, gensim level up token-id-count process of doc or word</p>

<ul>
<li><p><code>from gensim.corpora.dictionary import Dictionary</code></p></li>

<li><p>doc = [&lsquo;list of strings&rsquo;]</p></li>

<li><p>tokenise doc.lower</p></li>

<li><p><code>dict = Dictionary(doc_tk)</code>
<code>dict.token2id</code>
a {} of all token-ID allocation</p></li>

<li><p>build own corpus <code>[dict.doc2bow(doc) for doc in doc_tk]</code></p></li>

<li><p>corpus is now [[(id, count)],[]..]</p></li>
</ul></li>

<li><p><strong>Key</strong></p>

<ol>
<li>ease of storing, updating and reuse</li>
<li>dict is mutable</li>
<li>more advanced and feature rich BOW to used</li>
</ol></li>
</ul>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-python" data-lang="python"><span style="color:#75715e"># loop open files in filepath + pattern</span>

<span style="color:#f92672">import</span> os
<span style="color:#f92672">import</span> glob

article_sum <span style="color:#f92672">=</span> []

<span style="color:#66d9ef">for</span> filepath <span style="color:#f92672">in</span> glob<span style="color:#f92672">.</span>glob(os<span style="color:#f92672">.</span>path<span style="color:#f92672">.</span>join(<span style="color:#e6db74">&#39;Data_Folder/TxT/wiki_txt2/&#39;</span>, <span style="color:#e6db74">&#39;*.txt&#39;</span>)):
    <span style="color:#66d9ef">with</span> open(filepath,<span style="color:#e6db74">&#39;r&#39;</span>) <span style="color:#66d9ef">as</span> file:
        article_raw <span style="color:#f92672">=</span> file<span style="color:#f92672">.</span>read()
        article_lower <span style="color:#f92672">=</span> [t<span style="color:#f92672">.</span>lower() <span style="color:#66d9ef">for</span> t <span style="color:#f92672">in</span> word_tokenize(article_raw)]
        article_alpha <span style="color:#f92672">=</span> [t <span style="color:#66d9ef">for</span> t <span style="color:#f92672">in</span> article_lower <span style="color:#66d9ef">if</span> t<span style="color:#f92672">.</span>isalpha()]
        article_tk <span style="color:#f92672">=</span> [t <span style="color:#66d9ef">for</span> t <span style="color:#f92672">in</span> article_alpha <span style="color:#66d9ef">if</span> t <span style="color:#f92672">not</span> <span style="color:#f92672">in</span> english_stops]
        article_sum<span style="color:#f92672">.</span>append(article_tk)</code></pre></div><div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-python" data-lang="python">len(article_sum)
<span style="color:#66d9ef">for</span> i <span style="color:#f92672">in</span> range(len(article_sum)):
    <span style="color:#66d9ef">print</span>(len(article_sum[i]))</code></pre></div>
<pre><code>12



4095
2894
1051
2963
6947
1984
463
3235
2109
1271
722
3045
</code></pre>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-python" data-lang="python"><span style="color:#f92672">from</span> gensim.corpora.dictionary <span style="color:#f92672">import</span> Dictionary

dictionary <span style="color:#f92672">=</span> Dictionary(article_sum)

computer_id <span style="color:#f92672">=</span> dictionary<span style="color:#f92672">.</span>token2id<span style="color:#f92672">.</span>get(<span style="color:#e6db74">&#34;computer&#34;</span>)

<span style="color:#66d9ef">print</span>(dictionary<span style="color:#f92672">.</span>get(computer_id))

corpus <span style="color:#f92672">=</span> [dictionary<span style="color:#f92672">.</span>doc2bow(article) <span style="color:#66d9ef">for</span> article <span style="color:#f92672">in</span> article_sum]

<span style="color:#66d9ef">print</span>(corpus[<span style="color:#ae81ff">4</span>][:<span style="color:#ae81ff">10</span>]) <span style="color:#75715e"># 5th doc first 10 mots</span></code></pre></div>
<pre><code>computer
[(4, 1), (6, 6), (7, 2), (9, 5), (18, 1), (19, 1), (20, 1), (22, 1), (24, 2), (28, 3)]
</code></pre>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-python" data-lang="python">doc_fifth <span style="color:#f92672">=</span> corpus[<span style="color:#ae81ff">4</span>]

doc_fifth[:<span style="color:#ae81ff">4</span>] <span style="color:#75715e"># List of tuples (id, count)</span>

<span style="color:#75715e"># sort doc by frequency, or count</span>
bow_doc_fifth <span style="color:#f92672">=</span> sorted(doc_fifth,
                      key <span style="color:#f92672">=</span> <span style="color:#66d9ef">lambda</span> w: w[<span style="color:#ae81ff">1</span>],
                      reverse<span style="color:#f92672">=</span>True)

<span style="color:#75715e"># print top-5 (id + count)</span>
<span style="color:#66d9ef">for</span> id, count <span style="color:#f92672">in</span> bow_doc_fifth[:<span style="color:#ae81ff">5</span>]:
    <span style="color:#66d9ef">print</span>(dictionary<span style="color:#f92672">.</span>get(id), count)

<span style="color:#f92672">from</span> collections <span style="color:#f92672">import</span> defaultdict

total_word_count <span style="color:#f92672">=</span> defaultdict(int)

<span style="color:#f92672">import</span> itertools

<span style="color:#66d9ef">for</span> id, count <span style="color:#f92672">in</span> itertools<span style="color:#f92672">.</span>chain<span style="color:#f92672">.</span>from_iterable(corpus):
    total_word_count[id] <span style="color:#f92672">+=</span> count

sorted_word_count <span style="color:#f92672">=</span> sorted(total_word_count<span style="color:#f92672">.</span>items(), key<span style="color:#f92672">=</span><span style="color:#66d9ef">lambda</span> w: w[<span style="color:#ae81ff">1</span>], reverse<span style="color:#f92672">=</span>True)

<span style="color:#66d9ef">for</span> id, count <span style="color:#f92672">in</span> sorted_word_count[:<span style="color:#ae81ff">5</span>]:
    <span style="color:#66d9ef">print</span>(dictionary<span style="color:#f92672">.</span>get(id), count)</code></pre></div>
<pre><code>[(4, 1), (6, 6), (7, 2), (9, 5)]



computer 251
computers 100
first 61
cite 59
computing 59
computer 597
software 450
cite 322
ref 259
code 235
</code></pre>

<h1 id="tf-idf-gensim">Tf-idf + Gensim</h1>

<ul>
<li><strong>Term Frequency - Inverse Document Frequency</strong></li>
<li>Common model used to id importance in each document <strong>from the corpus</strong></li>
<li>Logic: each corpus may have shared words beyond just stopwords

<ul>
<li>down-weighted importance of context-word</li>
<li>e.g. astronomy: &lsquo;Sky&rsquo;</li>
<li>dismissing context-adjusted words</li>
<li>up-weighted specific frequency</li>
</ul></li>

<li><p>Function</p>

<h3 id="w-i-j-tf-i-j-log-frac-n-df-i">$w<em>{i,j} = tf</em>{i,j} * \log(\frac{N}{df_i})$</h3>

<ul>
<li>$w_{i,j}$ = tf-idf weight for token i in doc j</li>
<li>$tf_{i,j}$ = # occurences of token i in doc j</li>
<li>$df_i$ = # doc containing token i</li>
<li>N = total # of doc</li>
</ul></li>

<li><p>E.g. &lsquo;computer&rsquo; appears 5 times in a doc of 100 words; what&rsquo;s weight given a corpus of 200 doc of which 20 doc mentioned the word</p>

<ul>
<li>(<sup>5</sup>&frasl;<sub>100</sub>) * log(<sup>200</sup>&frasl;<sub>20</sub>)</li>
<li>tf = percentage share of word compared to all tokens in the doc idf = log(total doc / contained doc)</li>
</ul></li>
</ul>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-python" data-lang="python"><span style="color:#f92672">from</span> gensim.models.tfidfmodel <span style="color:#f92672">import</span> TfidfModel

tfidf <span style="color:#f92672">=</span> TfidfModel(corpus)

<span style="color:#75715e"># strange apply of TfidfModel() instance</span>
tfidf_weights <span style="color:#f92672">=</span> tfidf[doc_fifth]

<span style="color:#66d9ef">print</span>(tfidf_weights[:<span style="color:#ae81ff">5</span>],<span style="color:#e6db74">&#39;</span><span style="color:#ae81ff">\n</span><span style="color:#e6db74">&#39;</span>)

<span style="color:#75715e"># sort weights highest to lowest</span>
sorted_tfidf_weights <span style="color:#f92672">=</span> sorted(tfidf_weights, key<span style="color:#f92672">=</span><span style="color:#66d9ef">lambda</span> w: w[<span style="color:#ae81ff">1</span>], reverse<span style="color:#f92672">=</span>True)

<span style="color:#66d9ef">for</span> id, weight <span style="color:#f92672">in</span> sorted_tfidf_weights[:<span style="color:#ae81ff">5</span>]:
    <span style="color:#66d9ef">print</span>(dictionary<span style="color:#f92672">.</span>get(id), weight)</code></pre></div>
<pre><code>[(4, 0.005117037137639146), (6, 0.005095225240405224), (7, 0.00815539037400173), (9, 0.02558518568819573), (18, 0.003228490980266662)] 

mechanical 0.1836016185939278
circuit 0.15046224827624213
manchester 0.14187397800439872
alu 0.13888822917806967
thomson 0.12731421007989718
</code></pre>

<h1 id="named-entity-recognition">Named Entity Recognition</h1>

<ul>
<li>Motif: NLP task to identify important NE in the text

<ol>
<li>people, places, organisations</li>
<li>dates, states, nouns</li>
</ol></li>

<li><p>Used alongside topic identification</p>

<h2 id="stanford-corenlp-library">Stanford CoreNLP Library</h2></li>

<li><p>Integrated into Python via nltk</p>

<ul>
<li>Java based</li>
<li>API able</li>
<li>Support for NER + coReference and dependency trees</li>
</ul></li>

<li><p>Built-in pos_tag, etc in nltk</p></li>
</ul>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-python" data-lang="python"><span style="color:#66d9ef">with</span> open(<span style="color:#e6db74">&#39;Data_Folder/TxT/News articles/uber_apple.txt&#39;</span>,<span style="color:#e6db74">&#39;r&#39;</span>) <span style="color:#66d9ef">as</span> file:
    article_uber <span style="color:#f92672">=</span> file<span style="color:#f92672">.</span>read()

<span style="color:#f92672">import</span> nltk

<span style="color:#75715e"># first tk article into sentences</span>
sentence_uber <span style="color:#f92672">=</span> sent_tokenize(article_uber)

<span style="color:#75715e"># second tk sentences into words</span>
sentence_uber_tk <span style="color:#f92672">=</span> [word_tokenize(sent) <span style="color:#66d9ef">for</span> sent <span style="color:#f92672">in</span> sentence_uber]

<span style="color:#75715e"># tag words into speech-part </span>
pos_sentences <span style="color:#f92672">=</span> [nltk<span style="color:#f92672">.</span>pos_tag(sent) <span style="color:#66d9ef">for</span> sent <span style="color:#f92672">in</span> sentence_uber_tk]

<span style="color:#75715e"># Create Named Entity chunks</span>
chunked_sentences <span style="color:#f92672">=</span> nltk<span style="color:#f92672">.</span>ne_chunk_sents(pos_sentences, binary<span style="color:#f92672">=</span>True)

<span style="color:#75715e"># Test for stems of Tree with &#39;NE&#39; tags</span>

<span style="color:#66d9ef">for</span> sent <span style="color:#f92672">in</span> chunked_sentences:
    <span style="color:#66d9ef">for</span> chunk <span style="color:#f92672">in</span> sent:
        <span style="color:#66d9ef">if</span> hasattr(chunk, <span style="color:#e6db74">&#34;label&#34;</span>) <span style="color:#f92672">and</span> chunk<span style="color:#f92672">.</span>label() <span style="color:#f92672">==</span> <span style="color:#e6db74">&#34;NE&#34;</span>:
            <span style="color:#66d9ef">print</span>(chunk)</code></pre></div>
<pre><code>(NE Uber/NNP)
(NE Beyond/NN)
(NE Apple/NNP)
(NE Uber/NNP)
(NE Uber/NNP)
(NE Travis/NNP Kalanick/NNP)
(NE Tim/NNP Cook/NNP)
(NE Apple/NNP)
(NE Silicon/NNP Valley/NNP)
(NE CEO/NNP)
(NE Yahoo/NNP)
(NE Marissa/NNP Mayer/NNP)
</code></pre>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-python" data-lang="python"><span style="color:#75715e"># Charting NE</span>

ner_categories <span style="color:#f92672">=</span> defaultdict(int)

<span style="color:#75715e"># loading a new article in non-binary form</span>
<span style="color:#66d9ef">with</span> open(<span style="color:#e6db74">&#39;Data_Folder/TxT/News articles/articles.txt&#39;</span>, <span style="color:#e6db74">&#39;r&#39;</span>) <span style="color:#66d9ef">as</span> file:
    article_nonBinary <span style="color:#f92672">=</span> file<span style="color:#f92672">.</span>read()

chunked_nonBinary <span style="color:#f92672">=</span> nltk<span style="color:#f92672">.</span>ne_chunk_sents(
    [nltk<span style="color:#f92672">.</span>pos_tag(sent) <span style="color:#66d9ef">for</span> sent <span style="color:#f92672">in</span> 
     [word_tokenize(sent) <span style="color:#66d9ef">for</span> sent <span style="color:#f92672">in</span> 
      sent_tokenize(article_nonBinary)]], binary<span style="color:#f92672">=</span>False)

<span style="color:#66d9ef">for</span> sent <span style="color:#f92672">in</span> chunked_nonBinary:
    <span style="color:#66d9ef">for</span> chunk <span style="color:#f92672">in</span> sent:
        <span style="color:#66d9ef">if</span> hasattr(chunk, <span style="color:#e6db74">&#34;label&#34;</span>):
            ner_categories[chunk<span style="color:#f92672">.</span>label()] <span style="color:#f92672">+=</span> <span style="color:#ae81ff">1</span>

labels <span style="color:#f92672">=</span> list(ner_categories<span style="color:#f92672">.</span>keys())

values <span style="color:#f92672">=</span> [ner_categories<span style="color:#f92672">.</span>get(l) <span style="color:#66d9ef">for</span> l <span style="color:#f92672">in</span> labels]

plt<span style="color:#f92672">.</span>pie(values, labels<span style="color:#f92672">=</span>labels, autopct<span style="color:#f92672">=</span><span style="color:#e6db74">&#39;</span><span style="color:#e6db74">%1.1f%%</span><span style="color:#e6db74">&#39;</span>, startangle<span style="color:#f92672">=</span><span style="color:#ae81ff">140</span>)

plt<span style="color:#f92672">.</span>show()</code></pre></div>
<pre><code>([&lt;matplotlib.patches.Wedge at 0x1a2bd896a0&gt;,
  &lt;matplotlib.patches.Wedge at 0x1a2bd89da0&gt;,
  &lt;matplotlib.patches.Wedge at 0x1a2bd8e4e0&gt;,
  &lt;matplotlib.patches.Wedge at 0x1a2bd8ebe0&gt;,
  &lt;matplotlib.patches.Wedge at 0x1a2bd58320&gt;],
 [Text(-1.07181,0.247446,'ORGANIZATION'),
  Text(0.341578,-1.04562,'GPE'),
  Text(0.201317,1.08142,'PERSON'),
  Text(-0.811594,0.742506,'LOCATION'),
  Text(-0.832466,0.719027,'FACILITY')],
 [Text(-0.584622,0.134971,'15.0%'),
  Text(0.186315,-0.570339,'52.3%'),
  Text(0.109809,0.589866,'31.8%'),
  Text(-0.442688,0.405003,'0.5%'),
  Text(-0.454073,0.392197,'0.5%')])
</code></pre>

<p><img src="output_28_1.png" alt="png" /></p>

<h1 id="spacy-nlp-library-similar-to-gensim">SpaCy - NLP library similar to Gensim</h1>

<ul>
<li>Focus on creating pipeline to generate models and corpora</li>
<li>Focus on GTD not academic - ONLY ONE NER per langue, no-frills!</li>
<li>So what&rsquo;s <strong>academic</strong> stuff?

<ul>
<li>Focus not Edge-Cutting algo, <strong>NLTK</strong> focus on giving scholars toolkit to play around</li>
</ul></li>
<li>FEATURES OF SPACY

<ol>
<li>Non-destructive tokenisation</li>
<li>21+ langues</li>
<li>6 statsmodels for 5 langues</li>
<li>pre-trained WordVEC</li>
<li>esy DeepLearning integration</li>
<li>POS tagging</li>
<li>NER</li>
<li>Labeled DEPENDENCY parsing</li>
<li>Syntax-drven sentence segmentaion</li>
<li>Built-in visual for syntax and NER</li>
<li>easy string-to-hash mapping</li>
<li>export to NPArray</li>
<li>Efficient binary SERIALISATION</li>
<li>easy model pkg and deployment</li>
<li>Speed</li>
<li>Robust, rigorously evaluated accuracy</li>
</ol></li>
<li>e.g. Visualiser online by <code>Displacy</code></li>
<li>NER to load</li>

<li><p>Including advanced German and Chinese</p>

<h2 id="why-spacy-for-ner">Why SpaCy for NER</h2></li>

<li><p>easy pipelineing</p></li>

<li><p>different entity types</p></li>

<li><p>informal corpora - tweets chat</p></li>

<li><p>quicly growing</p></li>
</ul>

<h2 id="language-model-stats-model-for-nlp-tasks">Language Model - stats model for NLP tasks</h2>

<ul>
<li>need download separately</li>
<li>language specific</li>
<li>installation</li>
</ul>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-bash" data-lang="bash">spacy download en / de / es / fr / xx <span style="color:#75715e"># multi-langue
</span><span style="color:#75715e"></span>spacy download en_core_web_sm # best mactching version of specific model <span style="color:#66d9ef">for</span> spacy</code></pre></div>
<ul>
<li>Loading language model</li>
</ul>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-python" data-lang="python"><span style="color:#f92672">import</span> spacy
nlp <span style="color:#f92672">=</span> spacy<span style="color:#f92672">.</span>load(<span style="color:#e6db74">&#39;en&#39;</span>)
<span style="color:#75715e"># process text via pipeline</span>
doc <span style="color:#f92672">=</span> nlp(<span style="color:#e6db74">u</span><span style="color:#e6db74">&#39;This is a sentence.&#39;</span>) <span style="color:#75715e"># u can be ignore in Python3</span></code></pre></div>
<h3 id="basic-cleaning-by-spacy">Basic Cleaning by SpaCy</h3>

<blockquote>
<p>calling above on U-text spaCy first TOKENISES text to make a Doc Project, then processed in 7 steps
1. tokenizer
2. tensoriser
3. tagger
4. parser
5. NER
6. output Doc</p>
</blockquote>

<h3 id="tokenizing-text">Tokenizing Text</h3>

<ul>
<li>splitting text into meaningful tokens/parts - words, puncs, numbers, special char, building blocks</li>
<li>More than .split() - syntax-aware split, don&rsquo;t or U.K.

<ul>
<li>&ldquo;don&rsquo;t&rdquo; -&gt; 2 token {ORTH: &ldquo;do&rdquo;} and {&ldquo;ORTH&rdquo;: &ldquo;n&rsquo;t&rdquo;, LEMMA: &ldquo;not&rdquo;}</li>
<li>ORTH refers to textual content, LEMMA the word with no information suffix</li>
</ul></li>
<li>Creating own Tokenisers in <a href="https://spacy.io/usage/linguistic-features#section-tokenization">Linguisitic Features</a></li>
<li>Once done, Doc obj comprising tokens each is then worked on by other components of the PIPELINE</li>
</ul>

<h3 id="pos-tagging-tensorizer">POS tagging - <strong>tensorizer</strong></h3>

<ul>
<li>encode internal repr of doc as ARRAY of floats, necessary for NN need tensors</li>
<li>Mark token of sentence with proper part of speech</li>
<li>use Stats Models to perform POS tagging</li>
</ul>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-python" data-lang="python"><span style="color:#66d9ef">for</span> token <span style="color:#f92672">in</span> doc:
    token<span style="color:#f92672">.</span>text, token<span style="color:#f92672">.</span>pos_</code></pre></div>
<h3 id="dependency-parsing">Dependency Parsing</h3>

<ul>
<li>while parsing refers to any analysis of string of symbols to understand relationship, dependency parsing emphasises on DEPENDENCY</li>
<li>Subject or Object Noun</li>
</ul>

<h3 id="ner">NER</h3>

<ul>
<li>real-world object with name</li>
<li>spacy built-in training but may need tuning/training</li>
</ul>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-python" data-lang="python"><span style="color:#66d9ef">for</span> ent <span style="color:#f92672">in</span> doc<span style="color:#f92672">.</span>ents:
    ent<span style="color:#f92672">.</span>text, ent<span style="color:#f92672">.</span>start_char, ent<span style="color:#f92672">.</span>end_char, ent<span style="color:#f92672">.</span>label_</code></pre></div>
<ul>
<li>Built-int NE types
<strong>PERSON, NORP, FACILITY, ORG, GPE, LOC, PRODUCT, EVENT, WORK_OF_ART, LAW, LANGUAGE</strong></li>
</ul>

<h3 id="rule-based-matching">Rule-based matching</h3>

<p><strong>ORTH, LOWER,UPPER, IS_ALPHA, IS_ASCII, IS_DIGIT, IS_PUNCT, IS_SPACE, IS_STOP, LIKE_NUM, LIKE_URL, LIKE_EMAIL, POS, TG, DEP, LEMMA, SHAPE</strong>
- default pipeline perform further annotating tokens with more info
- self-defined rule is possible</p>

<h3 id="preprocessing">Preprocessing</h3>

<ul>
<li>stop-word by <code>token.IS_STOP</code> attribute boolean</li>
<li>self-defined stoppers</li>
</ul>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-python" data-lang="python">my_stops <span style="color:#f92672">=</span> [<span style="color:#e6db74">&#39;say&#39;</span>, <span style="color:#e6db74">&#39;be&#39;</span>, <span style="color:#e6db74">&#39;said&#39;</span>, <span style="color:#e6db74">&#39;says&#39;</span>, <span style="color:#e6db74">&#39;saying&#39;</span>, <span style="color:#e6db74">&#39;field&#39;</span>]
<span style="color:#66d9ef">for</span> stopword <span style="color:#f92672">in</span> my_stops:
    lexeme <span style="color:#f92672">=</span> nlp<span style="color:#f92672">.</span>vocab[stopword]
    lexeme<span style="color:#f92672">.</span>is_stop <span style="color:#f92672">=</span> True

<span style="color:#75715e"># alternatively</span>
<span style="color:#f92672">from</span> spacy.lang.en.stop_words <span style="color:#f92672">import</span> STOP_WORDS

<span style="color:#66d9ef">print</span>(STOP_WORDS)
STOP_WORDS<span style="color:#f92672">.</span>add(<span style="color:#e6db74">&#34;additioanl here&#34;</span>)</code></pre></div>
<ul>
<li><p>STEMMING &amp; LEMMATISATION</p>

<ul>
<li>Stemming often chops off end of word following basic rules / CONTEXTLESS not POS-based</li>
<li>Lemmatisation however conducts MORPHOLOGICAL analysis to find root word</li>
<li>Stanford NLP book explains</li>
<li>lemma accessed <code>.lemma_</code> attribute</li>
</ul></li>

<li><p>Basic cleaning</p></li>
</ul>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-python" data-lang="python">doc <span style="color:#f92672">=</span> nlp(<span style="color:#e6db74">&#39;some text&#39;</span>)
sentence <span style="color:#f92672">=</span> []
<span style="color:#66d9ef">for</span> w <span style="color:#f92672">in</span> doc:
    <span style="color:#66d9ef">if</span> w<span style="color:#f92672">.</span>text <span style="color:#f92672">!=</span> <span style="color:#e6db74">&#39;n&#39;</span> <span style="color:#f92672">and</span> <span style="color:#f92672">not</span> w<span style="color:#f92672">.</span>is_stop <span style="color:#f92672">and</span> <span style="color:#f92672">not</span> w<span style="color:#f92672">.</span>is_punct <span style="color:#f92672">and</span> <span style="color:#f92672">not</span> w<span style="color:#f92672">.</span>like_num:
        sentence<span style="color:#f92672">.</span>append(w<span style="color:#f92672">.</span>lemma_)
<span style="color:#66d9ef">print</span>(sentence)</code></pre></div>
<ul>
<li>removing trash NOTR appending lemmatised form of word !!</li>
<li>further remove based on need, e.g. <strong>removing all VERB via checking POS tag of token !</strong></li>
</ul>

<blockquote>
<p>RECAP: spaCy pipeline annotates text easily to retain info to process analysis, always first starting task in NLP
1. possible annotating text with LOTs of info (tokenisation, stoppers, POS, NER, etc)
2. possible TRAINING annotationg models on own, power to language models and processing pipeline!</p>
</blockquote>

<h2 id="gensim-vectorising-text-and-n-grams">GENSIM - Vectorising Text and N-grams</h2>

<ul>
<li>VECTORISING TEXT - BOW, TF-IDF, LSI (latent semantic indexing), WORD2VEC
&gt; GENSIM included novel kits like LDA (Latent Dirichlet allocation), Latent Semantic Analysis, Random projection, Hierarchical Dirichlet process, word2vec deep learning, cluster computing</li>
<li>Memory-efficient, scalable (generators/iterators, most IR algo ~ Matrix Decompo)</li>
<li><a href="https://github.com/RaRe-Technologies/gensim/tre/develop/docs/notebooks">Documentation</a></li>
</ul>

<h2 id="vectorised-word">Vectorised Word</h2>

<h3 id="bow-most-straightforward">BOW - most straightforward</h3>

<ul>
<li>Word-Freq mapped against VOCAB</li>
<li><strong>OrderLESS</strong> NO <strong>Spatial INFO</strong> - or semantics</li>
<li>BUT many cases no need for Spatial INFO in Information Retrieval Algo</li>
<li>EX: spam filtering via <strong>Naive Bayes Classifier</strong></li>
</ul>

<h3 id="tf-idf">TF-IDF</h3>

<ul>
<li>Largely used in search engines to find relevant docs based on query</li>
<li>Weigthed Frequency against Occurences</li>
</ul>

<h3 id="other-forms">Other Forms</h3>

<ul>
<li>Topic Models</li>
<li><a href="https://blog.acolyer.org/2016/04/21/the-amazing-power-of-word-vectors/">The Amazing Power of Word Vectors</a></li>
</ul>

<h3 id="vectorisation-in-gensim">Vectorisation in GENSIM</h3>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-python" data-lang="python"><span style="color:#f92672">from</span> gensim <span style="color:#f92672">import</span> corpora
documents <span style="color:#f92672">=</span> [<span style="color:#e6db74">u</span><span style="color:#e6db74">&#39;list of text&#39;</span>]
<span style="color:#f92672">import</span> spacy
nlp <span style="color:#f92672">=</span> spacy<span style="color:#f92672">.</span>load(<span style="color:#e6db74">&#39;en&#39;</span>)
texts <span style="color:#f92672">=</span> []
<span style="color:#66d9ef">for</span> document <span style="color:#f92672">in</span> documents:
    text <span style="color:#f92672">=</span> []
    doc <span style="color:#f92672">=</span> nlp(document)
    <span style="color:#66d9ef">for</span> w <span style="color:#f92672">in</span> doc:
        <span style="color:#66d9ef">if</span> <span style="color:#f92672">not</span> w<span style="color:#f92672">.</span>is_stop <span style="color:#f92672">and</span> <span style="color:#f92672">not</span> w<span style="color:#f92672">.</span>is_punct <span style="color:#f92672">and</span> <span style="color:#f92672">not</span> w<span style="color:#f92672">.</span>like_num:
            text<span style="color:#f92672">.</span>append(w<span style="color:#f92672">.</span>lemma_)
    texts<span style="color:#f92672">.</span>append(text)
<span style="color:#66d9ef">print</span>(texts)

<span style="color:#75715e"># whipping up BOW for mini-corpus</span>
dictionary <span style="color:#f92672">=</span> corpora<span style="color:#f92672">.</span>Dictionary(texts)
<span style="color:#66d9ef">print</span>(dictionary<span style="color:#f92672">.</span>token2id)

corpus <span style="color:#f92672">=</span> [dictionary<span style="color:#f92672">.</span>doc2bow(text) <span style="color:#66d9ef">for</span> text <span style="color:#f92672">in</span> texts]</code></pre></div>
<ul>
<li>A List of List each repr BOW <code>(word_id, word_count)</code> a tuple</li>
<li>NOTE

<ol>
<li>Above case is fully RAM-loaded, in production need to store corpus
<code>python
corpora.MmCorpus.serialize('/tmp/example.mm', corpus)
</code></li>
<li>Thus stored on disk away from RAM, at most one vector resides in RAM (<a href="https://radimrehurek.com/gensim/tut1.html">tutorial</a>)</li>

<li><p>From BOW into TF-IDF: tfidf is a table TRAINED on own corpus, involving simply going through supplied corpus \n once and computing df on all features (other models, LSA or LDA, much more invovled)</p>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-python" data-lang="python"><span style="color:#f92672">from</span> gensim <span style="color:#f92672">import</span> models
tfidf <span style="color:#f92672">=</span> models<span style="color:#f92672">.</span>TfidfModel(corpus)
<span style="color:#75715e"># Check the result</span>
<span style="color:#66d9ef">for</span> document <span style="color:#f92672">in</span> tfidf[corpus]:
    <span style="color:#66d9ef">print</span>(document)</code></pre></div></li>

<li><p>Score (0,1) measuring importance of word in corpus - used in ML models or further chain/link vectors by performing other transformation on them</p></li>
</ol></li>
</ul>

<h4 id="n-gram-plus-more-preprocessing">N-gram plus more preprocessing</h4>

<ul>
<li>Calculated by conditional proba around words called <strong>collocation</strong> based on corpus provided</li>
<li>This extra preprocess precedes <strong>DOC2BOW</strong> dictionary!</li>
</ul>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-python" data-lang="python">bigram <span style="color:#f92672">=</span> gensim<span style="color:#f92672">.</span>models<span style="color:#f92672">.</span>Phrases(texts) <span style="color:#75715e"># resulting a trained bi-gram model for corpus, then transformation on new text</span>
texts <span style="color:#f92672">=</span> [bigram[line] <span style="color:#66d9ef">for</span> line <span style="color:#f92672">in</span> texts] <span style="color:#75715e"># each line having all possible bi-grams created</span></code></pre></div>
<h3 id="recap">RECAP</h3>

<ul>
<li>Preprocess can be as complex as need dictates</li>
<li>EX Removing both high-freq and low-freq words (GENSIM <code>dictionary</code> module)
&gt; Rid of occurence &lt; 20 documents, or in &gt; 50% of documents
<code>python
dictionary.filter_extremes(no_below=20, no_above=0.5)
</code></li>
<li>EX remove most-freq tokens or prune out certain token IDs <a href="https://radimrehurek.com/gensim/corpora/dictionary.html">example</a></li>
</ul>

<h2 id="pos-tagging-and-applications">POS-Tagging and Applications</h2>

<ul>
<li>What

<ol>
<li>Not possible to tag POS unless in sentence or phrase</li>
<li>SpaCy has 19 POS tags <code>.tag_</code> attr and <code>.pos_</code></li>
<li>Brown Corpus used HMM to predict tags (HMM - sequentail model)</li>
</ol></li>
<li>Current

<ol>
<li>Stats model and Deep Learning <a href="https://aclweb.org/aclwiki/POS_Tagging_(State_of_the_art)">ACL list of results</a></li>
<li>spaCy early tagger is <strong>averaged perceptron</strong></li>
</ol></li>
<li>Why

<ol>
<li>Historically speech-to-text conversion / translation disambiguate homonyms</li>
<li>Dependency parsing</li>
<li>Demo by <a href="https://explosion.ai/demos/display">SpaCy Display</a></li>
</ol></li>
<li>PYTHONIC

<ol>
<li>NLTK the main rival POS-tagger
<code>python
import nltk
text = nltk.word_tokenize(&quot;And now for something completely different&quot;)
nltk.pos_tag(text)
bigram_tagger = nltk.BigramTagger(train_sents) # one of many tagger options in NLTK
bigram_tagger.tag(text)
</code></li>
<li>Resources <a href="https://www.nltk.org/api/nltk.tag.html">Official Doc of tag module</a>, <a href="https://www.nltk.org/book/ch05.html">NLTK book</a>, <a href="https://textminingonline.com/dive-into-nltk-part-iii-part-of-speech-tagging-and-pos-tagger">Training POS</a></li>
<li>Other Modules <a href="https://medium.com/@brianray_7981/ai-in-practice-identifying-parts-of-speech-in-python-8a690c7a1a08">AI in Practice: Identifying Parts of Speech in Python</a>

<ul>
<li>TEXTBLOB is likely the ONLY other POS tagger worth a look, performing similarly to one in SpaCy - algo written by spaCy maintainer <a href="https://stevenloria.com/pos-tagging/">Detail</a></li>
</ul></li>
</ol></li>
</ul>

<h4 id="pos-tagging-in-spacy-97-accuracy-battery-packed">POS-Tagging in SpaCy (97% accuracy battery-packed)</h4>

<ul>
<li>Built-in tagging in PIPELINE (from spacy.load(&lsquo;en&rsquo;) and loading text, <code>for token in sent: token.text, token.pos_, token.tag_</code></li>
<li>EX <strong>fishy</strong> a tricky word possible multi-POS but correctly machine-learned by multiple FEATURES (e.g. surrending POS, suffix-prefix, etc.)</li>
<li>SpaCy returns <strong>kills many matephorical birds</strong> with the same stone!</li>
</ul>

<h4 id="adding-defined-training-models">Adding Defined Training Models</h4>

<ul>
<li>Probabilistically improvable to relevant context and data - <a href="https://spacy.io/usage/training">Official SpaCy Traininng process</a></li>
<li>Training Loop:

<ol>
<li>Provide text + part to train (entities, heads, deps, tags, cats)</li>
</ol></li>
</ul>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-python" data-lang="python">TRAIN_DATA <span style="color:#f92672">=</span> [
    (<span style="color:#e6db74">&#34;Facebook has been accused for leaking personal data of users.&#34;</span>, {<span style="color:#e6db74">&#34;entities&#34;</span>: [(<span style="color:#ae81ff">0</span>, <span style="color:#ae81ff">8</span>, <span style="color:#e6db74">&#39;ORG&#39;</span>)]}),
    <span style="color:#f92672">...</span>] <span style="color:#75715e"># Facebook is the entity marked as ORG (start_index, end_index)</span>
nlp <span style="color:#f92672">=</span> spacy<span style="color:#f92672">.</span>blank(<span style="color:#e6db74">&#39;en&#39;</span>)
optimizer <span style="color:#f92672">=</span> nlp<span style="color:#f92672">.</span>begin_training()
<span style="color:#66d9ef">for</span> i <span style="color:#f92672">in</span> range(<span style="color:#ae81ff">20</span>):
    random<span style="color:#f92672">.</span>shuffle(TRAIN_DATA)
    <span style="color:#66d9ef">for</span> text, annotations <span style="color:#f92672">in</span> TRAIN_DATA:
        nlp<span style="color:#f92672">.</span>update([text], [annotateions], sgd <span style="color:#f92672">=</span> optimizer)
nlp<span style="color:#f92672">.</span>to_disk(<span style="color:#e6db74">&#39;/model&#39;</span>)</code></pre></div>
<ul>
<li>Trainng POS-tagger <a href="https://github.com/explosion/spacy/blob/master/examples/training/train_tagger.py">example code</a>

<ol>
<li>init dictionary, define mapping from data&rsquo;s POS to <a href="http://universaldependencies.org/docs/u/pos/index.html">Universsal POS tag set</a></li>
<li>More data better accuracy&hellip;</li>
</ol></li>
</ul>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-python" data-lang="python"><span style="color:#75715e"># nltk for POS-tagging</span>

<span style="color:#f92672">import</span> nltk
text <span style="color:#f92672">=</span> word_tokenize(<span style="color:#e6db74">&#34;And now for something completely different&#34;</span>)
nltk<span style="color:#f92672">.</span>pos_tag(text)

bigram_tagger <span style="color:#f92672">=</span> nltk<span style="color:#f92672">.</span>BigramTagger(train_sents)
bigram_tagger<span style="color:#f92672">.</span>tag(text)

<span style="color:#f92672">import</span> spacy
nlp <span style="color:#f92672">=</span> spacy<span style="color:#f92672">.</span>load(<span style="color:#e6db74">&#39;en&#39;</span>)
sent_0 <span style="color:#f92672">=</span> nlp(<span style="color:#e6db74">u</span><span style="color:#e6db74">&#39;Mathieu and I went to the park.&#39;</span>)
sent_1 <span style="color:#f92672">=</span> nlp(<span style="color:#e6db74">u</span><span style="color:#e6db74">&#39;If Clement was asked to take out the garbage, he would refuse.&#39;</span>)
sent_2 <span style="color:#f92672">=</span> nlp(<span style="color:#e6db74">u</span><span style="color:#e6db74">&#39;Baptiste was in charge of the refuse treatment center.&#39;</span>)
sent_3 <span style="color:#f92672">=</span> nlp(<span style="color:#e6db74">u</span><span style="color:#e6db74">&#39;Marie took out her rather suspicious and fishy cat to go fish for fish.&#39;</span>)

<span style="color:#66d9ef">for</span> token <span style="color:#f92672">in</span> sent_0:
    <span style="color:#66d9ef">print</span>(token<span style="color:#f92672">.</span>text, token<span style="color:#f92672">.</span>pos_, token<span style="color:#f92672">.</span>tag_)

<span style="color:#66d9ef">for</span> token <span style="color:#f92672">in</span> sent_1:
    <span style="color:#66d9ef">print</span>(token<span style="color:#f92672">.</span>text, token<span style="color:#f92672">.</span>pos_, token<span style="color:#f92672">.</span>tag_)

<span style="color:#66d9ef">for</span> token <span style="color:#f92672">in</span> sent_2:
    <span style="color:#66d9ef">print</span>(token<span style="color:#f92672">.</span>text, token<span style="color:#f92672">.</span>pos_, token<span style="color:#f92672">.</span>tag_)

<span style="color:#66d9ef">for</span> token <span style="color:#f92672">in</span> sent_3:
    <span style="color:#66d9ef">print</span>(token<span style="color:#f92672">.</span>text, token<span style="color:#f92672">.</span>pos_, token<span style="color:#f92672">.</span>tag_)

<span style="color:#75715e"># training NER</span>

TRAIN_DATA <span style="color:#f92672">=</span> [
     (<span style="color:#e6db74">&#34;Facebook has been accused for leaking personal data of users.&#34;</span>, {<span style="color:#e6db74">&#39;entities&#39;</span>: [(<span style="color:#ae81ff">0</span>, <span style="color:#ae81ff">8</span>, <span style="color:#e6db74">&#39;ORG&#39;</span>)]}),
     (<span style="color:#e6db74">&#34;Tinder uses sophisticated algorithms to find the perfect match.&#34;</span>, {<span style="color:#e6db74">&#39;entities&#39;</span>: [(<span style="color:#ae81ff">0</span>, <span style="color:#ae81ff">6</span>, <span style="color:#e6db74">&#34;ORG&#34;</span>)]})]

nlp <span style="color:#f92672">=</span> spacy<span style="color:#f92672">.</span>blank(<span style="color:#e6db74">&#39;en&#39;</span>)
optimizer <span style="color:#f92672">=</span> nlp<span style="color:#f92672">.</span>begin_training()
<span style="color:#66d9ef">for</span> i <span style="color:#f92672">in</span> range(<span style="color:#ae81ff">20</span>):
    random<span style="color:#f92672">.</span>shuffle(TRAIN_DATA)
    <span style="color:#66d9ef">for</span> text, annotations <span style="color:#f92672">in</span> TRAIN_DATA:
        nlp<span style="color:#f92672">.</span>update([text], [annotations], sgd<span style="color:#f92672">=</span>optimizer)
nlp<span style="color:#f92672">.</span>to_disk(<span style="color:#e6db74">&#39;/model&#39;</span>)



<span style="color:#75715e">## run this code as a seperate file</span>

<span style="color:#f92672">from</span> __future__ <span style="color:#f92672">import</span> unicode_literals, print_function

<span style="color:#f92672">import</span> plac
<span style="color:#f92672">import</span> random
<span style="color:#f92672">from</span> pathlib <span style="color:#f92672">import</span> Path
<span style="color:#f92672">import</span> spacy


<span style="color:#75715e"># You need to define a mapping from your data&#39;s part-of-speech tag names to the</span>
<span style="color:#75715e"># Universal Part-of-Speech tag set, as spaCy includes an enum of these tags.</span>
<span style="color:#75715e"># See here for the Universal Tag Set:</span>
<span style="color:#75715e"># http://universaldependencies.github.io/docs/u/pos/index.html</span>
<span style="color:#75715e"># You may also specify morphological features for your tags, from the universal</span>
<span style="color:#75715e"># scheme.</span>
TAG_MAP <span style="color:#f92672">=</span> {
    <span style="color:#e6db74">&#39;N&#39;</span>: {<span style="color:#e6db74">&#39;pos&#39;</span>: <span style="color:#e6db74">&#39;NOUN&#39;</span>},
    <span style="color:#e6db74">&#39;V&#39;</span>: {<span style="color:#e6db74">&#39;pos&#39;</span>: <span style="color:#e6db74">&#39;VERB&#39;</span>},
    <span style="color:#e6db74">&#39;J&#39;</span>: {<span style="color:#e6db74">&#39;pos&#39;</span>: <span style="color:#e6db74">&#39;ADJ&#39;</span>}
}

<span style="color:#75715e"># Usually you&#39;ll read this in, of course. Data formats vary. Ensure your</span>
<span style="color:#75715e"># strings are unicode and that the number of tags assigned matches spaCy&#39;s</span>
<span style="color:#75715e"># tokenization. If not, you can always add a &#39;words&#39; key to the annotations</span>
<span style="color:#75715e"># that specifies the gold-standard tokenization, e.g.:</span>
<span style="color:#75715e"># (&#34;Eatblueham&#34;, {&#39;words&#39;: [&#39;Eat&#39;, &#39;blue&#39;, &#39;ham&#39;] &#39;tags&#39;: [&#39;V&#39;, &#39;J&#39;, &#39;N&#39;]})</span>
TRAIN_DATA <span style="color:#f92672">=</span> [
    (<span style="color:#e6db74">&#34;I like green eggs&#34;</span>, {<span style="color:#e6db74">&#39;tags&#39;</span>: [<span style="color:#e6db74">&#39;N&#39;</span>, <span style="color:#e6db74">&#39;V&#39;</span>, <span style="color:#e6db74">&#39;J&#39;</span>, <span style="color:#e6db74">&#39;N&#39;</span>]}),
    (<span style="color:#e6db74">&#34;Eat blue ham&#34;</span>, {<span style="color:#e6db74">&#39;tags&#39;</span>: [<span style="color:#e6db74">&#39;V&#39;</span>, <span style="color:#e6db74">&#39;J&#39;</span>, <span style="color:#e6db74">&#39;N&#39;</span>]})
]


<span style="color:#a6e22e">@plac.annotations</span>(
    lang<span style="color:#f92672">=</span>(<span style="color:#e6db74">&#34;ISO Code of language to use&#34;</span>, <span style="color:#e6db74">&#34;option&#34;</span>, <span style="color:#e6db74">&#34;l&#34;</span>, str),
    output_dir<span style="color:#f92672">=</span>(<span style="color:#e6db74">&#34;Optional output directory&#34;</span>, <span style="color:#e6db74">&#34;option&#34;</span>, <span style="color:#e6db74">&#34;o&#34;</span>, Path),
    n_iter<span style="color:#f92672">=</span>(<span style="color:#e6db74">&#34;Number of training iterations&#34;</span>, <span style="color:#e6db74">&#34;option&#34;</span>, <span style="color:#e6db74">&#34;n&#34;</span>, int))
<span style="color:#66d9ef">def</span> <span style="color:#a6e22e">main</span>(lang<span style="color:#f92672">=</span><span style="color:#e6db74">&#39;en&#39;</span>, output_dir<span style="color:#f92672">=</span>None, n_iter<span style="color:#f92672">=</span><span style="color:#ae81ff">25</span>):
    <span style="color:#e6db74">&#34;&#34;&#34;Create a new model, set up the pipeline and train the tagger. In order to
</span><span style="color:#e6db74">    train the tagger with a custom tag map, we&#39;re creating a new Language
</span><span style="color:#e6db74">    instance with a custom vocab.
</span><span style="color:#e6db74">    &#34;&#34;&#34;</span>
    nlp <span style="color:#f92672">=</span> spacy<span style="color:#f92672">.</span>blank(lang)
    <span style="color:#75715e"># add the tagger to the pipeline</span>
    <span style="color:#75715e"># nlp.create_pipe works for built-ins that are registered with spaCy</span>
    tagger <span style="color:#f92672">=</span> nlp<span style="color:#f92672">.</span>create_pipe(<span style="color:#e6db74">&#39;tagger&#39;</span>)
    <span style="color:#75715e"># Add the tags. This needs to be done before you start training.</span>
    <span style="color:#66d9ef">for</span> tag, values <span style="color:#f92672">in</span> TAG_MAP<span style="color:#f92672">.</span>items():
        tagger<span style="color:#f92672">.</span>add_label(tag, values)
    nlp<span style="color:#f92672">.</span>add_pipe(tagger)

    optimizer <span style="color:#f92672">=</span> nlp<span style="color:#f92672">.</span>begin_training()
    <span style="color:#66d9ef">for</span> i <span style="color:#f92672">in</span> range(n_iter):
        random<span style="color:#f92672">.</span>shuffle(TRAIN_DATA)
        losses <span style="color:#f92672">=</span> {}
        <span style="color:#66d9ef">for</span> text, annotations <span style="color:#f92672">in</span> TRAIN_DATA:
            nlp<span style="color:#f92672">.</span>update([text], [annotations], sgd<span style="color:#f92672">=</span>optimizer, losses<span style="color:#f92672">=</span>losses)
        <span style="color:#66d9ef">print</span>(losses)

    <span style="color:#75715e"># test the trained model</span>
    test_text <span style="color:#f92672">=</span> <span style="color:#e6db74">&#34;I like blue eggs&#34;</span>
    doc <span style="color:#f92672">=</span> nlp(test_text)
    <span style="color:#66d9ef">print</span>(<span style="color:#e6db74">&#39;Tags&#39;</span>, [(t<span style="color:#f92672">.</span>text, t<span style="color:#f92672">.</span>tag_, t<span style="color:#f92672">.</span>pos_) <span style="color:#66d9ef">for</span> t <span style="color:#f92672">in</span> doc])

    <span style="color:#75715e"># save model to output directory</span>
    <span style="color:#66d9ef">if</span> output_dir <span style="color:#f92672">is</span> <span style="color:#f92672">not</span> None:
        output_dir <span style="color:#f92672">=</span> Path(output_dir)
        <span style="color:#66d9ef">if</span> <span style="color:#f92672">not</span> output_dir<span style="color:#f92672">.</span>exists():
            output_dir<span style="color:#f92672">.</span>mkdir()
        nlp<span style="color:#f92672">.</span>to_disk(output_dir)
        <span style="color:#66d9ef">print</span>(<span style="color:#e6db74">&#34;Saved model to&#34;</span>, output_dir)

        <span style="color:#75715e"># test the save model</span>
        <span style="color:#66d9ef">print</span>(<span style="color:#e6db74">&#34;Loading from&#34;</span>, output_dir)
        nlp2 <span style="color:#f92672">=</span> spacy<span style="color:#f92672">.</span>load(output_dir)
        doc <span style="color:#f92672">=</span> nlp2(test_text)
        <span style="color:#66d9ef">print</span>(<span style="color:#e6db74">&#39;Tags&#39;</span>, [(t<span style="color:#f92672">.</span>text, t<span style="color:#f92672">.</span>tag_, t<span style="color:#f92672">.</span>pos_) <span style="color:#66d9ef">for</span> t <span style="color:#f92672">in</span> doc])


<span style="color:#66d9ef">if</span> __name__ <span style="color:#f92672">==</span> <span style="color:#e6db74">&#39;__main__&#39;</span>:
    plac<span style="color:#f92672">.</span>call(main)

    <span style="color:#75715e"># Expected output:</span>
    <span style="color:#75715e"># [</span>
    <span style="color:#75715e">#   (&#39;I&#39;, &#39;N&#39;, &#39;NOUN&#39;),</span>
    <span style="color:#75715e">#   (&#39;like&#39;, &#39;V&#39;, &#39;VERB&#39;),</span>
    <span style="color:#75715e">#   (&#39;blue&#39;, &#39;J&#39;, &#39;ADJ&#39;),</span>
    <span style="color:#75715e">#   (&#39;eggs&#39;, &#39;N&#39;, &#39;NOUN&#39;)</span>
    <span style="color:#75715e"># ]</span></code></pre></div>
<ul>
<li>For real-case, training data massisve and assembling it a huge part of training work</li>
<li>This case the ML model abstracted as <code>update()</code> only that it works well and a NN</li>
<li>For advanced this <a href="https://nlpforhackers.io/training-pos-tagger/">blog</a> offers NLTK process using self-defined classifiers from SKL</li>
<li>The SpaCy POS training model <a href="https://explosion.ai/blog/part-of-speech-pos-tagger-in-python">A Good Part-Of-Speech Tagger in about 200 Lines of Python</a> same as Textblob</li>
</ul>

<h4 id="small-examples-of-pos-usage">Small Examples of POS usage</h4>

<ul>
<li>Change all Verbs to uppercase
<code>python
def make_verb_upper(text, pos):
    return text.upper() if pos == &quot;VERB&quot; else text
doc = nlp(u'Tom ran swiftly and walked slowly')
text = ''.join(make_verb_upper(w.text_with_ws, w.pos_) for w in doc)
print(text)
</code></li>

<li><p>Count of POS</p>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-python" data-lang="python">harry_potter <span style="color:#f92672">=</span> open(<span style="color:#e6db74">&#34;HP1.txt&#34;</span>)<span style="color:#f92672">.</span>read()
hp <span style="color:#f92672">=</span> nlp(harry_potter)
hpSents <span style="color:#f92672">=</span> list(hp<span style="color:#f92672">.</span>sents)
hpSentenceLenghts <span style="color:#f92672">=</span> [len(sent) <span style="color:#66d9ef">for</span> sent <span style="color:#f92672">in</span> hpSents]
[sent <span style="color:#66d9ef">for</span> sent <span style="color:#f92672">in</span> hSents <span style="color:#66d9ef">if</span> len(sent) <span style="color:#f92672">==</span> max(hpSentencLenghts)]
hpPOS <span style="color:#f92672">=</span> pd<span style="color:#f92672">.</span>Series(hp<span style="color:#f92672">.</span>count_by(spacy<span style="color:#f92672">.</span>attrs<span style="color:#f92672">.</span>POS)) <span style="color:#f92672">/</span> len(hp)
    
tagDict <span style="color:#f92672">=</span> {w<span style="color:#f92672">.</span>pos: w<span style="color:#f92672">.</span>pos_ <span style="color:#66d9ef">for</span> w <span style="color:#f92672">in</span> hp}
hpPOS <span style="color:#f92672">=</span> pd<span style="color:#f92672">.</span>Series(hp<span style="color:#f92672">.</span>count)By(spacy<span style="color:#f92672">.</span>attrs<span style="color:#f92672">.</span>POS)) <span style="color:#f92672">/</span> len(hp)
df <span style="color:#f92672">=</span> pd<span style="color:#f92672">.</span>DataFrame([hpPOS]], index <span style="color:#f92672">=</span> [<span style="color:#e6db74">&#39;Harry Potter&#39;</span>])
df<span style="color:#f92672">.</span>columns <span style="color:#f92672">=</span>  [tagDict[column] <span style="color:#66d9ef">for</span> column <span style="color:#f92672">in</span> df<span style="color:#f92672">.</span>columns]
df<span style="color:#f92672">.</span>T<span style="color:#f92672">.</span>plot(kind<span style="color:#f92672">=</span><span style="color:#e6db74">&#39;bar&#39;</span>)
    
<span style="color:#75715e"># most common pronoun</span>
hpAdjs <span style="color:#f92672">=</span> [w <span style="color:#66d9ef">for</span> w <span style="color:#f92672">in</span> hp <span style="color:#66d9ef">if</span> w<span style="color:#f92672">.</span>pos_ <span style="color:#f92672">==</span> <span style="color:#e6db74">&#39;PRON&#39;</span>]
Counter([w<span style="color:#f92672">.</span>string<span style="color:#f92672">.</span>strip() <span style="color:#66d9ef">for</span> w <span style="color:#f92672">in</span> hpAdjs])<span style="color:#f92672">.</span>most_common(<span style="color:#ae81ff">10</span>)</code></pre></div>
<blockquote>
<p><strong>Knowldge of POS-tags gives more in-depth text analysis, a pillar of NLP, and after tokenzisng text often the first piece of analysis</strong></p>
</blockquote></li>
</ul>

<h2 id="ner-tagging-and-applications">NER-Tagging and Applications</h2>

<ul>
<li>Real-world object <strong>GPE, PER, ORG</strong> etc</li>
<li><strong>Named Entity Disambiguation NED</strong></li>
<li>Unlike POS-tagging, NER-tagging is <strong>CONTEXT AND DOMAIN BASED</strong></li>
<li><strong>CONDITIONAL RANDOM FIELDS</strong> OFTEN USED TO TRAIN NER-TAGGER <a href="https://repository.upenn.edu/cgi/viewcontent.cig?referer=&amp;httpsredir=1&amp;article=1162&amp;context=cis_papers">CRF: Probabilistic Models for Segmenting and Labeling Sequence Data</a></li>
</ul>

<h3 id="ner-tagging-in-spacy-skipped-nltk-case">NER-Tagging in SpaCy (skipped NLTK case)</h3>

<blockquote>
<p><strong>the power of SpaCy battery-packed pipeline when loading pre-trained model, all of the above mentioned + dependency parsing are produced from that single method spacy.load() - POS, NER</strong></p>
</blockquote>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-python" data-lang="python"><span style="color:#66d9ef">for</span> token <span style="color:#f92672">in</span> sent_0: <span style="color:#75715e"># after nlp()</span>
    token<span style="color:#f92672">.</span>text, token<span style="color:#f92672">.</span>ent_type_
<span style="color:#75715e"># recall SpaCy intends user to access entities in doc.ents streamable object - slice of Doc class is called Span class</span>
<span style="color:#66d9ef">for</span> ent <span style="color:#f92672">in</span> sent_0<span style="color:#f92672">.</span>ents:
    ent<span style="color:#f92672">.</span>text, ent<span style="color:#f92672">.</span>label_</code></pre></div>
<h4 id="ner-tagging-training-in-spacy">NER-Tagging Training in SpaCy</h4>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-python" data-lang="python"><span style="color:#75715e"># nltk for NER-tagging</span>

<span style="color:#f92672">from</span> nltk.chunk <span style="color:#f92672">import</span> conlltags2tree, tree2conlltags

sentence <span style="color:#f92672">=</span> <span style="color:#e6db74">&#34;Clement and Mathieu are working at Apple.&#34;</span>
ne_tree <span style="color:#f92672">=</span> ne_chunk(pos_tag(word_tokenize(sentence)))
 
iob_tagged <span style="color:#f92672">=</span> tree2conlltags(ne_tree)
<span style="color:#66d9ef">print</span> iob_tagged

ne_tree <span style="color:#f92672">=</span> conlltags2tree(iob_tagged)
<span style="color:#66d9ef">print</span> ne_tree

<span style="color:#f92672">from</span> nltk.tag <span style="color:#f92672">import</span> StanfordNERTagger
st <span style="color:#f92672">=</span> StanfordNERTagger(<span style="color:#e6db74">&#39;/usr/share/stanford-ner/classifiers/english.all.3class.distsim.crf.ser.gz&#39;</span>,  <span style="color:#e6db74">&#39;/usr/share/stanford-ner/stanford-ner.jar&#39;</span>, encoding<span style="color:#f92672">=</span><span style="color:#e6db74">&#39;utf-8&#39;</span>)

st<span style="color:#f92672">.</span>tag(<span style="color:#960050;background-color:#1e0010">‘</span>Baptiste Capdeville <span style="color:#f92672">is</span> studying at Columbia University <span style="color:#f92672">in</span> NY<span style="color:#960050;background-color:#1e0010">’</span><span style="color:#f92672">.</span>split())

<span style="color:#f92672">import</span> spacy
nlp <span style="color:#f92672">=</span> spacy<span style="color:#f92672">.</span>load(<span style="color:#e6db74">&#39;en&#39;</span>)

sent_0 <span style="color:#f92672">=</span> nlp(<span style="color:#e6db74">u</span><span style="color:#e6db74">&#39;Donald Trump visited at the government headquarters in France today.&#39;</span>)

sent_1 <span style="color:#f92672">=</span> nlp(<span style="color:#e6db74">u</span><span style="color:#e6db74">&#39;Emmanuel Jean-Michel Frédéric Macron is a French politician serving as President of France and ex officio Co-Prince of Andorra since 14 May 2017.&#39;</span>)

sent_2 <span style="color:#f92672">=</span> nlp(<span style="color:#e6db74">u</span><span style="color:#e6db74">&#39;He studied philosophy at Paris Nanterre University, completed a Master’s of Public Affairs at Sciences Po, and graduated from the École nationale d</span><span style="color:#ae81ff">\&#39;</span><span style="color:#e6db74">administration (ÉNA) in 2004. &#39;</span>)

sent_3 <span style="color:#f92672">=</span> nlp(<span style="color:#e6db74">u</span><span style="color:#e6db74">&#39;He worked at the Inspectorate General of Finances, and later became an investment banker at Rothschild &amp; Cie Banque.&#39;</span>)

<span style="color:#66d9ef">for</span> token <span style="color:#f92672">in</span> sent_0:
    <span style="color:#66d9ef">print</span>(token<span style="color:#f92672">.</span>text, token<span style="color:#f92672">.</span>ent_type_)

<span style="color:#66d9ef">for</span> ent <span style="color:#f92672">in</span> sent_0<span style="color:#f92672">.</span>ents:
    <span style="color:#66d9ef">print</span>(ent<span style="color:#f92672">.</span>text, ent<span style="color:#f92672">.</span>label_)

<span style="color:#66d9ef">for</span> token <span style="color:#f92672">in</span> sent_1:
    <span style="color:#66d9ef">print</span>(token<span style="color:#f92672">.</span>text, token<span style="color:#f92672">.</span>ent_type_)

<span style="color:#66d9ef">for</span> ent <span style="color:#f92672">in</span> sent_1<span style="color:#f92672">.</span>ents:
    <span style="color:#66d9ef">print</span>(ent<span style="color:#f92672">.</span>text, ent<span style="color:#f92672">.</span>label_)

<span style="color:#66d9ef">for</span> token <span style="color:#f92672">in</span> sent_2:
    <span style="color:#66d9ef">print</span>(token<span style="color:#f92672">.</span>text, token<span style="color:#f92672">.</span>ent_type_)

<span style="color:#66d9ef">for</span> ent <span style="color:#f92672">in</span> sent_2<span style="color:#f92672">.</span>ents:
    <span style="color:#66d9ef">print</span>(ent<span style="color:#f92672">.</span>text, ent<span style="color:#f92672">.</span>label_)

<span style="color:#66d9ef">for</span> token <span style="color:#f92672">in</span> sent_3:
    <span style="color:#66d9ef">print</span>(token<span style="color:#f92672">.</span>text, token<span style="color:#f92672">.</span>ent_type_)

<span style="color:#66d9ef">for</span> ent <span style="color:#f92672">in</span> sent_3<span style="color:#f92672">.</span>ents:
    <span style="color:#66d9ef">print</span>(ent<span style="color:#f92672">.</span>text, ent<span style="color:#f92672">.</span>label_)</code></pre></div>
<h4 id="2-training-example-blank-model-and-adding-new-entity">2 Training Example: Blank model and Adding New Entity</h4>

<blockquote>
<p>The same princiles as POS-tagger
1. Start by adding NER label to pipeline
2. Disabling all other components of PIPI so that only train/update NER-Tagger
3. Training is backend, API by nlp.update()</p>
</blockquote>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-python" data-lang="python"><span style="color:#75715e"># run this code seperately</span>

<span style="color:#f92672">from</span> __future__ <span style="color:#f92672">import</span> unicode_literals, print_function

<span style="color:#f92672">import</span> plac
<span style="color:#f92672">import</span> random
<span style="color:#f92672">from</span> pathlib <span style="color:#f92672">import</span> Path
<span style="color:#f92672">import</span> spacy


<span style="color:#75715e"># training data</span>
TRAIN_DATA <span style="color:#f92672">=</span> [
    (<span style="color:#e6db74">&#39;Who is Shaka Khan?&#39;</span>, {
        <span style="color:#e6db74">&#39;entities&#39;</span>: [(<span style="color:#ae81ff">7</span>, <span style="color:#ae81ff">17</span>, <span style="color:#e6db74">&#39;PERSON&#39;</span>)]
    }),
    (<span style="color:#e6db74">&#39;I like London and Berlin.&#39;</span>, {
        <span style="color:#e6db74">&#39;entities&#39;</span>: [(<span style="color:#ae81ff">7</span>, <span style="color:#ae81ff">13</span>, <span style="color:#e6db74">&#39;LOC&#39;</span>), (<span style="color:#ae81ff">18</span>, <span style="color:#ae81ff">24</span>, <span style="color:#e6db74">&#39;LOC&#39;</span>)]
    })
]


<span style="color:#a6e22e">@plac.annotations</span>(
    model<span style="color:#f92672">=</span>(<span style="color:#e6db74">&#34;Model name. Defaults to blank &#39;en&#39; model.&#34;</span>, <span style="color:#e6db74">&#34;option&#34;</span>, <span style="color:#e6db74">&#34;m&#34;</span>, str),
    output_dir<span style="color:#f92672">=</span>(<span style="color:#e6db74">&#34;Optional output directory&#34;</span>, <span style="color:#e6db74">&#34;option&#34;</span>, <span style="color:#e6db74">&#34;o&#34;</span>, Path),
    n_iter<span style="color:#f92672">=</span>(<span style="color:#e6db74">&#34;Number of training iterations&#34;</span>, <span style="color:#e6db74">&#34;option&#34;</span>, <span style="color:#e6db74">&#34;n&#34;</span>, int))
<span style="color:#66d9ef">def</span> <span style="color:#a6e22e">main</span>(model<span style="color:#f92672">=</span>None, output_dir<span style="color:#f92672">=</span>None, n_iter<span style="color:#f92672">=</span><span style="color:#ae81ff">100</span>):
    <span style="color:#e6db74">&#34;&#34;&#34;Load the model, set up the pipeline and train the entity recognizer.&#34;&#34;&#34;</span>
    <span style="color:#66d9ef">if</span> model <span style="color:#f92672">is</span> <span style="color:#f92672">not</span> None:
        nlp <span style="color:#f92672">=</span> spacy<span style="color:#f92672">.</span>load(model)  <span style="color:#75715e"># load existing spaCy model</span>
        <span style="color:#66d9ef">print</span>(<span style="color:#e6db74">&#34;Loaded model &#39;</span><span style="color:#e6db74">%s</span><span style="color:#e6db74">&#39;&#34;</span> <span style="color:#f92672">%</span> model)
    <span style="color:#66d9ef">else</span>:
        nlp <span style="color:#f92672">=</span> spacy<span style="color:#f92672">.</span>blank(<span style="color:#e6db74">&#39;en&#39;</span>)  <span style="color:#75715e"># create blank Language class</span>
        <span style="color:#66d9ef">print</span>(<span style="color:#e6db74">&#34;Created blank &#39;en&#39; model&#34;</span>)

    <span style="color:#75715e"># create the built-in pipeline components and add them to the pipeline</span>
    <span style="color:#75715e"># nlp.create_pipe works for built-ins that are registered with spaCy</span>
    <span style="color:#66d9ef">if</span> <span style="color:#e6db74">&#39;ner&#39;</span> <span style="color:#f92672">not</span> <span style="color:#f92672">in</span> nlp<span style="color:#f92672">.</span>pipe_names:
        ner <span style="color:#f92672">=</span> nlp<span style="color:#f92672">.</span>create_pipe(<span style="color:#e6db74">&#39;ner&#39;</span>)
        nlp<span style="color:#f92672">.</span>add_pipe(ner, last<span style="color:#f92672">=</span>True)
    <span style="color:#75715e"># otherwise, get it so we can add labels</span>
    <span style="color:#66d9ef">else</span>:
        ner <span style="color:#f92672">=</span> nlp<span style="color:#f92672">.</span>get_pipe(<span style="color:#e6db74">&#39;ner&#39;</span>)

    <span style="color:#75715e"># add labels</span>
    <span style="color:#66d9ef">for</span> _, annotations <span style="color:#f92672">in</span> TRAIN_DATA:
        <span style="color:#66d9ef">for</span> ent <span style="color:#f92672">in</span> annotations<span style="color:#f92672">.</span>get(<span style="color:#e6db74">&#39;entities&#39;</span>):
            ner<span style="color:#f92672">.</span>add_label(ent[<span style="color:#ae81ff">2</span>])

    <span style="color:#75715e"># get names of other pipes to disable them during training</span>
    other_pipes <span style="color:#f92672">=</span> [pipe <span style="color:#66d9ef">for</span> pipe <span style="color:#f92672">in</span> nlp<span style="color:#f92672">.</span>pipe_names <span style="color:#66d9ef">if</span> pipe <span style="color:#f92672">!=</span> <span style="color:#e6db74">&#39;ner&#39;</span>]
    <span style="color:#66d9ef">with</span> nlp<span style="color:#f92672">.</span>disable_pipes(<span style="color:#f92672">*</span>other_pipes):  <span style="color:#75715e"># only train NER</span>
        optimizer <span style="color:#f92672">=</span> nlp<span style="color:#f92672">.</span>begin_training()
        <span style="color:#66d9ef">for</span> itn <span style="color:#f92672">in</span> range(n_iter):
            random<span style="color:#f92672">.</span>shuffle(TRAIN_DATA)
            losses <span style="color:#f92672">=</span> {}
            <span style="color:#66d9ef">for</span> text, annotations <span style="color:#f92672">in</span> TRAIN_DATA:
                nlp<span style="color:#f92672">.</span>update(
                    [text],  <span style="color:#75715e"># batch of texts</span>
                    [annotations],  <span style="color:#75715e"># batch of annotations</span>
                    drop<span style="color:#f92672">=</span><span style="color:#ae81ff">0.5</span>,  <span style="color:#75715e"># dropout - make it harder to memorise data</span>
                    sgd<span style="color:#f92672">=</span>optimizer,  <span style="color:#75715e"># callable to update weights</span>
                    losses<span style="color:#f92672">=</span>losses)
            <span style="color:#66d9ef">print</span>(losses)

    <span style="color:#75715e"># test the trained model</span>
    <span style="color:#66d9ef">for</span> text, _ <span style="color:#f92672">in</span> TRAIN_DATA:
        doc <span style="color:#f92672">=</span> nlp(text)
        <span style="color:#66d9ef">print</span>(<span style="color:#e6db74">&#39;Entities&#39;</span>, [(ent<span style="color:#f92672">.</span>text, ent<span style="color:#f92672">.</span>label_) <span style="color:#66d9ef">for</span> ent <span style="color:#f92672">in</span> doc<span style="color:#f92672">.</span>ents])
        <span style="color:#66d9ef">print</span>(<span style="color:#e6db74">&#39;Tokens&#39;</span>, [(t<span style="color:#f92672">.</span>text, t<span style="color:#f92672">.</span>ent_type_, t<span style="color:#f92672">.</span>ent_iob) <span style="color:#66d9ef">for</span> t <span style="color:#f92672">in</span> doc])

    <span style="color:#75715e"># save model to output directory</span>
    <span style="color:#66d9ef">if</span> output_dir <span style="color:#f92672">is</span> <span style="color:#f92672">not</span> None:
        output_dir <span style="color:#f92672">=</span> Path(output_dir)
        <span style="color:#66d9ef">if</span> <span style="color:#f92672">not</span> output_dir<span style="color:#f92672">.</span>exists():
            output_dir<span style="color:#f92672">.</span>mkdir()
        nlp<span style="color:#f92672">.</span>to_disk(output_dir)
        <span style="color:#66d9ef">print</span>(<span style="color:#e6db74">&#34;Saved model to&#34;</span>, output_dir)

        <span style="color:#75715e"># test the saved model</span>
        <span style="color:#66d9ef">print</span>(<span style="color:#e6db74">&#34;Loading from&#34;</span>, output_dir)
        nlp2 <span style="color:#f92672">=</span> spacy<span style="color:#f92672">.</span>load(output_dir)
        <span style="color:#66d9ef">for</span> text, _ <span style="color:#f92672">in</span> TRAIN_DATA:
            doc <span style="color:#f92672">=</span> nlp2(text)
            <span style="color:#66d9ef">print</span>(<span style="color:#e6db74">&#39;Entities&#39;</span>, [(ent<span style="color:#f92672">.</span>text, ent<span style="color:#f92672">.</span>label_) <span style="color:#66d9ef">for</span> ent <span style="color:#f92672">in</span> doc<span style="color:#f92672">.</span>ents])
            <span style="color:#66d9ef">print</span>(<span style="color:#e6db74">&#39;Tokens&#39;</span>, [(t<span style="color:#f92672">.</span>text, t<span style="color:#f92672">.</span>ent_type_, t<span style="color:#f92672">.</span>ent_iob) <span style="color:#66d9ef">for</span> t <span style="color:#f92672">in</span> doc])


<span style="color:#66d9ef">if</span> __name__ <span style="color:#f92672">==</span> <span style="color:#e6db74">&#39;__main__&#39;</span>:
    plac<span style="color:#f92672">.</span>call(main)

    <span style="color:#75715e"># Expected output:</span>
    <span style="color:#75715e"># Entities [(&#39;Shaka Khan&#39;, &#39;PERSON&#39;)]</span>
    <span style="color:#75715e"># Tokens [(&#39;Who&#39;, &#39;&#39;, 2), (&#39;is&#39;, &#39;&#39;, 2), (&#39;Shaka&#39;, &#39;PERSON&#39;, 3),</span>
    <span style="color:#75715e"># (&#39;Khan&#39;, &#39;PERSON&#39;, 1), (&#39;?&#39;, &#39;&#39;, 2)]</span>
    <span style="color:#75715e"># Entities [(&#39;London&#39;, &#39;LOC&#39;), (&#39;Berlin&#39;, &#39;LOC&#39;)]</span>
    <span style="color:#75715e"># Tokens [(&#39;I&#39;, &#39;&#39;, 2), (&#39;like&#39;, &#39;&#39;, 2), (&#39;London&#39;, &#39;LOC&#39;, 3),</span>
    <span style="color:#75715e"># (&#39;and&#39;, &#39;&#39;, 2), (&#39;Berlin&#39;, &#39;LOC&#39;, 3), (&#39;.&#39;, &#39;&#39;, 2)]</span></code></pre></div>
<h4 id="adding-new-class-to-model">Adding New Class to Model</h4>

<blockquote>
<p>Same principle
1. Load model, disable PIPE not updating
2. Add new label, then loop over the examples and update them
3. Actual training is performed by looping over the examples and calling <code>nlp.entity.update()</code>
4. <code>update()</code> predict each word then consults annotations provided on <code>GoldParse</code> instance to check
5. If wrong, adjusting weight to correct</p>
</blockquote>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-python" data-lang="python"><span style="color:#75715e"># run this code seperately:</span>

<span style="color:#e6db74">&#34;&#34;&#34;Example of training an additional entity type
</span><span style="color:#e6db74">This script shows how to add a new entity type to an existing pre-trained NER
</span><span style="color:#e6db74">model. To keep the example short and simple, only four sentences are provided
</span><span style="color:#e6db74">as examples. In practice, you&#39;ll need many more — a few hundred would be a
</span><span style="color:#e6db74">good start. You will also likely need to mix in examples of other entity
</span><span style="color:#e6db74">types, which might be obtained by running the entity recognizer over unlabelled
</span><span style="color:#e6db74">sentences, and adding their annotations to the training set.
</span><span style="color:#e6db74">The actual training is performed by looping over the examples, and calling
</span><span style="color:#e6db74">`nlp.entity.update()`. The `update()` method steps through the words of the
</span><span style="color:#e6db74">input. At each word, it makes a prediction. It then consults the annotations
</span><span style="color:#e6db74">provided on the GoldParse instance, to see whether it was right. If it was
</span><span style="color:#e6db74">wrong, it adjusts its weights so that the correct action will score higher
</span><span style="color:#e6db74">next time.
</span><span style="color:#e6db74">After training your model, you can save it to a directory. We recommend
</span><span style="color:#e6db74">wrapping models as Python packages, for ease of deployment.
</span><span style="color:#e6db74">For more details, see the documentation:
</span><span style="color:#e6db74">* Training: https://spacy.io/usage/training
</span><span style="color:#e6db74">* NER: https://spacy.io/usage/linguistic-features#named-entities
</span><span style="color:#e6db74">Compatible with: spaCy v2.0.0+
</span><span style="color:#e6db74">&#34;&#34;&#34;</span>
<span style="color:#f92672">from</span> __future__ <span style="color:#f92672">import</span> unicode_literals, print_function

<span style="color:#f92672">import</span> plac
<span style="color:#f92672">import</span> random
<span style="color:#f92672">from</span> pathlib <span style="color:#f92672">import</span> Path
<span style="color:#f92672">import</span> spacy


<span style="color:#75715e"># new entity label</span>
LABEL <span style="color:#f92672">=</span> <span style="color:#e6db74">&#39;ANIMAL&#39;</span>

<span style="color:#75715e"># training data</span>
<span style="color:#75715e"># Note: If you&#39;re using an existing model, make sure to mix in examples of</span>
<span style="color:#75715e"># other entity types that spaCy correctly recognized before. Otherwise, your</span>
<span style="color:#75715e"># model might learn the new type, but &#34;forget&#34; what it previously knew.</span>
<span style="color:#75715e"># https://explosion.ai/blog/pseudo-rehearsal-catastrophic-forgetting</span>
TRAIN_DATA <span style="color:#f92672">=</span> [
    (<span style="color:#e6db74">&#34;Horses are too tall and they pretend to care about your feelings&#34;</span>, {
        <span style="color:#e6db74">&#39;entities&#39;</span>: [(<span style="color:#ae81ff">0</span>, <span style="color:#ae81ff">6</span>, <span style="color:#e6db74">&#39;ANIMAL&#39;</span>)]
    }),

    (<span style="color:#e6db74">&#34;Do they bite?&#34;</span>, {
        <span style="color:#e6db74">&#39;entities&#39;</span>: []
    }),

    (<span style="color:#e6db74">&#34;horses are too tall and they pretend to care about your feelings&#34;</span>, {
        <span style="color:#e6db74">&#39;entities&#39;</span>: [(<span style="color:#ae81ff">0</span>, <span style="color:#ae81ff">6</span>, <span style="color:#e6db74">&#39;ANIMAL&#39;</span>)]
    }),

    (<span style="color:#e6db74">&#34;horses pretend to care about your feelings&#34;</span>, {
        <span style="color:#e6db74">&#39;entities&#39;</span>: [(<span style="color:#ae81ff">0</span>, <span style="color:#ae81ff">6</span>, <span style="color:#e6db74">&#39;ANIMAL&#39;</span>)]
    }),

    (<span style="color:#e6db74">&#34;they pretend to care about your feelings, those horses&#34;</span>, {
        <span style="color:#e6db74">&#39;entities&#39;</span>: [(<span style="color:#ae81ff">48</span>, <span style="color:#ae81ff">54</span>, <span style="color:#e6db74">&#39;ANIMAL&#39;</span>)]
    }),

    (<span style="color:#e6db74">&#34;horses?&#34;</span>, {
        <span style="color:#e6db74">&#39;entities&#39;</span>: [(<span style="color:#ae81ff">0</span>, <span style="color:#ae81ff">6</span>, <span style="color:#e6db74">&#39;ANIMAL&#39;</span>)]
    })
]


<span style="color:#a6e22e">@plac.annotations</span>(
    model<span style="color:#f92672">=</span>(<span style="color:#e6db74">&#34;Model name. Defaults to blank &#39;en&#39; model.&#34;</span>, <span style="color:#e6db74">&#34;option&#34;</span>, <span style="color:#e6db74">&#34;m&#34;</span>, str),
    new_model_name<span style="color:#f92672">=</span>(<span style="color:#e6db74">&#34;New model name for model meta.&#34;</span>, <span style="color:#e6db74">&#34;option&#34;</span>, <span style="color:#e6db74">&#34;nm&#34;</span>, str),
    output_dir<span style="color:#f92672">=</span>(<span style="color:#e6db74">&#34;Optional output directory&#34;</span>, <span style="color:#e6db74">&#34;option&#34;</span>, <span style="color:#e6db74">&#34;o&#34;</span>, Path),
    n_iter<span style="color:#f92672">=</span>(<span style="color:#e6db74">&#34;Number of training iterations&#34;</span>, <span style="color:#e6db74">&#34;option&#34;</span>, <span style="color:#e6db74">&#34;n&#34;</span>, int))
<span style="color:#66d9ef">def</span> <span style="color:#a6e22e">main</span>(model<span style="color:#f92672">=</span>None, new_model_name<span style="color:#f92672">=</span><span style="color:#e6db74">&#39;animal&#39;</span>, output_dir<span style="color:#f92672">=</span>None, n_iter<span style="color:#f92672">=</span><span style="color:#ae81ff">20</span>):
    <span style="color:#e6db74">&#34;&#34;&#34;Set up the pipeline and entity recognizer, and train the new entity.&#34;&#34;&#34;</span>
    <span style="color:#66d9ef">if</span> model <span style="color:#f92672">is</span> <span style="color:#f92672">not</span> None:
        nlp <span style="color:#f92672">=</span> spacy<span style="color:#f92672">.</span>load(model)  <span style="color:#75715e"># load existing spaCy model</span>
        <span style="color:#66d9ef">print</span>(<span style="color:#e6db74">&#34;Loaded model &#39;</span><span style="color:#e6db74">%s</span><span style="color:#e6db74">&#39;&#34;</span> <span style="color:#f92672">%</span> model)
    <span style="color:#66d9ef">else</span>:
        nlp <span style="color:#f92672">=</span> spacy<span style="color:#f92672">.</span>blank(<span style="color:#e6db74">&#39;en&#39;</span>)  <span style="color:#75715e"># create blank Language class</span>
        <span style="color:#66d9ef">print</span>(<span style="color:#e6db74">&#34;Created blank &#39;en&#39; model&#34;</span>)
    <span style="color:#75715e"># Add entity recognizer to model if it&#39;s not in the pipeline</span>
    <span style="color:#75715e"># nlp.create_pipe works for built-ins that are registered with spaCy</span>
    <span style="color:#66d9ef">if</span> <span style="color:#e6db74">&#39;ner&#39;</span> <span style="color:#f92672">not</span> <span style="color:#f92672">in</span> nlp<span style="color:#f92672">.</span>pipe_names:
        ner <span style="color:#f92672">=</span> nlp<span style="color:#f92672">.</span>create_pipe(<span style="color:#e6db74">&#39;ner&#39;</span>)
        nlp<span style="color:#f92672">.</span>add_pipe(ner)
    <span style="color:#75715e"># otherwise, get it, so we can add labels to it</span>
    <span style="color:#66d9ef">else</span>:
        ner <span style="color:#f92672">=</span> nlp<span style="color:#f92672">.</span>get_pipe(<span style="color:#e6db74">&#39;ner&#39;</span>)

    ner<span style="color:#f92672">.</span>add_label(LABEL)   <span style="color:#75715e"># add new entity label to entity recognizer</span>
    <span style="color:#66d9ef">if</span> model <span style="color:#f92672">is</span> None:
        optimizer <span style="color:#f92672">=</span> nlp<span style="color:#f92672">.</span>begin_training()
    <span style="color:#66d9ef">else</span>:
        <span style="color:#75715e"># Note that &#39;begin_training&#39; initializes the models, so it&#39;ll zero out</span>
        <span style="color:#75715e"># existing entity types.</span>
        optimizer <span style="color:#f92672">=</span> nlp<span style="color:#f92672">.</span>entity<span style="color:#f92672">.</span>create_optimizer()



    <span style="color:#75715e"># get names of other pipes to disable them during training</span>
    other_pipes <span style="color:#f92672">=</span> [pipe <span style="color:#66d9ef">for</span> pipe <span style="color:#f92672">in</span> nlp<span style="color:#f92672">.</span>pipe_names <span style="color:#66d9ef">if</span> pipe <span style="color:#f92672">!=</span> <span style="color:#e6db74">&#39;ner&#39;</span>]
    <span style="color:#66d9ef">with</span> nlp<span style="color:#f92672">.</span>disable_pipes(<span style="color:#f92672">*</span>other_pipes):  <span style="color:#75715e"># only train NER</span>
        <span style="color:#66d9ef">for</span> itn <span style="color:#f92672">in</span> range(n_iter):
            random<span style="color:#f92672">.</span>shuffle(TRAIN_DATA)
            losses <span style="color:#f92672">=</span> {}
            <span style="color:#66d9ef">for</span> text, annotations <span style="color:#f92672">in</span> TRAIN_DATA:
                nlp<span style="color:#f92672">.</span>update([text], [annotations], sgd<span style="color:#f92672">=</span>optimizer, drop<span style="color:#f92672">=</span><span style="color:#ae81ff">0.35</span>,
                           losses<span style="color:#f92672">=</span>losses)
            <span style="color:#66d9ef">print</span>(losses)

    <span style="color:#75715e"># test the trained model</span>
    test_text <span style="color:#f92672">=</span> <span style="color:#e6db74">&#39;Do you like horses?&#39;</span>
    doc <span style="color:#f92672">=</span> nlp(test_text)
    <span style="color:#66d9ef">print</span>(<span style="color:#e6db74">&#34;Entities in &#39;</span><span style="color:#e6db74">%s</span><span style="color:#e6db74">&#39;&#34;</span> <span style="color:#f92672">%</span> test_text)
    <span style="color:#66d9ef">for</span> ent <span style="color:#f92672">in</span> doc<span style="color:#f92672">.</span>ents:
        <span style="color:#66d9ef">print</span>(ent<span style="color:#f92672">.</span>label_, ent<span style="color:#f92672">.</span>text)

    <span style="color:#75715e"># save model to output directory</span>
    <span style="color:#66d9ef">if</span> output_dir <span style="color:#f92672">is</span> <span style="color:#f92672">not</span> None:
        output_dir <span style="color:#f92672">=</span> Path(output_dir)
        <span style="color:#66d9ef">if</span> <span style="color:#f92672">not</span> output_dir<span style="color:#f92672">.</span>exists():
            output_dir<span style="color:#f92672">.</span>mkdir()
        nlp<span style="color:#f92672">.</span>meta[<span style="color:#e6db74">&#39;name&#39;</span>] <span style="color:#f92672">=</span> new_model_name  <span style="color:#75715e"># rename model</span>
        nlp<span style="color:#f92672">.</span>to_disk(output_dir)
        <span style="color:#66d9ef">print</span>(<span style="color:#e6db74">&#34;Saved model to&#34;</span>, output_dir)

        <span style="color:#75715e"># test the saved model</span>
        <span style="color:#66d9ef">print</span>(<span style="color:#e6db74">&#34;Loading from&#34;</span>, output_dir)
        nlp2 <span style="color:#f92672">=</span> spacy<span style="color:#f92672">.</span>load(output_dir)
        doc2 <span style="color:#f92672">=</span> nlp2(test_text)
        <span style="color:#66d9ef">for</span> ent <span style="color:#f92672">in</span> doc2<span style="color:#f92672">.</span>ents:
            <span style="color:#66d9ef">print</span>(ent<span style="color:#f92672">.</span>label_, ent<span style="color:#f92672">.</span>text)


<span style="color:#66d9ef">if</span> __name__ <span style="color:#f92672">==</span> <span style="color:#e6db74">&#39;__main__&#39;</span>:
    plac<span style="color:#f92672">.</span>call(main)</code></pre></div>
<blockquote>
<p>The rest code remains the same logic, CRUCIAL DIFFERENCE is training data, ADDING NEW CLASS, and considering need to ADD OLDER EXAMPLES TOO
1. <a href="https://spacy.io/usage/training#section-ner">spaCy&rsquo;s NER linguistic features</a> with useful advice on HOW TO SET ENTITY ANNOTATIONS
2. spaCy main PIPELINE, offers customisation at each STEP
3. Backend is statistical model accepting FEATURES making PRED
4. There&rsquo;re tuto on how to build CLASSIFIER or update NLTK clf
5. <a href="https://nlpforhackers.io/named-entity-extraction/">complete guide to building own NER</a>, <a href="https://depends-on-the-definition.com/introduction-named-entity-recognition-python/">Intro to NER</a>, <a href="www.albertauyeung.com/post/python-sequence-labelling-with-crf/">Performing Sequence Labelling using CRF</a></p>
</blockquote>

<h3 id="visualisation">VISUALISATION</h3>

<h3 id="dependency-parsing-1">DEPENDENCY PARSING</h3>

<ul>
<li>Parsing is possible on any form/kind with formal GRAMMAR</li>
<li>2 Kinds TRADITIONAL UNDERSTANDING vs COMPUTATIONAL LINGUISTICS formal analysis by algo in PARSE TREE</li>
<li>2 SCHOOLS IN TRADITIONAL

<ol>
<li>Dependency Parsing VS Phrase Structure Parsing
&gt; DP is new approach credited to French linguist Lucien Tesnière</li>
</ol></li>
<li>Constituency Parsing on the other hand is older to Aristole&rsquo;s ideas on term logic.</li>
<li>Formally credited to Noam Chomsky, father of linguistics</li>
<li>IDEA: words in sentences are connected to each other with directed links - info about relationship between words</li>
<li>IDEA: phrase structure parsing, break up into prahses or constituents - grouping sentences</li>
<li>Spacy uses SYNTACTIC PARSING</li>
</ul>

<h5 id="dependency-parsing-in-python-nltk">Dependency Parsing in Python (NLTK)</h5>

<ul>
<li>NLTK provides most options in parsing methods, BUT forced to pass own GRAMMAR for effective results</li>
<li>Not purpose to learn grammars before run compLing algo</li>
<li>Demo below is how <strong>Stanford Dependency Parser</strong> wrapped NLTK</li>
<li>First step to download necessary JAR files <a href="https://nlp.stanford.edu/software/lex-parser.shtml">Historical Stanford Statistical Parser</a></li>
</ul>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-python" data-lang="python"><span style="color:#75715e"># NLTK example, be sure to download JAR</span>

<span style="color:#f92672">from</span> nltk.parse.stanford <span style="color:#f92672">import</span> StanfordDependencyParser
path_to_jar <span style="color:#f92672">=</span> <span style="color:#e6db74">&#39;path_to/stanford-parser-full-2014-08-27/stanford-parser.jar&#39;</span>
path_to_models_jar <span style="color:#f92672">=</span> <span style="color:#e6db74">&#39;path_to/stanford-parser-full-2014-08-27/stanford-parser-3.4.1-models.jar&#39;</span>
dependency_parser <span style="color:#f92672">=</span> StanfordDependencyParser(path_to_jar<span style="color:#f92672">=</span>path_to_jar, path_to_models_jar<span style="color:#f92672">=</span>path_to_models_jar)

result <span style="color:#f92672">=</span> dependency_parser<span style="color:#f92672">.</span>raw_parse(<span style="color:#e6db74">&#39;I shot an elephant in my sleep&#39;</span>)
dep <span style="color:#f92672">=</span> result<span style="color:#f92672">.</span>_next_()
list(dep<span style="color:#f92672">.</span>triples())</code></pre></div>
<h4 id="dependency-parsing-in-spacy">Dependency Parsing in SpaCy</h4>

<ul>
<li>Parsing part of PIPELINE does both PHRASAL and DEPENDENCY parsing - able to get info about what NOUN and VERB chunks in sentence are as well as info on dependencies between words</li>
<li><strong>Phrasal parsing can also be referred to as chunking, part of sentences or phrases <code>noun_chunks</code> attribute</strong></li>
</ul>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-python" data-lang="python"><span style="color:#75715e"># spaCy</span>

<span style="color:#f92672">import</span> spacy
nlp <span style="color:#f92672">=</span> spacy<span style="color:#f92672">.</span>load(<span style="color:#e6db74">&#39;en&#39;</span>)

sent_0 <span style="color:#f92672">=</span> nlp(<span style="color:#e6db74">u</span><span style="color:#e6db74">&#39;Myriam saw Clement with a telescope.&#39;</span>)
sent_1 <span style="color:#f92672">=</span> nlp(<span style="color:#e6db74">u</span><span style="color:#e6db74">&#39;Self-driving cars shift insurance liability toward manufacturers.&#39;</span>)
sent_2 <span style="color:#f92672">=</span> nlp(<span style="color:#e6db74">u</span><span style="color:#e6db74">&#39;I shot the elephant in my pyjamas.&#39;</span>)

<span style="color:#66d9ef">for</span> chunk <span style="color:#f92672">in</span> sent_0<span style="color:#f92672">.</span>noun_chunks:
    <span style="color:#66d9ef">print</span>(chunk<span style="color:#f92672">.</span>text, chunk<span style="color:#f92672">.</span>root<span style="color:#f92672">.</span>text, chunk<span style="color:#f92672">.</span>root<span style="color:#f92672">.</span>dep_,
          chunk<span style="color:#f92672">.</span>root<span style="color:#f92672">.</span>head<span style="color:#f92672">.</span>text)

<span style="color:#66d9ef">for</span> chunk <span style="color:#f92672">in</span> sent_1<span style="color:#f92672">.</span>noun_chunks:
    <span style="color:#66d9ef">print</span>(chunk<span style="color:#f92672">.</span>text, chunk<span style="color:#f92672">.</span>root<span style="color:#f92672">.</span>text, chunk<span style="color:#f92672">.</span>root<span style="color:#f92672">.</span>dep_,
          chunk<span style="color:#f92672">.</span>root<span style="color:#f92672">.</span>head<span style="color:#f92672">.</span>text)

<span style="color:#66d9ef">for</span> chunk <span style="color:#f92672">in</span> sent_2<span style="color:#f92672">.</span>noun_chunks:
    <span style="color:#66d9ef">print</span>(chunk<span style="color:#f92672">.</span>text, chunk<span style="color:#f92672">.</span>root<span style="color:#f92672">.</span>text, chunk<span style="color:#f92672">.</span>root<span style="color:#f92672">.</span>dep_,
          chunk<span style="color:#f92672">.</span>root<span style="color:#f92672">.</span>head<span style="color:#f92672">.</span>text)

<span style="color:#66d9ef">for</span> token <span style="color:#f92672">in</span> sent_0:
    <span style="color:#66d9ef">print</span>(token<span style="color:#f92672">.</span>text, token<span style="color:#f92672">.</span>dep_, token<span style="color:#f92672">.</span>head<span style="color:#f92672">.</span>text, token<span style="color:#f92672">.</span>head<span style="color:#f92672">.</span>pos_,
          [child <span style="color:#66d9ef">for</span> child <span style="color:#f92672">in</span> token<span style="color:#f92672">.</span>children])

<span style="color:#66d9ef">for</span> token <span style="color:#f92672">in</span> sent_1:
    <span style="color:#66d9ef">print</span>(token<span style="color:#f92672">.</span>text, token<span style="color:#f92672">.</span>dep_, token<span style="color:#f92672">.</span>head<span style="color:#f92672">.</span>text, token<span style="color:#f92672">.</span>head<span style="color:#f92672">.</span>pos_,
          [child <span style="color:#66d9ef">for</span> child <span style="color:#f92672">in</span> token<span style="color:#f92672">.</span>children])

<span style="color:#66d9ef">for</span> token <span style="color:#f92672">in</span> sent_2:
    <span style="color:#66d9ef">print</span>(token<span style="color:#f92672">.</span>text, token<span style="color:#f92672">.</span>dep_, token<span style="color:#f92672">.</span>head<span style="color:#f92672">.</span>text, token<span style="color:#f92672">.</span>head<span style="color:#f92672">.</span>pos_,
          [child <span style="color:#66d9ef">for</span> child <span style="color:#f92672">in</span> token<span style="color:#f92672">.</span>children])

<span style="color:#f92672">from</span> spacy.symbols <span style="color:#f92672">import</span> nsubj, VERB
<span style="color:#75715e"># Other ways navigating tree - identify one head per sentence via iterating possible subjects instead of verbs</span>

verbs <span style="color:#f92672">=</span> set()
<span style="color:#66d9ef">for</span> possible_subject <span style="color:#f92672">in</span> sent_1:
    <span style="color:#66d9ef">if</span> possible_subject<span style="color:#f92672">.</span>dep <span style="color:#f92672">==</span> nsubj <span style="color:#f92672">and</span> possible_subject<span style="color:#f92672">.</span>head<span style="color:#f92672">.</span>pos <span style="color:#f92672">==</span> VERB:
        verbs<span style="color:#f92672">.</span>add(possible_subject<span style="color:#f92672">.</span>head)

<span style="color:#75715e"># Iterated through all words and checked cases where a nomial subject and head is verb</span>

<span style="color:#75715e"># possibel to search verbs directly but by double-iteration</span>
verbs <span style="color:#f92672">=</span> []
<span style="color:#66d9ef">for</span> possible_verb <span style="color:#f92672">in</span> doc:
    <span style="color:#66d9ef">if</span> possible_verb<span style="color:#f92672">.</span>pos <span style="color:#f92672">==</span> VERB:
        <span style="color:#66d9ef">for</span> possible_subject <span style="color:#f92672">in</span> possible_verb<span style="color:#f92672">.</span>children:
            <span style="color:#66d9ef">if</span> possible_subject<span style="color:#f92672">.</span>dep <span style="color:#f92672">==</span> nsubj:
                verbs<span style="color:#f92672">.</span>append(possible_verb)
                <span style="color:#66d9ef">break</span>

<span style="color:#75715e"># Also useful ATTR lefts, rights, n_rigths ,n_lefts giving info about particular token in tree</span>
<span style="color:#75715e"># example to finding phrases using syntactic head</span>

root <span style="color:#f92672">=</span> [token <span style="color:#66d9ef">for</span> token <span style="color:#f92672">in</span> sent_1 <span style="color:#66d9ef">if</span> token<span style="color:#f92672">.</span>head <span style="color:#f92672">==</span> token][<span style="color:#ae81ff">0</span>]
subject <span style="color:#f92672">=</span> list(root<span style="color:#f92672">.</span>lefts)[<span style="color:#ae81ff">0</span>]
<span style="color:#66d9ef">for</span> descendant <span style="color:#f92672">in</span> subject<span style="color:#f92672">.</span>subtree:
    <span style="color:#66d9ef">assert</span> subject <span style="color:#f92672">is</span> descendant <span style="color:#f92672">or</span> subject<span style="color:#f92672">.</span>is_ancestor(descendant)
    <span style="color:#66d9ef">print</span>(descendant<span style="color:#f92672">.</span>text, descendant<span style="color:#f92672">.</span>dep_, descendant<span style="color:#f92672">.</span>n_lefts, descendant<span style="color:#f92672">.</span>n_rights,
          [ancestor<span style="color:#f92672">.</span>text <span style="color:#66d9ef">for</span> ancestor <span style="color:#f92672">in</span> descendant<span style="color:#f92672">.</span>ancestors])

<span style="color:#75715e"># Find root by seeing where head is token-itself. Subject to the left of tree, iterate subject priting descenddants and number of leaves</span>

<span style="color:#75715e"># above more realistic finding commonly used ADJ to describe a character in a book</span>
adjectives <span style="color:#f92672">=</span> []
<span style="color:#66d9ef">for</span> sent <span style="color:#f92672">in</span> book<span style="color:#f92672">.</span>sents: 
    <span style="color:#66d9ef">for</span> word <span style="color:#f92672">in</span> sent: 
        <span style="color:#66d9ef">if</span> <span style="color:#e6db74">&#39;Character&#39;</span> <span style="color:#f92672">in</span> word<span style="color:#f92672">.</span>string: 
            <span style="color:#66d9ef">for</span> child <span style="color:#f92672">in</span> word<span style="color:#f92672">.</span>children: 
                <span style="color:#66d9ef">if</span> child<span style="color:#f92672">.</span>pos_ <span style="color:#f92672">==</span> <span style="color:#e6db74">&#39;ADJ&#39;</span>: adjectives<span style="color:#f92672">.</span>append(child<span style="color:#f92672">.</span>string<span style="color:#f92672">.</span>strip())
Counter(adjectives)<span style="color:#f92672">.</span>most_common(<span style="color:#ae81ff">10</span>)</code></pre></div>
<ul>
<li>Code remains simple but effective - iterating over books sentences, looking for character of sentence, children of character</li>
<li>Then check if child is an ADJ. Being child means likely marked as DEP, with root word (i.e. Character) described by child</li>
<li>By checking most common ADJ to mini-analyse characters of books</li>
</ul>

<h4 id="training-dep-parsers">Training DEP Parsers</h4>

<ul>
<li>Again, SpaCy abstruct the hardest part of ML - <strong>selecting features</strong></li>
<li>All left to do is inputting proper training data and set up API to update models</li>
</ul>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-python" data-lang="python"><span style="color:#75715e"># run the next code as a seperate file</span>

<span style="color:#e6db74">&#34;&#34;&#34;Example of training spaCy dependency parser, starting off with an existing
</span><span style="color:#e6db74">model or a blank model. For more details, see the documentation:
</span><span style="color:#e6db74">* Training: https://spacy.io/usage/training
</span><span style="color:#e6db74">* Dependency Parse: https://spacy.io/usage/linguistic-features#dependency-parse
</span><span style="color:#e6db74">Compatible with: spaCy v2.0.0+
</span><span style="color:#e6db74">&#34;&#34;&#34;</span>
<span style="color:#f92672">from</span> __future__ <span style="color:#f92672">import</span> unicode_literals, print_function

<span style="color:#f92672">import</span> plac
<span style="color:#f92672">import</span> random
<span style="color:#f92672">from</span> pathlib <span style="color:#f92672">import</span> Path
<span style="color:#f92672">import</span> spacy


<span style="color:#75715e"># training data</span>
TRAIN_DATA <span style="color:#f92672">=</span> [
    (<span style="color:#e6db74">&#34;They trade mortgage-backed securities.&#34;</span>, {
        <span style="color:#e6db74">&#39;heads&#39;</span>: [<span style="color:#ae81ff">1</span>, <span style="color:#ae81ff">1</span>, <span style="color:#ae81ff">4</span>, <span style="color:#ae81ff">4</span>, <span style="color:#ae81ff">5</span>, <span style="color:#ae81ff">1</span>, <span style="color:#ae81ff">1</span>],
        <span style="color:#e6db74">&#39;deps&#39;</span>: [<span style="color:#e6db74">&#39;nsubj&#39;</span>, <span style="color:#e6db74">&#39;ROOT&#39;</span>, <span style="color:#e6db74">&#39;compound&#39;</span>, <span style="color:#e6db74">&#39;punct&#39;</span>, <span style="color:#e6db74">&#39;nmod&#39;</span>, <span style="color:#e6db74">&#39;dobj&#39;</span>, <span style="color:#e6db74">&#39;punct&#39;</span>]
    }),
    (<span style="color:#e6db74">&#34;I like London and Berlin.&#34;</span>, {
        <span style="color:#e6db74">&#39;heads&#39;</span>: [<span style="color:#ae81ff">1</span>, <span style="color:#ae81ff">1</span>, <span style="color:#ae81ff">1</span>, <span style="color:#ae81ff">2</span>, <span style="color:#ae81ff">2</span>, <span style="color:#ae81ff">1</span>],
        <span style="color:#e6db74">&#39;deps&#39;</span>: [<span style="color:#e6db74">&#39;nsubj&#39;</span>, <span style="color:#e6db74">&#39;ROOT&#39;</span>, <span style="color:#e6db74">&#39;dobj&#39;</span>, <span style="color:#e6db74">&#39;cc&#39;</span>, <span style="color:#e6db74">&#39;conj&#39;</span>, <span style="color:#e6db74">&#39;punct&#39;</span>]
    })
]

<span style="color:#75715e"># give exmaples of heads and dep label - i.e. verb is word at index 0, DEP clearly defined</span>
<span style="color:#a6e22e">@plac.annotations</span>(
    model<span style="color:#f92672">=</span>(<span style="color:#e6db74">&#34;Model name. Defaults to blank &#39;en&#39; model.&#34;</span>, <span style="color:#e6db74">&#34;option&#34;</span>, <span style="color:#e6db74">&#34;m&#34;</span>, str),
    output_dir<span style="color:#f92672">=</span>(<span style="color:#e6db74">&#34;Optional output directory&#34;</span>, <span style="color:#e6db74">&#34;option&#34;</span>, <span style="color:#e6db74">&#34;o&#34;</span>, Path),
    n_iter<span style="color:#f92672">=</span>(<span style="color:#e6db74">&#34;Number of training iterations&#34;</span>, <span style="color:#e6db74">&#34;option&#34;</span>, <span style="color:#e6db74">&#34;n&#34;</span>, int))
<span style="color:#66d9ef">def</span> <span style="color:#a6e22e">main</span>(model<span style="color:#f92672">=</span>None, output_dir<span style="color:#f92672">=</span>None, n_iter<span style="color:#f92672">=</span><span style="color:#ae81ff">10</span>):
    <span style="color:#e6db74">&#34;&#34;&#34;Load the model, set up the pipeline and train the parser.&#34;&#34;&#34;</span>
    <span style="color:#66d9ef">if</span> model <span style="color:#f92672">is</span> <span style="color:#f92672">not</span> None:
        nlp <span style="color:#f92672">=</span> spacy<span style="color:#f92672">.</span>load(model)  <span style="color:#75715e"># load existing spaCy model</span>
        <span style="color:#66d9ef">print</span>(<span style="color:#e6db74">&#34;Loaded model &#39;</span><span style="color:#e6db74">%s</span><span style="color:#e6db74">&#39;&#34;</span> <span style="color:#f92672">%</span> model)
    <span style="color:#66d9ef">else</span>:
        nlp <span style="color:#f92672">=</span> spacy<span style="color:#f92672">.</span>blank(<span style="color:#e6db74">&#39;en&#39;</span>)  <span style="color:#75715e"># create blank Language class</span>
        <span style="color:#66d9ef">print</span>(<span style="color:#e6db74">&#34;Created blank &#39;en&#39; model&#34;</span>)

    <span style="color:#75715e"># add the parser to the pipeline if it doesn&#39;t exist</span>
    <span style="color:#75715e"># nlp.create_pipe works for built-ins that are registered with spaCy</span>
    <span style="color:#66d9ef">if</span> <span style="color:#e6db74">&#39;parser&#39;</span> <span style="color:#f92672">not</span> <span style="color:#f92672">in</span> nlp<span style="color:#f92672">.</span>pipe_names:
        parser <span style="color:#f92672">=</span> nlp<span style="color:#f92672">.</span>create_pipe(<span style="color:#e6db74">&#39;parser&#39;</span>)
        nlp<span style="color:#f92672">.</span>add_pipe(parser, first<span style="color:#f92672">=</span>True)
    <span style="color:#75715e"># otherwise, get it, so we can add labels to it</span>
    <span style="color:#66d9ef">else</span>:
        parser <span style="color:#f92672">=</span> nlp<span style="color:#f92672">.</span>get_pipe(<span style="color:#e6db74">&#39;parser&#39;</span>)

    <span style="color:#75715e"># add labels to the parser</span>
    <span style="color:#66d9ef">for</span> _, annotations <span style="color:#f92672">in</span> TRAIN_DATA:
        <span style="color:#66d9ef">for</span> dep <span style="color:#f92672">in</span> annotations<span style="color:#f92672">.</span>get(<span style="color:#e6db74">&#39;deps&#39;</span>, []):
            parser<span style="color:#f92672">.</span>add_label(dep)

    <span style="color:#75715e"># get names of other pipes to disable them during training</span>
    other_pipes <span style="color:#f92672">=</span> [pipe <span style="color:#66d9ef">for</span> pipe <span style="color:#f92672">in</span> nlp<span style="color:#f92672">.</span>pipe_names <span style="color:#66d9ef">if</span> pipe <span style="color:#f92672">!=</span> <span style="color:#e6db74">&#39;parser&#39;</span>]
    <span style="color:#66d9ef">with</span> nlp<span style="color:#f92672">.</span>disable_pipes(<span style="color:#f92672">*</span>other_pipes):  <span style="color:#75715e"># only train parser</span>
        optimizer <span style="color:#f92672">=</span> nlp<span style="color:#f92672">.</span>begin_training()
        <span style="color:#66d9ef">for</span> itn <span style="color:#f92672">in</span> range(n_iter):
            random<span style="color:#f92672">.</span>shuffle(TRAIN_DATA)
            losses <span style="color:#f92672">=</span> {}
            <span style="color:#66d9ef">for</span> text, annotations <span style="color:#f92672">in</span> TRAIN_DATA:
                nlp<span style="color:#f92672">.</span>update([text], [annotations], sgd<span style="color:#f92672">=</span>optimizer, losses<span style="color:#f92672">=</span>losses)
            <span style="color:#66d9ef">print</span>(losses)

    <span style="color:#75715e"># test the trained model</span>
    test_text <span style="color:#f92672">=</span> <span style="color:#e6db74">&#34;I like securities.&#34;</span>
    doc <span style="color:#f92672">=</span> nlp(test_text)
    <span style="color:#66d9ef">print</span>(<span style="color:#e6db74">&#39;Dependencies&#39;</span>, [(t<span style="color:#f92672">.</span>text, t<span style="color:#f92672">.</span>dep_, t<span style="color:#f92672">.</span>head<span style="color:#f92672">.</span>text) <span style="color:#66d9ef">for</span> t <span style="color:#f92672">in</span> doc])

    <span style="color:#75715e"># save model to output directory</span>
    <span style="color:#66d9ef">if</span> output_dir <span style="color:#f92672">is</span> <span style="color:#f92672">not</span> None:
        output_dir <span style="color:#f92672">=</span> Path(output_dir)
        <span style="color:#66d9ef">if</span> <span style="color:#f92672">not</span> output_dir<span style="color:#f92672">.</span>exists():
            output_dir<span style="color:#f92672">.</span>mkdir()
        nlp<span style="color:#f92672">.</span>to_disk(output_dir)
        <span style="color:#66d9ef">print</span>(<span style="color:#e6db74">&#34;Saved model to&#34;</span>, output_dir)

        <span style="color:#75715e"># test the saved model</span>
        <span style="color:#66d9ef">print</span>(<span style="color:#e6db74">&#34;Loading from&#34;</span>, output_dir)
        nlp2 <span style="color:#f92672">=</span> spacy<span style="color:#f92672">.</span>load(output_dir)
        doc <span style="color:#f92672">=</span> nlp2(test_text)
        <span style="color:#66d9ef">print</span>(<span style="color:#e6db74">&#39;Dependencies&#39;</span>, [(t<span style="color:#f92672">.</span>text, t<span style="color:#f92672">.</span>dep_, t<span style="color:#f92672">.</span>head<span style="color:#f92672">.</span>text) <span style="color:#66d9ef">for</span> t <span style="color:#f92672">in</span> doc])


<span style="color:#66d9ef">if</span> __name__ <span style="color:#f92672">==</span> <span style="color:#e6db74">&#39;__main__&#39;</span>:
    plac<span style="color:#f92672">.</span>call(main)

    <span style="color:#75715e"># expected result:</span>
    <span style="color:#75715e"># [</span>
    <span style="color:#75715e">#   (&#39;I&#39;, &#39;nsubj&#39;, &#39;like&#39;),</span>
    <span style="color:#75715e">#   (&#39;like&#39;, &#39;ROOT&#39;, &#39;like&#39;),</span>
    <span style="color:#75715e">#   (&#39;securities&#39;, &#39;dobj&#39;, &#39;like&#39;),</span>
    <span style="color:#75715e">#   (&#39;.&#39;, &#39;punct&#39;, &#39;like&#39;)</span>
    <span style="color:#75715e"># ]</span>

<span style="color:#e6db74">&#34;&#34;&#34;Above rather vanilla, below follows same style as POS and NER, but adding custom semantics.
</span><span style="color:#e6db74">WHY? Train parsers to understand new semantic relationships or DEP among words.
</span><span style="color:#e6db74">Particularly interesting as able to model own DEP FOR USE-CASE; BUT caution it may not alreays output CORRECT DEP.
</span><span style="color:#e6db74">&#34;&#34;&#34;</span>
    

<span style="color:#75715e"># run the next code as a seperate file</span>

<span style="color:#75715e">#!/usr/bin/env python</span>
<span style="color:#75715e"># coding: utf-8</span>
<span style="color:#e6db74">&#34;&#34;&#34;Using the parser to recognise your own semantics
</span><span style="color:#e6db74">spaCy&#39;s parser component can be used to trained to predict any type of tree
</span><span style="color:#e6db74">structure over your input text. You can also predict trees over whole documents
</span><span style="color:#e6db74">or chat logs, with connections between the sentence-roots used to annotate
</span><span style="color:#e6db74">discourse structure. In this example, we&#39;ll build a message parser for a common
</span><span style="color:#e6db74">&#34;chat intent&#34;: finding local businesses. Our message semantics will have the
</span><span style="color:#e6db74">following types of relations: ROOT, PLACE, QUALITY, ATTRIBUTE, TIME, LOCATION.
</span><span style="color:#e6db74">&#34;show me the best hotel in berlin&#34;
</span><span style="color:#e6db74">(&#39;show&#39;, &#39;ROOT&#39;, &#39;show&#39;)
</span><span style="color:#e6db74">(&#39;best&#39;, &#39;QUALITY&#39;, &#39;hotel&#39;) --&gt; hotel with QUALITY best
</span><span style="color:#e6db74">(&#39;hotel&#39;, &#39;PLACE&#39;, &#39;show&#39;) --&gt; show PLACE hotel
</span><span style="color:#e6db74">(&#39;berlin&#39;, &#39;LOCATION&#39;, &#39;hotel&#39;) --&gt; hotel with LOCATION berlin
</span><span style="color:#e6db74">Compatible with: spaCy v2.0.0+
</span><span style="color:#e6db74">&#34;&#34;&#34;</span>
<span style="color:#f92672">from</span> __future__ <span style="color:#f92672">import</span> unicode_literals, print_function

<span style="color:#f92672">import</span> plac
<span style="color:#f92672">import</span> random
<span style="color:#f92672">import</span> spacy
<span style="color:#f92672">from</span> pathlib <span style="color:#f92672">import</span> Path


<span style="color:#75715e"># training data: texts, heads and dependency labels</span>
<span style="color:#75715e"># for no relation, we simply chose an arbitrary dependency label, e.g. &#39;-&#39;</span>
TRAIN_DATA <span style="color:#f92672">=</span> [
    (<span style="color:#e6db74">&#34;find a cafe with great wifi&#34;</span>, {
        <span style="color:#e6db74">&#39;heads&#39;</span>: [<span style="color:#ae81ff">0</span>, <span style="color:#ae81ff">2</span>, <span style="color:#ae81ff">0</span>, <span style="color:#ae81ff">5</span>, <span style="color:#ae81ff">5</span>, <span style="color:#ae81ff">2</span>],  <span style="color:#75715e"># index of token head</span>
        <span style="color:#e6db74">&#39;deps&#39;</span>: [<span style="color:#e6db74">&#39;ROOT&#39;</span>, <span style="color:#e6db74">&#39;-&#39;</span>, <span style="color:#e6db74">&#39;PLACE&#39;</span>, <span style="color:#e6db74">&#39;-&#39;</span>, <span style="color:#e6db74">&#39;QUALITY&#39;</span>, <span style="color:#e6db74">&#39;ATTRIBUTE&#39;</span>]
    }),
    (<span style="color:#e6db74">&#34;find a hotel near the beach&#34;</span>, {
        <span style="color:#e6db74">&#39;heads&#39;</span>: [<span style="color:#ae81ff">0</span>, <span style="color:#ae81ff">2</span>, <span style="color:#ae81ff">0</span>, <span style="color:#ae81ff">5</span>, <span style="color:#ae81ff">5</span>, <span style="color:#ae81ff">2</span>],
        <span style="color:#e6db74">&#39;deps&#39;</span>: [<span style="color:#e6db74">&#39;ROOT&#39;</span>, <span style="color:#e6db74">&#39;-&#39;</span>, <span style="color:#e6db74">&#39;PLACE&#39;</span>, <span style="color:#e6db74">&#39;QUALITY&#39;</span>, <span style="color:#e6db74">&#39;-&#39;</span>, <span style="color:#e6db74">&#39;ATTRIBUTE&#39;</span>]
    }),
    (<span style="color:#e6db74">&#34;find me the closest gym that&#39;s open late&#34;</span>, {
        <span style="color:#e6db74">&#39;heads&#39;</span>: [<span style="color:#ae81ff">0</span>, <span style="color:#ae81ff">0</span>, <span style="color:#ae81ff">4</span>, <span style="color:#ae81ff">4</span>, <span style="color:#ae81ff">0</span>, <span style="color:#ae81ff">6</span>, <span style="color:#ae81ff">4</span>, <span style="color:#ae81ff">6</span>, <span style="color:#ae81ff">6</span>],
        <span style="color:#e6db74">&#39;deps&#39;</span>: [<span style="color:#e6db74">&#39;ROOT&#39;</span>, <span style="color:#e6db74">&#39;-&#39;</span>, <span style="color:#e6db74">&#39;-&#39;</span>, <span style="color:#e6db74">&#39;QUALITY&#39;</span>, <span style="color:#e6db74">&#39;PLACE&#39;</span>, <span style="color:#e6db74">&#39;-&#39;</span>, <span style="color:#e6db74">&#39;-&#39;</span>, <span style="color:#e6db74">&#39;ATTRIBUTE&#39;</span>, <span style="color:#e6db74">&#39;TIME&#39;</span>]
    }),
    (<span style="color:#e6db74">&#34;show me the cheapest store that sells flowers&#34;</span>, {
        <span style="color:#e6db74">&#39;heads&#39;</span>: [<span style="color:#ae81ff">0</span>, <span style="color:#ae81ff">0</span>, <span style="color:#ae81ff">4</span>, <span style="color:#ae81ff">4</span>, <span style="color:#ae81ff">0</span>, <span style="color:#ae81ff">4</span>, <span style="color:#ae81ff">4</span>, <span style="color:#ae81ff">4</span>],  <span style="color:#75715e"># attach &#34;flowers&#34; to store!</span>
        <span style="color:#e6db74">&#39;deps&#39;</span>: [<span style="color:#e6db74">&#39;ROOT&#39;</span>, <span style="color:#e6db74">&#39;-&#39;</span>, <span style="color:#e6db74">&#39;-&#39;</span>, <span style="color:#e6db74">&#39;QUALITY&#39;</span>, <span style="color:#e6db74">&#39;PLACE&#39;</span>, <span style="color:#e6db74">&#39;-&#39;</span>, <span style="color:#e6db74">&#39;-&#39;</span>, <span style="color:#e6db74">&#39;PRODUCT&#39;</span>]
    }),
    (<span style="color:#e6db74">&#34;find a nice restaurant in london&#34;</span>, {
        <span style="color:#e6db74">&#39;heads&#39;</span>: [<span style="color:#ae81ff">0</span>, <span style="color:#ae81ff">3</span>, <span style="color:#ae81ff">3</span>, <span style="color:#ae81ff">0</span>, <span style="color:#ae81ff">3</span>, <span style="color:#ae81ff">3</span>],
        <span style="color:#e6db74">&#39;deps&#39;</span>: [<span style="color:#e6db74">&#39;ROOT&#39;</span>, <span style="color:#e6db74">&#39;-&#39;</span>, <span style="color:#e6db74">&#39;QUALITY&#39;</span>, <span style="color:#e6db74">&#39;PLACE&#39;</span>, <span style="color:#e6db74">&#39;-&#39;</span>, <span style="color:#e6db74">&#39;LOCATION&#39;</span>]
    }),
    (<span style="color:#e6db74">&#34;show me the coolest hostel in berlin&#34;</span>, {
        <span style="color:#e6db74">&#39;heads&#39;</span>: [<span style="color:#ae81ff">0</span>, <span style="color:#ae81ff">0</span>, <span style="color:#ae81ff">4</span>, <span style="color:#ae81ff">4</span>, <span style="color:#ae81ff">0</span>, <span style="color:#ae81ff">4</span>, <span style="color:#ae81ff">4</span>],
        <span style="color:#e6db74">&#39;deps&#39;</span>: [<span style="color:#e6db74">&#39;ROOT&#39;</span>, <span style="color:#e6db74">&#39;-&#39;</span>, <span style="color:#e6db74">&#39;-&#39;</span>, <span style="color:#e6db74">&#39;QUALITY&#39;</span>, <span style="color:#e6db74">&#39;PLACE&#39;</span>, <span style="color:#e6db74">&#39;-&#39;</span>, <span style="color:#e6db74">&#39;LOCATION&#39;</span>]
    }),
    (<span style="color:#e6db74">&#34;find a good italian restaurant near work&#34;</span>, {
        <span style="color:#e6db74">&#39;heads&#39;</span>: [<span style="color:#ae81ff">0</span>, <span style="color:#ae81ff">4</span>, <span style="color:#ae81ff">4</span>, <span style="color:#ae81ff">4</span>, <span style="color:#ae81ff">0</span>, <span style="color:#ae81ff">4</span>, <span style="color:#ae81ff">5</span>],
        <span style="color:#e6db74">&#39;deps&#39;</span>: [<span style="color:#e6db74">&#39;ROOT&#39;</span>, <span style="color:#e6db74">&#39;-&#39;</span>, <span style="color:#e6db74">&#39;QUALITY&#39;</span>, <span style="color:#e6db74">&#39;ATTRIBUTE&#39;</span>, <span style="color:#e6db74">&#39;PLACE&#39;</span>, <span style="color:#e6db74">&#39;ATTRIBUTE&#39;</span>, <span style="color:#e6db74">&#39;LOCATION&#39;</span>]
    })
]

<span style="color:#75715e"># Worthwhile to check training data, those new DEP shows qualities in examples</span>
<span style="color:#75715e"># These feedbacks are vital in building custom semantics information graph</span>

<span style="color:#a6e22e">@plac.annotations</span>(
    model<span style="color:#f92672">=</span>(<span style="color:#e6db74">&#34;Model name. Defaults to blank &#39;en&#39; model.&#34;</span>, <span style="color:#e6db74">&#34;option&#34;</span>, <span style="color:#e6db74">&#34;m&#34;</span>, str),
    output_dir<span style="color:#f92672">=</span>(<span style="color:#e6db74">&#34;Optional output directory&#34;</span>, <span style="color:#e6db74">&#34;option&#34;</span>, <span style="color:#e6db74">&#34;o&#34;</span>, Path),
    n_iter<span style="color:#f92672">=</span>(<span style="color:#e6db74">&#34;Number of training iterations&#34;</span>, <span style="color:#e6db74">&#34;option&#34;</span>, <span style="color:#e6db74">&#34;n&#34;</span>, int))
<span style="color:#66d9ef">def</span> <span style="color:#a6e22e">main</span>(model<span style="color:#f92672">=</span>None, output_dir<span style="color:#f92672">=</span>None, n_iter<span style="color:#f92672">=</span><span style="color:#ae81ff">5</span>):
    <span style="color:#e6db74">&#34;&#34;&#34;Load the model, set up the pipeline and train the parser.&#34;&#34;&#34;</span>
    <span style="color:#66d9ef">if</span> model <span style="color:#f92672">is</span> <span style="color:#f92672">not</span> None:
        nlp <span style="color:#f92672">=</span> spacy<span style="color:#f92672">.</span>load(model)  <span style="color:#75715e"># load existing spaCy model</span>
        <span style="color:#66d9ef">print</span>(<span style="color:#e6db74">&#34;Loaded model &#39;</span><span style="color:#e6db74">%s</span><span style="color:#e6db74">&#39;&#34;</span> <span style="color:#f92672">%</span> model)
    <span style="color:#66d9ef">else</span>:
        nlp <span style="color:#f92672">=</span> spacy<span style="color:#f92672">.</span>blank(<span style="color:#e6db74">&#39;en&#39;</span>)  <span style="color:#75715e"># create blank Language class</span>
        <span style="color:#66d9ef">print</span>(<span style="color:#e6db74">&#34;Created blank &#39;en&#39; model&#34;</span>)

    <span style="color:#75715e"># We&#39;ll use the built-in dependency parser class, but we want to create a</span>
    <span style="color:#75715e"># fresh instance – just in case.</span>
    <span style="color:#66d9ef">if</span> <span style="color:#e6db74">&#39;parser&#39;</span> <span style="color:#f92672">in</span> nlp<span style="color:#f92672">.</span>pipe_names:
        nlp<span style="color:#f92672">.</span>remove_pipe(<span style="color:#e6db74">&#39;parser&#39;</span>)
    parser <span style="color:#f92672">=</span> nlp<span style="color:#f92672">.</span>create_pipe(<span style="color:#e6db74">&#39;parser&#39;</span>)
    nlp<span style="color:#f92672">.</span>add_pipe(parser, first<span style="color:#f92672">=</span>True)

    <span style="color:#66d9ef">for</span> text, annotations <span style="color:#f92672">in</span> TRAIN_DATA:
        <span style="color:#66d9ef">for</span> dep <span style="color:#f92672">in</span> annotations<span style="color:#f92672">.</span>get(<span style="color:#e6db74">&#39;deps&#39;</span>, []):
            parser<span style="color:#f92672">.</span>add_label(dep)

    other_pipes <span style="color:#f92672">=</span> [pipe <span style="color:#66d9ef">for</span> pipe <span style="color:#f92672">in</span> nlp<span style="color:#f92672">.</span>pipe_names <span style="color:#66d9ef">if</span> pipe <span style="color:#f92672">!=</span> <span style="color:#e6db74">&#39;parser&#39;</span>]
    <span style="color:#66d9ef">with</span> nlp<span style="color:#f92672">.</span>disable_pipes(<span style="color:#f92672">*</span>other_pipes):  <span style="color:#75715e"># only train parser</span>
        optimizer <span style="color:#f92672">=</span> nlp<span style="color:#f92672">.</span>begin_training()
        <span style="color:#66d9ef">for</span> itn <span style="color:#f92672">in</span> range(n_iter):
            random<span style="color:#f92672">.</span>shuffle(TRAIN_DATA)
            losses <span style="color:#f92672">=</span> {}
            <span style="color:#66d9ef">for</span> text, annotations <span style="color:#f92672">in</span> TRAIN_DATA:
                nlp<span style="color:#f92672">.</span>update([text], [annotations], sgd<span style="color:#f92672">=</span>optimizer, losses<span style="color:#f92672">=</span>losses)
            <span style="color:#66d9ef">print</span>(losses)

    <span style="color:#75715e"># test the trained model</span>
    test_model(nlp)

    <span style="color:#75715e"># save model to output directory</span>
    <span style="color:#66d9ef">if</span> output_dir <span style="color:#f92672">is</span> <span style="color:#f92672">not</span> None:
        output_dir <span style="color:#f92672">=</span> Path(output_dir)
        <span style="color:#66d9ef">if</span> <span style="color:#f92672">not</span> output_dir<span style="color:#f92672">.</span>exists():
            output_dir<span style="color:#f92672">.</span>mkdir()
        nlp<span style="color:#f92672">.</span>to_disk(output_dir)
        <span style="color:#66d9ef">print</span>(<span style="color:#e6db74">&#34;Saved model to&#34;</span>, output_dir)

        <span style="color:#75715e"># test the saved model</span>
        <span style="color:#66d9ef">print</span>(<span style="color:#e6db74">&#34;Loading from&#34;</span>, output_dir)
        nlp2 <span style="color:#f92672">=</span> spacy<span style="color:#f92672">.</span>load(output_dir)
        test_model(nlp2)


<span style="color:#66d9ef">def</span> <span style="color:#a6e22e">test_model</span>(nlp):
    texts <span style="color:#f92672">=</span> [<span style="color:#e6db74">&#34;find a hotel with good wifi&#34;</span>,
             <span style="color:#e6db74">&#34;find me the cheapest gym near work&#34;</span>,
             <span style="color:#e6db74">&#34;show me the best hotel in berlin&#34;</span>]
    docs <span style="color:#f92672">=</span> nlp<span style="color:#f92672">.</span>pipe(texts)
    <span style="color:#66d9ef">for</span> doc <span style="color:#f92672">in</span> docs:
        <span style="color:#66d9ef">print</span>(doc<span style="color:#f92672">.</span>text)
        <span style="color:#66d9ef">print</span>([(t<span style="color:#f92672">.</span>text, t<span style="color:#f92672">.</span>dep_, t<span style="color:#f92672">.</span>head<span style="color:#f92672">.</span>text) <span style="color:#66d9ef">for</span> t <span style="color:#f92672">in</span> doc <span style="color:#66d9ef">if</span> t<span style="color:#f92672">.</span>dep_ <span style="color:#f92672">!=</span> <span style="color:#e6db74">&#39;-&#39;</span>])


<span style="color:#66d9ef">if</span> __name__ <span style="color:#f92672">==</span> <span style="color:#e6db74">&#39;__main__&#39;</span>:
    plac<span style="color:#f92672">.</span>call(main)

    <span style="color:#75715e"># Expected output:</span>
    <span style="color:#75715e"># find a hotel with good wifi</span>
    <span style="color:#75715e"># [</span>
    <span style="color:#75715e">#   (&#39;find&#39;, &#39;ROOT&#39;, &#39;find&#39;),</span>
    <span style="color:#75715e">#   (&#39;hotel&#39;, &#39;PLACE&#39;, &#39;find&#39;),</span>
    <span style="color:#75715e">#   (&#39;good&#39;, &#39;QUALITY&#39;, &#39;wifi&#39;),</span>
    <span style="color:#75715e">#   (&#39;wifi&#39;, &#39;ATTRIBUTE&#39;, &#39;hotel&#39;)</span>
    <span style="color:#75715e"># ]</span>
    <span style="color:#75715e"># find me the cheapest gym near work</span>
    <span style="color:#75715e"># [</span>
    <span style="color:#75715e">#   (&#39;find&#39;, &#39;ROOT&#39;, &#39;find&#39;),</span>
    <span style="color:#75715e">#   (&#39;cheapest&#39;, &#39;QUALITY&#39;, &#39;gym&#39;),</span>
    <span style="color:#75715e">#   (&#39;gym&#39;, &#39;PLACE&#39;, &#39;find&#39;)</span>
    <span style="color:#75715e">#   (&#39;work&#39;, &#39;LOCATION&#39;, &#39;near&#39;)</span>
    <span style="color:#75715e"># ]</span>
    <span style="color:#75715e"># show me the best hotel in berlin</span>
    <span style="color:#75715e"># [</span>
    <span style="color:#75715e">#   (&#39;show&#39;, &#39;ROOT&#39;, &#39;show&#39;),</span>
    <span style="color:#75715e">#   (&#39;best&#39;, &#39;QUALITY&#39;, &#39;hotel&#39;),</span>
    <span style="color:#75715e">#   (&#39;hotel&#39;, &#39;PLACE&#39;, &#39;show&#39;),</span>
    <span style="color:#75715e">#   (&#39;berlin&#39;, &#39;LOCATION&#39;, &#39;hotel&#39;)</span>
    <span style="color:#75715e"># ]</span></code></pre></div>
<blockquote>
<p>Example illustrate real power of spaCy in creating custom models, both retrain model with domain ken and traing completely new DEP</p>
</blockquote>

<ul>
<li><a href="https://stackoverflow.com/questions/36610179/how-to-get-the-dependency-tree-with-spacy">Dependency Tree with spaCy</a></li>
<li><a href="https://explosion.ai/blog/parsing-english-in-python">Parsing English in 500 Lines</a></li>
</ul>

<h2 id="topic-models">Topic Models</h2>

<ul>
<li><strong>Previous dealt with CompLing algo and SpaCy, how to use them to ANNOTATE data and decipher sentence structure, finer details of text</strong></li>
<li>BUT BIG PICTURE and theme!</li>
<li>Definition
&gt; Probabilistic model having info on topics in the text
&gt;     1. Topic can be theme, or underlying ideas EX corpus of news topically weather, politics, sports etc
&gt;     2. Useful to Represent documents as topic DISTRIBUTIONS ! (instead of BOW or TF-IDF)
&gt;     3. Cluster in topics, further zoom in one topic to decipher deeper topics/themes !!
&gt;     4. Chronological / time-stamped Topic variation reveals info (DYNAMIC TOPIC MODELING)
&gt;     5. NOTE TOPIC ~ Distribution(Words) NOT labelled or titled (e.g. weather is a collection of sun, temperature, storm, forecast)
&gt;     6. Human assign topic from Distribution
&gt;     7. Theoretical Papers <a href="http://www.jmlr.org/papers/volume3/blei03a/blei03a.pdf">Blei LDA</a>, <a href="http://blog.echen.met/2011/08/22/introduction-to-latent-dirichlet-allocation/">Edwin Chen</a>, <a href="http://www.cs.columbia.edu/~blei/papers/Blei2012.pdf">Blei Probabilistic Topic Models</a></li>
</ul>

<h4 id="topic-models-in-gensim">Topic Models in GENSIM</h4>

<ul>
<li>Arguably most popular TM due to many algos

<ul>
<li><a href="https://github.com/bhargavvader/personal/blob/master/notebooks/text_analysis_tutorial/topic_modelling.ipynb">Topic Modelling</a></li>
</ul></li>
<li>Hierarchical Dirichlet Process <strong>non-parametric</strong> no hyperparam of topic-number

<ul>
<li><a href="https://nips.cc/">NIPS</a> and <a href="http://papers.nips.cc/paper/2698-sharing-clusters-among-related-groups-hierarchical-dirichlet-processes.pdf">Sharing Clusters Among Related Groups: Hierarchical Dirichlet Processes</a></li>
</ul></li>
<li>DTM - time-stamped evolution of topics

<ul>
<li>Unlikely see underlying topic change but prominence and replacement</li>
</ul></li>
<li>GENSIM <a href="https://github.com/RaRe-Technologies/gensim/blob/develop/docs/notebooks/ldaseqmodel.ipynb">notebook</a></li>
</ul>

<h4 id="topic-model-in-skl">Topic Model in SKL</h4>

<ul>
<li>Fast LDA and NMF (<strong>NonNegative Matrix Factorization</strong>)</li>
<li>Differing to GENSIM

<ol>
<li>Perplexity bounds are not expected to agree exactly here as bound is computed differently, how topics CONVERGE in TM algos</li>
<li>SKL uses CYTHON in making numerical 6th decimal point differences</li>
</ol></li>
<li><a href="http://papers.nips.cc/paper/1861-algorithms-for-non-negative-matrix-factorization">NMF</a> is LinAlg reconstructing a Single Matrix V into W and H, used to identify topics as they best represent original V - document matrix having info on words in docs</li>
<li>Positive-Semi-Definite is inherent property in audio / text processing - insolvable in closed-form but numerically approx, by DISTANCE NORM Euclidean Norm2 often, and <a href="https://projecteuclid.org/euclid.aoms/1177729694">Kullback-Leibler Divergence</a></li>
<li>NMF used for DimReduction, source separation, topic extraction, etc - this example uses generalised KL divergence, equivalent to <a href="https://arxiv.org/ftp/arxiv/papers/1301/1301.6705.pdf">Probabilistic Latent Sementic Indexing PLSI</a></li>
<li>SLK consistent pipelien <strong>fit, transform, predict</strong> DECOMPOSITION ONLY USE FIT then extract components</li>
</ul>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-python" data-lang="python"><span style="color:#75715e"># we need to first set up the text and corpus as it was done in section 3.3</span>
<span style="color:#75715e"># this refers to the code set-up in the Chapter 3</span>

<span style="color:#f92672">from</span> gensim.models <span style="color:#f92672">import</span> LdaModel

ldamodel <span style="color:#f92672">=</span> LdaModel(corpus<span style="color:#f92672">=</span>corpus, num_topics<span style="color:#f92672">=</span><span style="color:#ae81ff">10</span>, id2word<span style="color:#f92672">=</span>dictionary)
ldamodel<span style="color:#f92672">.</span>show_topics()

lsimodel <span style="color:#f92672">=</span> LsiModel(corpus<span style="color:#f92672">=</span>corpus, num_topics<span style="color:#f92672">=</span><span style="color:#ae81ff">10</span>, id2word<span style="color:#f92672">=</span>dictionary)
lsimodel<span style="color:#f92672">.</span>show_topics(num_topics<span style="color:#f92672">=</span><span style="color:#ae81ff">5</span>)  <span style="color:#75715e"># Showing only the top 5 topics</span>

hdpmodel <span style="color:#f92672">=</span> HdpModel(corpus<span style="color:#f92672">=</span>corpus, id2word<span style="color:#f92672">=</span>dictionary)
hdpmodel<span style="color:#f92672">.</span>show_topics()




<span style="color:#f92672">from</span> sklearn.decomposition <span style="color:#f92672">import</span> NMF, LatentDirichletAllocation
nmf <span style="color:#f92672">=</span> NMF(n_components<span style="color:#f92672">=</span>no_topic)<span style="color:#f92672">.</span>fit(tfidf_corpus)
lda <span style="color:#f92672">=</span> LatentDirichletAllocation(n_topics<span style="color:#f92672">=</span>no_topics)<span style="color:#f92672">.</span>fit(tf_corpus)

<span style="color:#66d9ef">def</span> <span style="color:#a6e22e">display_topics</span>(model, feature_names, no_top_words):
    <span style="color:#66d9ef">for</span> topic_idx, topic <span style="color:#f92672">in</span> enumerate(model<span style="color:#f92672">.</span>components_):
        <span style="color:#66d9ef">print</span> <span style="color:#e6db74">&#34;Topic </span><span style="color:#e6db74">%d</span><span style="color:#e6db74">:&#34;</span> <span style="color:#f92672">%</span> (topic_idx)
        <span style="color:#66d9ef">print</span> <span style="color:#e6db74">&#34; &#34;</span><span style="color:#f92672">.</span>join([feature_names[i]
                        <span style="color:#66d9ef">for</span> i <span style="color:#f92672">in</span> topic<span style="color:#f92672">.</span>argsort()[:<span style="color:#f92672">-</span>no_top_words <span style="color:#f92672">-</span> <span style="color:#ae81ff">1</span>:<span style="color:#f92672">-</span><span style="color:#ae81ff">1</span>]])

display_topics(nmf, tfidf_feature_names, no_top_words)</code></pre></div>
<h2 id="advanced-topic-model">Advanced Topic Model</h2>

<ul>
<li>Previous Preprocessing (Language Model and Vectorising-Transformation) geared more towards generating TM than other forms of Text Analysis Algo</li>
<li>E.g. Lemmatisation instead of Stemming especially useful in TM as lemmatised words tend to be more legible than stemming.</li>
<li>Similary bi-grams or tri-grams as part of CORPUS before applying TM give further legbility</li>
<li>TM ultimately is human-understanding, unlike Clustering only higher accuracy</li>
<li>Any preprocessing customised in pipeline conducive to that goal is preferred</li>
<li>Multiple Runs of TM may required before any meaningful result, e.g. adding new stop words after viewing first TM</li>
<li>EX removing lemmatised SAY
<code>python
my_stop = [u'say', u'\'s', u'Mr', u'be', u'said', u'says', u'saying']
for stopwrod in my_stop:
    lexeme = nlp.vocab[stopword]
    lexeme.is_stop = True
</code>
&gt; For every word to add as stop, change <code>is_stop</code> attr for that <code>lexeme</code> class, which are case-insensitive, so ignorable</li>
<li>A more common way to remove stop is to put all in list and remove from Corpus (e.g. in NLTK <code>from nltk.corpus import stopwords; stopword_list = stopwords.words(&quot;english&quot;)</code></li>
<li>Another way is GENSIM <code>Dictionary</code> class
<code>python
filter_n_most_frequent(remove_n)
from gensim.corpora import Dictionary
corpus = [[ 'mama', 'mela', 'maso'], ['ema', 'ma', 'mama']]
dct = Dictionary(corpus)
dct.filter_n_most_frequent(2)
</code>
&gt; this process of TM, often manually inspecting and change as need is common in almost all ML or DS projects, in text, the extra is human interpretable nature of results</li>
</ul>

<h4 id="hyperparam-in-tm">HyperParam in TM</h4>

<ul>
<li>GENSIM

<ol>
<li><code>chunksize</code> controls # doc processed at once in training algo - speed for RAM fit</li>
<li><code>passes</code> controls how often train model on entire corpus or <strong>epochs</strong></li>
<li><code>iterations</code> controls freq repeating a loop over each doc, often higher</li>
</ol></li>
<li><a href="https://radimrehurek.com/gensim/models/ldamodel.html">LdaModel</a> and <a href="http://scikit-learn.org/stable/modules/generated/sklearn.decomposition.Laten-DirichletAllocation.html">LDA in SKL</a>

<ol>
<li><strong>Alpha</strong> repr doc-topic density, higher the more topics</li>
<li><strong>Beta</strong> repr topic-word density</li>
<li><strong>Numer of topics</strong></li>
</ol></li>

<li><p>Logging is useful to monitor during training (GENSIM)</p>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-python" data-lang="python"><span style="color:#f92672">import</span> logging
logging<span style="color:#f92672">.</span>basicConfig(filename<span style="color:#f92672">=</span><span style="color:#e6db74">&#39;logfile.log&#39;</span>, format<span style="color:#f92672">=</span><span style="color:#e6db74">&#39;</span><span style="color:#e6db74">%(asctime)s</span><span style="color:#e6db74"> : </span><span style="color:#e6db74">%(levelname)s</span><span style="color:#e6db74"> : </span><span style="color:#e6db74">%(message)s</span><span style="color:#e6db74">&#39;</span>, level<span style="color:#f92672">=</span>logging<span style="color:#f92672">.</span>INFO)
    
<span style="color:#75715e"># document - topic proportions</span>
ldamodel[corpus[<span style="color:#ae81ff">0</span>]] 
    
<span style="color:#75715e"># printing first topic</span>
    
ldamodel<span style="color:#f92672">.</span>show_topics()[<span style="color:#ae81ff">1</span>]
    
texts <span style="color:#f92672">=</span> [[<span style="color:#e6db74">&#39;bank&#39;</span>,<span style="color:#e6db74">&#39;river&#39;</span>,<span style="color:#e6db74">&#39;shore&#39;</span>,<span style="color:#e6db74">&#39;water&#39;</span>],
        [<span style="color:#e6db74">&#39;river&#39;</span>,<span style="color:#e6db74">&#39;water&#39;</span>,<span style="color:#e6db74">&#39;flow&#39;</span>,<span style="color:#e6db74">&#39;fast&#39;</span>,<span style="color:#e6db74">&#39;tree&#39;</span>],
        [<span style="color:#e6db74">&#39;bank&#39;</span>,<span style="color:#e6db74">&#39;water&#39;</span>,<span style="color:#e6db74">&#39;fall&#39;</span>,<span style="color:#e6db74">&#39;flow&#39;</span>],
        [<span style="color:#e6db74">&#39;bank&#39;</span>,<span style="color:#e6db74">&#39;bank&#39;</span>,<span style="color:#e6db74">&#39;water&#39;</span>,<span style="color:#e6db74">&#39;rain&#39;</span>,<span style="color:#e6db74">&#39;river&#39;</span>],
        [<span style="color:#e6db74">&#39;river&#39;</span>,<span style="color:#e6db74">&#39;water&#39;</span>,<span style="color:#e6db74">&#39;mud&#39;</span>,<span style="color:#e6db74">&#39;tree&#39;</span>],
        [<span style="color:#e6db74">&#39;money&#39;</span>,<span style="color:#e6db74">&#39;transaction&#39;</span>,<span style="color:#e6db74">&#39;bank&#39;</span>,<span style="color:#e6db74">&#39;finance&#39;</span>],
        [<span style="color:#e6db74">&#39;bank&#39;</span>,<span style="color:#e6db74">&#39;borrow&#39;</span>,<span style="color:#e6db74">&#39;money&#39;</span>], 
        [<span style="color:#e6db74">&#39;bank&#39;</span>,<span style="color:#e6db74">&#39;finance&#39;</span>],
        [<span style="color:#e6db74">&#39;finance&#39;</span>,<span style="color:#e6db74">&#39;money&#39;</span>,<span style="color:#e6db74">&#39;sell&#39;</span>,<span style="color:#e6db74">&#39;bank&#39;</span>],
        [<span style="color:#e6db74">&#39;borrow&#39;</span>,<span style="color:#e6db74">&#39;sell&#39;</span>],
        [<span style="color:#e6db74">&#39;bank&#39;</span>,<span style="color:#e6db74">&#39;loan&#39;</span>,<span style="color:#e6db74">&#39;sell&#39;</span>]]
    
model<span style="color:#f92672">.</span>get_term_topics(<span style="color:#e6db74">&#39;water&#39;</span>)
model<span style="color:#f92672">.</span>get_term_topics(<span style="color:#e6db74">&#39;finance&#39;</span>)
    
bow_water <span style="color:#f92672">=</span> [<span style="color:#e6db74">&#39;bank&#39;</span>,<span style="color:#e6db74">&#39;water&#39;</span>,<span style="color:#e6db74">&#39;bank&#39;</span>]
bow_finance <span style="color:#f92672">=</span> [<span style="color:#e6db74">&#39;bank&#39;</span>,<span style="color:#e6db74">&#39;finance&#39;</span>,<span style="color:#e6db74">&#39;bank&#39;</span>]
bow <span style="color:#f92672">=</span> model<span style="color:#f92672">.</span>id2word<span style="color:#f92672">.</span>doc2bow(bow_water) <span style="color:#75715e"># convert to bag of words format first</span>
doc_topics, word_topics, phi_values <span style="color:#f92672">=</span> model<span style="color:#f92672">.</span>get_document_topics(bow, per_word_topics<span style="color:#f92672">=</span>True)</code></pre></div></li>

<li><p>GENSIM <a href="https://github.com/RaRe-Technologies/gensim/wiki/Recipes-&amp;-FAQ">FAQ</a> and <a href="https://miningthedetails.com/blog/python/lda/GensimLDA/">Chris Tufts blog</a></p></li>
</ul>

<h4 id="exploring-documents-after-satisfactory-tm-runs">Exploring Documents after Satisfactory TM runs</h4>

<ul>
<li><a href="https://github.com/RaRe-Technologies/gensim/blob/develop/docs/notebooks/topic_methods.ipynb">Top Methods</a>
&gt; As shown, based on context, most likely topics associated with a word can vary, differing from <code>get_term_topics</code> where it is a STATIC Topic Distribution
&gt;     1. NOTE GENSIM implementation of LDA uses <strong>VARIATIONAL BAYES SAMPLING</strong>, a <code>word_type</code> in doc is onlly given one Topic Distribution. E.g. <code>the bank by the river bank</code> is likely to be assigned to topic_0 and each of bank word instances has the same distribution
&gt;     2. These 2 methods ensemble to infer further info from using TM - topic distribution means able to use info to do some visualisation - colour all words in doc based on which topic belonging to, or usng distance metrics to infer how close or far pairs of topics are</li>
<li><a href="https://github.com/RaRe-Technologies/gensim/blob/develop/docs/notebooks/distance_metrics.ipynb">Distance Metric</a></li>
<li><a href="https://towardsdatascience.com/improving-the-interpretation-of-topic-models-87fd2ee3847d">SKL Implementation</a></li>
</ul>

<h4 id="topic-coherence-and-evaluation">Topic Coherence and Evaluation</h4>

<ul>
<li><p>Previous more Qualitative measures</p>

<blockquote>
<p>Topic Coherence <a href="https://rare-technologies.com/what-is-topic-coherence/">overview</a> and <a href="https://svn.aksw.org/papers/2015/WSDM_Topic_Evaluation/public.pdf">Exploring Space of Topic Coherence</a></p>
</blockquote></li>

<li><p>In essence, TC is <strong>quantative measure</strong> of TM, making possible comparing two models <strong>RIGHT, OPTIMAL NUMBER OF TOPICS</strong> is the goal</p></li>

<li><p>Before Topic Coherence, <a href="http://qpleple.com/perplexity-to-evalutate-topic-models/">perplexity</a> used to measure model fit</p></li>

<li><p>Resources</p>

<ol>
<li><a href="https://radimrehurek.com/gensim/models/coherencemodel.html">Coherence Model Pipeline</a></li>
<li><a href="https://github.com/RaRe-Technologies/gensim/blob/develop/docs/notebooks/gensim_news_classification.ipynb">News Classification with GENSIM</a></li>
<li><a href="https://github.com/Rare-Technologies/gensim/blob/develop/docs/notebooks/topic_coherence-movies.ipynb">TC on Movies Dataset</a></li>
<li><a href="https://github.com/Rare-Technologies/gensim/blob/develop/docs/notebooks/topic_coherence_tutorial.ipynb">TC Intro</a></li>
<li><a href="https://gist.github.com/dsquareindia/ac9d3bf57579d02302f9655db8dfdd55">TC Use Cases</a></li>
<li><a href="https://github.com/Rare-Technologies/gensim/blob/develop/docs/notebooks/topic_coherence_model_selection.ipynb">TC Model Selection</a></li>
</ol></li>
</ul>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-python" data-lang="python"><span style="color:#75715e"># coherence models</span>

lsi_coherence <span style="color:#f92672">=</span> CoherenceModel(topics<span style="color:#f92672">=</span>lsitopics[:<span style="color:#ae81ff">10</span>], texts<span style="color:#f92672">=</span>texts, dictionary<span style="color:#f92672">=</span>dictionary, window_size<span style="color:#f92672">=</span><span style="color:#ae81ff">10</span>)
hdp_coherence <span style="color:#f92672">=</span> CoherenceModel(topics<span style="color:#f92672">=</span>hdptopics[:<span style="color:#ae81ff">10</span>], texts<span style="color:#f92672">=</span>texts, dictionary<span style="color:#f92672">=</span>dictionary, window_size<span style="color:#f92672">=</span><span style="color:#ae81ff">10</span>)
lda_coherence <span style="color:#f92672">=</span> CoherenceModel(topics<span style="color:#f92672">=</span>ldatopics, texts<span style="color:#f92672">=</span>texts, dictionary<span style="color:#f92672">=</span>dictionary, window_size<span style="color:#f92672">=</span><span style="color:#ae81ff">10</span>)

<span style="color:#75715e"># train two models, one poorly trained (1 pass), and one trained with more passes (50 passes)</span>

<span style="color:#66d9ef">print</span>(goodcm<span style="color:#f92672">.</span>get_coherence())
<span style="color:#66d9ef">print</span>(badcm<span style="color:#f92672">.</span>get_coherence())


c_v <span style="color:#f92672">=</span> []
<span style="color:#66d9ef">for</span> num_topics <span style="color:#f92672">in</span> range(<span style="color:#ae81ff">1</span>, limit):
        lm <span style="color:#f92672">=</span> LdaModel(corpus<span style="color:#f92672">=</span>corpus, num_topics<span style="color:#f92672">=</span>num_topics, id2word<span style="color:#f92672">=</span>dictionary)
        cm <span style="color:#f92672">=</span> CoherenceModel(model<span style="color:#f92672">=</span>lm, texts<span style="color:#f92672">=</span>texts, dictionary<span style="color:#f92672">=</span>dictionary,          coherence<span style="color:#f92672">=</span><span style="color:#e6db74">&#39;c_v&#39;</span>)
        c_v<span style="color:#f92672">.</span>append(cm<span style="color:#f92672">.</span>get_coherence())</code></pre></div>
<h4 id="visualising-tm">Visualising TM</h4>

<ul>
<li>TM better understood qualitatively on textual data - visual is best ways to check</li>

<li><p><code>pyLDAvis</code> agnostic to model trained - beyond GENSIM or LDA - only require topic-term distributions and document-topic distributions plus basic info on corpus trained on</p>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-python" data-lang="python"><span style="color:#f92672">import</span> pyLDAvis.gensim
pyLDAvis<span style="color:#f92672">.</span>gensim<span style="color:#f92672">.</span>prepare(model, corpus, dictionary)</code></pre></div>
<ol>
<li>Model is palceholder for trained LDA model for example</li>
<li><a href="http://nbviewer.jupyter.org/github/bmabey/pyLDAvis/blob/master/notebooks/pyLDAvis_overview.ipynb">Full notebook</a></li>
</ol></li>

<li><p>Visualising Live Training Model (coherence, perplexity, etc)</p>

<ul>
<li><a href="https://github.com/facebookresearch/visdom">visdom server</a></li>
<li><a href="https://github.com/parulsethi/gensim/blob/tensorboard_logs/docs/notebooks/Training_visualisations.ipynb">GENSIM Setup</a></li>
<li>Further viewed as CLUSTER by <a href="https://shuaiw.github.io/2016/12/22/topic-modeling-and-tsne-visualzation.html">T-SNE</a></li>
<li>Also Clustering via <a href="https://github.com/Rare-Technologies/gensim/blob/develop/docs/notebooks/Tensorboard_visualisations.ipynb"><strong>WORD2VEC</strong> </a></li>
<li><a href="https://github.com/Rare-Technologies/gensim/blob/develop/docs/notebooks/Topic_dendrogram.ipynb">Dendrograms</a></li>
</ul></li>

<li><p>Visual Resources</p>

<ol>
<li><a href="https://de.dariah.eu/tatom/visualizing_trends.html">Visualising Trend</a></li>
<li><a href="https://de.ariah.eu/tatom/topic_model_visualizaiton.html">Visualizing Topic Share</a></li>
<li><a href="https://www.aaai.org/ocs/index.php/ICWSM/ICWSM12/paper/viewFile/4645/5021">Blei Visual</a></li>
</ol></li>
</ul>

<h2 id="clustering-and-classifying">Clustering and Classifying</h2>

<h4 id="clustering">Clustering</h4>

<ul>
<li>RECAP
&gt; so far processing text or corpus via POS, NER, what kind of words present; in TM to seek theme hidden;
&gt;     1. TM could be used to cluster articles, BUT it is NOT its purpose!
&gt;     2. E.g. after performing TM, a doc can be made of 30% topic 1, 30% topic 2, etc, hence no way to cluster</li>
<li>Datapoint as documents or words</li>
<li>EXTRA CAUTION of Text: high number of dimension in text vector !! - entire vocab or corpus (Best effort via of DimReduction via SVD, LDA, LSI, etc)</li>
<li>Pipeline: rid of stop, lemmatise, vectorise</li>
<li><a href="https://github.com/bhargavvader/personal/blob/master/notebooks/clustering_classing.ipynb">Example</a></li>
<li><a href="https://towardsdatascience.com/automatic-topic-clustering-using-doc2vec-e1cea88449c">Doc2Vec Clustering</a></li>
</ul>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-python" data-lang="python"><span style="color:#75715e"># using scikit-learn</span>

<span style="color:#f92672">from</span> sklearn.datasets <span style="color:#f92672">import</span> fetch_20newsgroups


categories <span style="color:#f92672">=</span> [
    <span style="color:#e6db74">&#39;alt.atheism&#39;</span>,
    <span style="color:#e6db74">&#39;talk.religion.misc&#39;</span>,
    <span style="color:#e6db74">&#39;comp.graphics&#39;</span>,
    <span style="color:#e6db74">&#39;sci.space&#39;</span>,
]

dataset <span style="color:#f92672">=</span> fetch_20newsgroups(subset<span style="color:#f92672">=</span><span style="color:#e6db74">&#39;all&#39;</span>, categories<span style="color:#f92672">=</span>categories, shuffle<span style="color:#f92672">=</span>True, random_state<span style="color:#f92672">=</span><span style="color:#ae81ff">42</span>)

labels <span style="color:#f92672">=</span> dataset<span style="color:#f92672">.</span>target
true_k <span style="color:#f92672">=</span> np<span style="color:#f92672">.</span>unique(labels)<span style="color:#f92672">.</span>shape[<span style="color:#ae81ff">0</span>]
data <span style="color:#f92672">=</span> dataset<span style="color:#f92672">.</span>data  

<span style="color:#f92672">from</span> sklearn.feature_extraction.text <span style="color:#f92672">import</span> TfidfVectorizer

vectorizer <span style="color:#f92672">=</span> TfidfVectorizer(max_df<span style="color:#f92672">=</span><span style="color:#ae81ff">0.5</span>, min_df<span style="color:#f92672">=</span><span style="color:#ae81ff">2</span>, stop_words<span style="color:#f92672">=</span><span style="color:#e6db74">&#39;english&#39;</span>, use_idf<span style="color:#f92672">=</span>True)

X <span style="color:#f92672">=</span> vectorizer<span style="color:#f92672">.</span>fit_transform(data)


<span style="color:#f92672">from</span> sklearn.decomposition <span style="color:#f92672">import</span> PCA


newsgroups_train <span style="color:#f92672">=</span> fetch_20newsgroups(subset<span style="color:#f92672">=</span><span style="color:#e6db74">&#39;train&#39;</span>, 
                                      categories<span style="color:#f92672">=</span>[<span style="color:#e6db74">&#39;alt.atheism&#39;</span>, <span style="color:#e6db74">&#39;sci.space&#39;</span>])
pipeline <span style="color:#f92672">=</span> Pipeline([
    (<span style="color:#e6db74">&#39;vect&#39;</span>, CountVectorizer()),
    (<span style="color:#e6db74">&#39;tfidf&#39;</span>, TfidfTransformer()),
])        
X_visualise <span style="color:#f92672">=</span> pipeline<span style="color:#f92672">.</span>fit_transform(newsgroups_train<span style="color:#f92672">.</span>data)<span style="color:#f92672">.</span>todense()

pca <span style="color:#f92672">=</span> PCA(n_components<span style="color:#f92672">=</span><span style="color:#ae81ff">2</span>)<span style="color:#f92672">.</span>fit(X_visualise)
data2D <span style="color:#f92672">=</span> pca<span style="color:#f92672">.</span>transform(X_visualise)
plt<span style="color:#f92672">.</span>scatter(data2D[:,<span style="color:#ae81ff">0</span>], data2D[:,<span style="color:#ae81ff">1</span>], c<span style="color:#f92672">=</span>newsgroups_train<span style="color:#f92672">.</span>target)


n_components <span style="color:#f92672">=</span> <span style="color:#ae81ff">5</span>
svd <span style="color:#f92672">=</span> TruncatedSVD(n_components)
normalizer <span style="color:#f92672">=</span> Normalizer(copy<span style="color:#f92672">=</span>False)
lsa <span style="color:#f92672">=</span> make_pipeline(svd, normalizer)

X <span style="color:#f92672">=</span> lsa<span style="color:#f92672">.</span>fit_transform(X)


Minibatch <span style="color:#f92672">=</span> True
<span style="color:#66d9ef">if</span> minibatch:
    km <span style="color:#f92672">=</span> MiniBatchKMeans(n_clusters<span style="color:#f92672">=</span>true_k, init<span style="color:#f92672">=</span><span style="color:#e6db74">&#39;k-means++&#39;</span>, n_init<span style="color:#f92672">=</span><span style="color:#ae81ff">1</span>, init_size<span style="color:#f92672">=</span><span style="color:#ae81ff">1000</span>, batch_size<span style="color:#f92672">=</span><span style="color:#ae81ff">1000</span>)
<span style="color:#66d9ef">else</span>:
    km <span style="color:#f92672">=</span> KMeans(n_clusters<span style="color:#f92672">=</span>true_k, init<span style="color:#f92672">=</span><span style="color:#e6db74">&#39;k-means++&#39;</span>, max_iter<span style="color:#f92672">=</span><span style="color:#ae81ff">100</span>, n_init<span style="color:#f92672">=</span><span style="color:#ae81ff">1</span>)
km<span style="color:#f92672">.</span>fit(X)

original_space_centroids <span style="color:#f92672">=</span> svd<span style="color:#f92672">.</span>inverse_transform(km<span style="color:#f92672">.</span>cluster_centers_) 

order_centroids <span style="color:#f92672">=</span> original_space_centroids<span style="color:#f92672">.</span>argsort()[:, ::<span style="color:#f92672">-</span><span style="color:#ae81ff">1</span>]

<span style="color:#75715e"># [The above bit of code is necessary because of our LSI transformation]</span>

terms <span style="color:#f92672">=</span> vectorizer<span style="color:#f92672">.</span>get_feature_names()

<span style="color:#66d9ef">for</span> i <span style="color:#f92672">in</span> range(true_k):
    <span style="color:#66d9ef">print</span>(<span style="color:#e6db74">&#34;Cluster </span><span style="color:#e6db74">%d</span><span style="color:#e6db74">:&#34;</span> <span style="color:#f92672">%</span> i)
    <span style="color:#66d9ef">for</span> ind <span style="color:#f92672">in</span> order_centroids[i, :<span style="color:#ae81ff">10</span>]:
        <span style="color:#66d9ef">print</span>(<span style="color:#e6db74">&#39; </span><span style="color:#e6db74">%s</span><span style="color:#e6db74">&#39;</span> <span style="color:#f92672">%</span> terms[ind])

<span style="color:#f92672">from</span> sklearn.metrics.pairwise <span style="color:#f92672">import</span> cosine_similarity
dist <span style="color:#f92672">=</span> <span style="color:#ae81ff">1</span> <span style="color:#f92672">-</span> cosine_similarity(X)

<span style="color:#f92672">from</span> scipy.cluster.hierarchy <span style="color:#f92672">import</span> ward, dendrogram

linkage_matrix <span style="color:#f92672">=</span> ward(dist) 
fig, ax <span style="color:#f92672">=</span> plt<span style="color:#f92672">.</span>subplots(figsize<span style="color:#f92672">=</span>(<span style="color:#ae81ff">10</span>, <span style="color:#ae81ff">15</span>)) <span style="color:#75715e"># set size</span>
ax <span style="color:#f92672">=</span> dendrogram(linkage_matrix, orientation<span style="color:#f92672">=</span><span style="color:#e6db74">&#34;right&#34;</span>)</code></pre></div>
<h4 id="classifying">Classifying</h4>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-python" data-lang="python"><span style="color:#75715e"># classification</span>

<span style="color:#f92672">from</span> sklearn.naive_bayes <span style="color:#f92672">import</span> GaussianNB
gnb <span style="color:#f92672">=</span> GaussianNB()
gnb<span style="color:#f92672">.</span>fit(X, labels)

<span style="color:#f92672">from</span> sklearn.svm <span style="color:#f92672">import</span> SVC
svm <span style="color:#f92672">=</span> SVC()
svm<span style="color:#f92672">.</span>fit(X, labels)</code></pre></div>
<h2 id="similarity-queries-and-summarisation">Similarity Queries and Summarisation</h2>

<ul>
<li>Vectorised Text opens door to simiarlity or distance</li>
</ul>

<h4 id="similarity-metrics">Similarity Metrics</h4>

<ul>
<li><p><a href="https://github.com/Rare-Technologies/gensim/blob/develop/docs/notebooks/distance.metrics.ipynb">notebook</a></p>

<h4 id="similarity-queries">Similarity Queries</h4></li>

<li><p>extract out most similar for an input query - simply index each of doc then search for lowest distance returned between corpus and query, and return the docu with lowest distance</p></li>
</ul>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-python" data-lang="python"><span style="color:#75715e"># make sure to have appropriate gensim installations and imports done</span>

texts <span style="color:#f92672">=</span> [[<span style="color:#e6db74">&#39;bank&#39;</span>,<span style="color:#e6db74">&#39;river&#39;</span>,<span style="color:#e6db74">&#39;shore&#39;</span>,<span style="color:#e6db74">&#39;water&#39;</span>],
        [<span style="color:#e6db74">&#39;river&#39;</span>,<span style="color:#e6db74">&#39;water&#39;</span>,<span style="color:#e6db74">&#39;flow&#39;</span>,<span style="color:#e6db74">&#39;fast&#39;</span>,<span style="color:#e6db74">&#39;tree&#39;</span>],
        [<span style="color:#e6db74">&#39;bank&#39;</span>,<span style="color:#e6db74">&#39;water&#39;</span>,<span style="color:#e6db74">&#39;fall&#39;</span>,<span style="color:#e6db74">&#39;flow&#39;</span>],
        [<span style="color:#e6db74">&#39;bank&#39;</span>,<span style="color:#e6db74">&#39;bank&#39;</span>,<span style="color:#e6db74">&#39;water&#39;</span>,<span style="color:#e6db74">&#39;rain&#39;</span>,<span style="color:#e6db74">&#39;river&#39;</span>],
        [<span style="color:#e6db74">&#39;river&#39;</span>,<span style="color:#e6db74">&#39;water&#39;</span>,<span style="color:#e6db74">&#39;mud&#39;</span>,<span style="color:#e6db74">&#39;tree&#39;</span>],
        [<span style="color:#e6db74">&#39;money&#39;</span>,<span style="color:#e6db74">&#39;transaction&#39;</span>,<span style="color:#e6db74">&#39;bank&#39;</span>,<span style="color:#e6db74">&#39;finance&#39;</span>],
        [<span style="color:#e6db74">&#39;bank&#39;</span>,<span style="color:#e6db74">&#39;borrow&#39;</span>,<span style="color:#e6db74">&#39;money&#39;</span>], 
        [<span style="color:#e6db74">&#39;bank&#39;</span>,<span style="color:#e6db74">&#39;finance&#39;</span>],
        [<span style="color:#e6db74">&#39;finance&#39;</span>,<span style="color:#e6db74">&#39;money&#39;</span>,<span style="color:#e6db74">&#39;sell&#39;</span>,<span style="color:#e6db74">&#39;bank&#39;</span>],
        [<span style="color:#e6db74">&#39;borrow&#39;</span>,<span style="color:#e6db74">&#39;sell&#39;</span>],
        [<span style="color:#e6db74">&#39;bank&#39;</span>, <span style="color:#e6db74">&#39;loan&#39;</span>, <span style="color:#e6db74">&#39;sell&#39;</span>]

dictionary <span style="color:#f92672">=</span> Dictionary(texts)
corpus <span style="color:#f92672">=</span> [dictionary<span style="color:#f92672">.</span>doc2bow(text) <span style="color:#66d9ef">for</span> text <span style="color:#f92672">in</span> texts]

tfidf <span style="color:#f92672">=</span> TfidfModel(corpus)
model <span style="color:#f92672">=</span> ldamodel<span style="color:#f92672">.</span>LdaModel(corpus, id2word<span style="color:#f92672">=</span>dictionary, num_topics<span style="color:#f92672">=</span><span style="color:#ae81ff">2</span>)

model<span style="color:#f92672">.</span>show_topics()

doc_water <span style="color:#f92672">=</span> [<span style="color:#e6db74">&#39;river&#39;</span>, <span style="color:#e6db74">&#39;water&#39;</span>, <span style="color:#e6db74">&#39;shore&#39;</span>]
doc_finance <span style="color:#f92672">=</span> [<span style="color:#e6db74">&#39;finance&#39;</span>, <span style="color:#e6db74">&#39;money&#39;</span>, <span style="color:#e6db74">&#39;sell&#39;</span>]
doc_bank <span style="color:#f92672">=</span> [<span style="color:#e6db74">&#39;finance&#39;</span>, <span style="color:#e6db74">&#39;bank&#39;</span>, <span style="color:#e6db74">&#39;tree&#39;</span>, <span style="color:#e6db74">&#39;water&#39;</span>]

bow_water <span style="color:#f92672">=</span> model<span style="color:#f92672">.</span>id2word<span style="color:#f92672">.</span>doc2bow(doc_water)   
bow_finance <span style="color:#f92672">=</span> model<span style="color:#f92672">.</span>id2word<span style="color:#f92672">.</span>doc2bow(doc_finance)   
bow_bank <span style="color:#f92672">=</span> model<span style="color:#f92672">.</span>id2word<span style="color:#f92672">.</span>doc2bow(doc_bank)   

lda_bow_water <span style="color:#f92672">=</span> model[bow_water]
lda_bow_finance <span style="color:#f92672">=</span> model[bow_finance]
lda_bow_bank <span style="color:#f92672">=</span> model[bow_bank]

tfidf_bow_water <span style="color:#f92672">=</span> tfidf[bow_water]
tfidf_bow_finance <span style="color:#f92672">=</span> tfidf[bow_finance]
tfidf_bow_bank <span style="color:#f92672">=</span> tfidf[bow_bank]

<span style="color:#f92672">from</span> gensim.matutils <span style="color:#f92672">import</span> kullback_leibler, jaccard, hellinger

hellinger(lda_bow_water, lda_bow_finance)
hellinger(lda_bow_finance, lda_bow_bank)
hellinger(lda_bow_bank, lda_bow_water)

hellinger(lda_bow_finance, lda_bow_water)
kullback_leibler(lda_bow_water, lda_bow_bank)
kullback_leibler(lda_bow_bank, lda_bow_water)


jaccard(bow_water, bow_bank)
jaccard(doc_water, doc_bank)
jaccard([<span style="color:#e6db74">&#39;word&#39;</span>], [<span style="color:#e6db74">&#39;word&#39;</span>])

<span style="color:#66d9ef">def</span> <span style="color:#a6e22e">make_topics_bow</span>(topic):
    <span style="color:#75715e"># takes the string returned by model.show_topics()</span>
    <span style="color:#75715e"># split on strings to get topics and the probabilities</span>
    topic <span style="color:#f92672">=</span> topic<span style="color:#f92672">.</span>split(<span style="color:#e6db74">&#39;+&#39;</span>)
    <span style="color:#75715e"># list to store topic bows</span>
    topic_bow <span style="color:#f92672">=</span> []
    <span style="color:#66d9ef">for</span> word <span style="color:#f92672">in</span> topic:
        <span style="color:#75715e"># split probability and word</span>
        prob, word <span style="color:#f92672">=</span> word<span style="color:#f92672">.</span>split(<span style="color:#e6db74">&#39;*&#39;</span>)
        <span style="color:#75715e"># get rid of spaces</span>
        word <span style="color:#f92672">=</span> word<span style="color:#f92672">.</span>replace(<span style="color:#e6db74">&#34; &#34;</span>,<span style="color:#e6db74">&#34;&#34;</span>)
        <span style="color:#75715e"># convert to word_type</span>
        word <span style="color:#f92672">=</span> model<span style="color:#f92672">.</span>id2word<span style="color:#f92672">.</span>doc2bow([word])[<span style="color:#ae81ff">0</span>][<span style="color:#ae81ff">0</span>]
        topic_bow<span style="color:#f92672">.</span>append((word, float(prob)))
    <span style="color:#66d9ef">return</span> topic_bow


topic_water, topic_finance <span style="color:#f92672">=</span> model<span style="color:#f92672">.</span>show_topics()
finance_distribution <span style="color:#f92672">=</span> make_topics_bow(topic_finance[<span style="color:#ae81ff">1</span>])
water_distribution <span style="color:#f92672">=</span> make_topics_bow(topic_water[<span style="color:#ae81ff">1</span>])

hellinger(water_distribution, finance_distribution)

<span style="color:#f92672">from</span> gensim <span style="color:#f92672">import</span> similarities

index <span style="color:#f92672">=</span> similarities<span style="color:#f92672">.</span>MatrixSimilarity(model[corpus])
sims <span style="color:#f92672">=</span> index[lda_bow_finance]
<span style="color:#66d9ef">print</span>(list(enumerate(sims)))

sims <span style="color:#f92672">=</span> sorted(enumerate(sims), key<span style="color:#f92672">=</span><span style="color:#66d9ef">lambda</span> item: <span style="color:#f92672">-</span>item[<span style="color:#ae81ff">1</span>])

<span style="color:#66d9ef">for</span> doc_id, similarity <span style="color:#f92672">in</span> sims:
    <span style="color:#66d9ef">print</span> texts[doc_id], similarity

<span style="color:#f92672">from</span> gensim.summarization <span style="color:#f92672">import</span> summarize
<span style="color:#66d9ef">print</span> (summarize(text))

<span style="color:#66d9ef">print</span> (summarize(text, word_count<span style="color:#f92672">=</span><span style="color:#ae81ff">50</span>))

<span style="color:#f92672">from</span> gensim.summarization <span style="color:#f92672">import</span> keywords

<span style="color:#66d9ef">print</span> (keywords(text))

<span style="color:#f92672">from</span> gensim.summarization <span style="color:#f92672">import</span> mz_keywords
mz_keywords(text,scores<span style="color:#f92672">=</span>True,weighted<span style="color:#f92672">=</span>False,threshold<span style="color:#f92672">=</span><span style="color:#ae81ff">1.0</span>)</code></pre></div>
<ul>
<li>Resources

<ol>
<li><a href="https://radimrehurek.com/topic_modeling_tutorial/3%20%20Indexing%20and%20Retrieval.html">Wiki Query</a></li>
<li><a href="https://radimrehurek.com/gensim/simserver.html">Simserver Tutorial</a></li>
<li><a href="https://github.com/RaRe-Technologies/gensim-simserver">GENIM SIMSERVER CODE</a></li>
</ol></li>
</ul>

<h4 id="summarising-text">Summarising Text</h4>

<ul>
<li>GENSIM algo <strong>TextRank</strong> from <a href="http://webeecs.umich.edu/~michalcea/papers/mihalcea.emnlp04.pdf">Mihalcea</a></li>
<li>Improved <a href="https://arxiv.org/pdf/1602.03606.pdf">BM25 Ranking Function</a></li>
<li><a href="https://arxiv.org/abs/0907.1558">Montemurro and Zanettes MZ entropy-based keyword extraction algo</a></li>
</ul>

<h2 id="word2vec-doc2vec-in-gensim">Word2Vec, Doc2Vec in GENSIM</h2>

<ul>
<li><strong>Word Embedding</strong> magic W2V is how it <strong>manages to capture semantic repr of wrods in a vector</strong> based on many papers</li>
<li><strong>V(King) - V(Man) + V(Woman) approx V(Queen) or V(Vietname) + V(Capital) approx V(Hanoi)</strong></li>
<li>Concept
&gt; W2V sliding window size attempting to ID Cond-Proba of observing output word based on adjacent ones EX
&gt;   - Two Methods for W2V training
&gt;       1. Continuous BOW (CBOW)
&gt;       2. Skip Gram - <a href="http://mccormickml.com/2016/04/19/word2vec-tutorial-the-skip-gram-model">Word2Vec Tutorial</a><br />
&gt;   - <a href="https://blog.acolyer.org/2016/04/21/the-amazing-power-of-word-vectors">The amazing power of word vectors</a>
&gt;   - <a href="http://mccormickml.com/2016/04/27/word2vec-resources/">Resources Page</a>
&gt;   while it remians most POPULAR word vectoriser, not first time attempted not last - others to follow</li>
</ul>

<h4 id="w2v-with-gensim">W2V with GENSIM</h4>

<ul>
<li><a href="https://rare-technologies.com/deep-learning-with-word2vec-and-gensim">Code history</a></li>
<li><a href="https://rare-technologies.com/word2vec-tutorial">Online Interactive Tutorial</a></li>
<li>#### Importancy of <code>word2vec</code> class and <code>KeyedVector</code> which tuning relies heavily on
&gt; List of Params for <code>word2vec.Word2Vec</code>
&gt;     1. SG defines algo default=0 CBOW used or =1 Skip-gram
&gt;     2. SIZE dimensionality of feature vectors
&gt;     3. WINDOW max distance entre current-predicted word within a sentence
&gt;     4. ALPHA initial learning rate (linearly drop to <code>min_alpha</code>)
&gt;     5. SEED randNumGenerator, initial vectors for each word seeded with hash of concatenation of word + str(seed), NOTE for fully deterministically reproducible run, must also LIMIT the model to SINGLE WORKER THREAD, eliminating ordering jitter from OS thread scheduling
&gt;     6. MIN_COUNT ignore all words with a total freq lower
&gt;     7. MAX_VOCAB_SIZE lmit RAM during vocab building; if more unique, prune infrequent ones. Every 10m word types need about 1 GB of RAM (None for no limit as default)
&gt;     8. SAMPLE threshold for configuring which higer-freq words randomly downsampled; default 1e-3 useful (0, 1e-5)
&gt;     9. WORKERS use threads to train (faster with multicore)
&gt;     10. HS if 1, hierarchical softmax used else 0 negative is non-zero, negative sampling used
&gt;     11. NEGATIVE if &gt; 0, negative smaple
&gt;     12. CBOW_MEAN if 0, use sum of context word vectors, 1 for mean
&gt;     13. HASHFXN hash func use to randomly init weights, for rised training reproducibility - default is Python\s rudimentary hash func
&gt;     14. ITER num of iterations or epochs over corpus default 5
&gt;     15. TRIM_RULE vocab trimming rule discard if word count &lt; min_cuount (If none, min_count used, or callable accpeting params like word, count and min_count, returns either utils.RULE_DISCARD, UTILS.RULE_KEEP OR UTILS.RULE_DEFAULT) NOTE if given, only used to prune during build_vocab and not stored as part of model
&gt;     16. SORTED_VOCAB if 1 default sort desc before assigning index
&gt;     17. BATCH_WORDS target size in words passed to worker threas default 10k</li>
<li><a href="https://github.com/bhargavvader/personal/blob/master/notebooks/text_analysis/word2vec.ipynb">Notebook</a></li>
<li><a href="https://github.com/RaRe-Technologies/gensim/blob/develop/docs/notebooks/online_w2v_tutorial.ipynb">GENSIM TUtorial</a></li>
<li>Training on generic corpus preferable - <a href="http://mattmahoney.net/dc/text/data.html">Text8 from Wiki</a></li>
<li>GENSIM allows **similar API to download models using other word EMBEDDINGS</li>
<li>Equipped to train, load models, use word embeddings to conduct experiments</li>
</ul>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-python" data-lang="python"><span style="color:#75715e"># be sure to make appropriate imports and installations</span>

<span style="color:#f92672">from</span> gensim.models <span style="color:#f92672">import</span> word2vec

sentences <span style="color:#f92672">=</span> word2vec<span style="color:#f92672">.</span>Text8Corpus(<span style="color:#e6db74">&#39;text8&#39;</span>) 
model <span style="color:#f92672">=</span> word2vec<span style="color:#f92672">.</span>Word2Vec(sentences, size<span style="color:#f92672">=</span><span style="color:#ae81ff">200</span>, hs<span style="color:#f92672">=</span><span style="color:#ae81ff">1</span>)

<span style="color:#66d9ef">print</span>(model)
model<span style="color:#f92672">.</span>wv<span style="color:#f92672">.</span>most_similar(positive<span style="color:#f92672">=</span>[<span style="color:#e6db74">&#39;woman&#39;</span>, <span style="color:#e6db74">&#39;king&#39;</span>], negative<span style="color:#f92672">=</span>[<span style="color:#e6db74">&#39;man&#39;</span>], topn<span style="color:#f92672">=</span><span style="color:#ae81ff">1</span>)[<span style="color:#ae81ff">0</span>]

model<span style="color:#f92672">.</span>wv<span style="color:#f92672">.</span>most_similar_cosmul(positive<span style="color:#f92672">=</span>[<span style="color:#e6db74">&#39;woman&#39;</span>, <span style="color:#e6db74">&#39;king&#39;</span>], negative<span style="color:#f92672">=</span>[<span style="color:#e6db74">&#39;man&#39;</span>])

model<span style="color:#f92672">.</span>wv[<span style="color:#e6db74">&#39;computer&#39;</span>]model<span style="color:#f92672">.</span>save(<span style="color:#e6db74">&#34;text8_model&#34;</span>)

model<span style="color:#f92672">.</span>save(<span style="color:#e6db74">&#34;text8_model&#34;</span>)
model <span style="color:#f92672">=</span> word2vec<span style="color:#f92672">.</span>Word2Vec<span style="color:#f92672">.</span>load(<span style="color:#e6db74">&#34;text8_model&#34;</span>)

model<span style="color:#f92672">.</span>wv<span style="color:#f92672">.</span>doesnt_match(<span style="color:#e6db74">&#34;breakfast cereal dinner lunch&#34;</span><span style="color:#f92672">.</span>split())

model<span style="color:#f92672">.</span>wv<span style="color:#f92672">.</span>similarity(<span style="color:#e6db74">&#39;woman&#39;</span>, <span style="color:#e6db74">&#39;man&#39;</span>)

model<span style="color:#f92672">.</span>wv<span style="color:#f92672">.</span>similarity(<span style="color:#e6db74">&#39;woman&#39;</span>, <span style="color:#e6db74">&#39;cereal&#39;</span>)
 
model<span style="color:#f92672">.</span>wv<span style="color:#f92672">.</span>distance(<span style="color:#e6db74">&#39;man&#39;</span>, <span style="color:#e6db74">&#39;woman&#39;</span>)


word_vectors <span style="color:#f92672">=</span> model<span style="color:#f92672">.</span>wv
<span style="color:#66d9ef">del</span> model

model<span style="color:#f92672">.</span>wv<span style="color:#f92672">.</span>evaluate_word_pairs(os<span style="color:#f92672">.</span>path<span style="color:#f92672">.</span>join(module_path, <span style="color:#e6db74">&#39;test_data&#39;</span>,<span style="color:#e6db74">&#39;wordsim353.tsv&#39;</span>))
model<span style="color:#f92672">.</span>wv<span style="color:#f92672">.</span>accuracy(os<span style="color:#f92672">.</span>path<span style="color:#f92672">.</span>join(module_path, <span style="color:#e6db74">&#39;test_data&#39;</span>, <span style="color:#e6db74">&#39;questions-words.txt&#39;</span>))

<span style="color:#f92672">from</span> gensim.models <span style="color:#f92672">import</span> KeyedVectors
<span style="color:#75715e"># load the google word2vec model</span>
filename <span style="color:#f92672">=</span> <span style="color:#e6db74">&#39;GoogleNews-vectors-negative300.bin&#39;</span>
model <span style="color:#f92672">=</span> KeyedVectors<span style="color:#f92672">.</span>load_word2vec_format(filename, binary<span style="color:#f92672">=</span>True)</code></pre></div>
<h3 id="doc2vec">Doc2Vec</h3>

<ul>
<li>Extending Word2Vec with another vector <strong>paragraph ID</strong>

<ol>
<li><a href="https://cs.stanford.edu/~quocle/paragraph_vector.pdf">Distributed Representations of Senatences and Documents</a></li>
<li><a href="https://medium.com/scaleabout/a-gentle-introduction-to-doc2vec-db3e8c0cce5e">A gentle Introduction to Doc2Vec</a></li>
</ol></li>
<li>One major diff about GENSIM is not expecting a simple corpus as intpu - algo expects TAGS or LABELS as part of input</li>
</ul>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-python" data-lang="python">gensim<span style="color:#f92672">.</span>models<span style="color:#f92672">.</span>doc2vec<span style="color:#f92672">.</span>LabeledSentence
<span style="color:#75715e"># alternatively</span>
gensim<span style="color:#f92672">.</span>models<span style="color:#f92672">.</span>doc2vec<span style="color:#f92672">.</span>TaggedDocument
sentence <span style="color:#f92672">=</span> LabeledSentence(words<span style="color:#f92672">=</span>[<span style="color:#e6db74">u</span><span style="color:#e6db74">&#39;some&#39;</span>, <span style="color:#e6db74">u</span><span style="color:#e6db74">&#39;words&#39;</span>, <span style="color:#e6db74">u</span><span style="color:#e6db74">&#39;here&#39;</span>], labels<span style="color:#f92672">=</span>[<span style="color:#e6db74">u</span><span style="color:#e6db74">&#39;SENT_1&#39;</span>])
<span style="color:#75715e"># in case of error, try</span>
sentence <span style="color:#f92672">=</span> LabeledSentence(words<span style="color:#f92672">=</span>[<span style="color:#e6db74">u</span><span style="color:#e6db74">&#39;some&#39;</span>, <span style="color:#e6db74">u</span><span style="color:#e6db74">&#39;words&#39;</span>, <span style="color:#e6db74">u</span><span style="color:#e6db74">&#39;here&#39;</span>], tags<span style="color:#f92672">=</span>[<span style="color:#e6db74">u</span><span style="color:#e6db74">&#39;SENT_1&#39;</span>])</code></pre></div>
<pre><code>&gt; Here`sentence` an example of what input resembles
</code></pre>

<ul>
<li><a href="https://github.com/RaRe-Technologies/gensim/blob/develop/docs/notebooks/doc2vec-lee.ipynb">Tutorial Notebook based on LEE Corpus</a></li>
</ul>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-python" data-lang="python"><span style="color:#75715e"># LEE corpus</span>

test_data_dir <span style="color:#f92672">=</span> <span style="color:#e6db74">&#39;{}&#39;</span><span style="color:#f92672">.</span>format(os<span style="color:#f92672">.</span>sep)<span style="color:#f92672">.</span>join([gensim<span style="color:#f92672">.</span>__path__[<span style="color:#ae81ff">0</span>], <span style="color:#e6db74">&#39;test&#39;</span>, <span style="color:#e6db74">&#39;test_data&#39;</span>])
lee_train_file <span style="color:#f92672">=</span> test_data_dir <span style="color:#f92672">+</span> os<span style="color:#f92672">.</span>sep <span style="color:#f92672">+</span> <span style="color:#e6db74">&#39;lee_background.cor&#39;</span>
lee_test_file <span style="color:#f92672">=</span> test_data_dir <span style="color:#f92672">+</span> os<span style="color:#f92672">.</span>sep <span style="color:#f92672">+</span> <span style="color:#e6db74">&#39;lee.cor&#39;</span>

<span style="color:#66d9ef">def</span> <span style="color:#a6e22e">read_corpus</span>(file_name, tokens_only<span style="color:#f92672">=</span>False):
    <span style="color:#66d9ef">with</span> smart_open<span style="color:#f92672">.</span>smart_open(file_name) <span style="color:#66d9ef">as</span> f:
        <span style="color:#66d9ef">for</span> i, line <span style="color:#f92672">in</span> enumerate(f):
            <span style="color:#66d9ef">if</span> tokens_only:
                <span style="color:#66d9ef">yield</span> gensim<span style="color:#f92672">.</span>utils<span style="color:#f92672">.</span>simple_preprocess(line)
            <span style="color:#66d9ef">else</span>:
                <span style="color:#75715e"># For training data, add tags</span>
                <span style="color:#66d9ef">yield</span> gensim<span style="color:#f92672">.</span>models<span style="color:#f92672">.</span>doc2vec<span style="color:#f92672">.</span>TaggedDocument(gensim<span style="color:#f92672">.</span>utils<span style="color:#f92672">.</span>simple_preprocess(line), [i])

train_corpus <span style="color:#f92672">=</span> list(read_corpus(lee_train_file))
test_corpus <span style="color:#f92672">=</span> list(read_corpus(lee_test_file, tokens_only<span style="color:#f92672">=</span>True))

model <span style="color:#f92672">=</span> gensim<span style="color:#f92672">.</span>models<span style="color:#f92672">.</span>doc2vec<span style="color:#f92672">.</span>Doc2Vec(vector_size<span style="color:#f92672">=</span><span style="color:#ae81ff">50</span>, min_count<span style="color:#f92672">=</span><span style="color:#ae81ff">2</span>, epochs<span style="color:#f92672">=</span><span style="color:#ae81ff">100</span>)
model<span style="color:#f92672">.</span>build_vocab(train_corpus) 
model<span style="color:#f92672">.</span>train(train_corpus, total_examples<span style="color:#f92672">=</span>model<span style="color:#f92672">.</span>corpus_count, epochs<span style="color:#f92672">=</span>model<span style="color:#f92672">.</span>iter)

models <span style="color:#f92672">=</span> [
    <span style="color:#75715e"># PV-DBOW </span>
    Doc2Vec(dm<span style="color:#f92672">=</span><span style="color:#ae81ff">0</span>, dbow_words<span style="color:#f92672">=</span><span style="color:#ae81ff">1</span>, vector_size<span style="color:#f92672">=</span><span style="color:#ae81ff">200</span>, window<span style="color:#f92672">=</span><span style="color:#ae81ff">8</span>, min_count<span style="color:#f92672">=</span><span style="color:#ae81ff">10</span>, epochs<span style="color:#f92672">=</span><span style="color:#ae81ff">50</span>),
    
    <span style="color:#75715e"># PV-DM w/average</span>
    Doc2Vec(dm<span style="color:#f92672">=</span><span style="color:#ae81ff">1</span>, dm_mean<span style="color:#f92672">=</span><span style="color:#ae81ff">1</span>, vector_size<span style="color:#f92672">=</span><span style="color:#ae81ff">200</span>, window<span style="color:#f92672">=</span><span style="color:#ae81ff">8</span>, min_count<span style="color:#f92672">=</span><span style="color:#ae81ff">10</span>, epochs <span style="color:#f92672">=</span><span style="color:#ae81ff">50</span>),
]

models[<span style="color:#ae81ff">0</span>]<span style="color:#f92672">.</span>build_vocab(documents)
models[<span style="color:#ae81ff">1</span>]<span style="color:#f92672">.</span>reset_from(models[<span style="color:#ae81ff">0</span>])

<span style="color:#66d9ef">for</span> model <span style="color:#f92672">in</span> models:
   model<span style="color:#f92672">.</span>train(documents, total_examples<span style="color:#f92672">=</span>model<span style="color:#f92672">.</span>corpus_count, epochs<span style="color:#f92672">=</span>model<span style="color:#f92672">.</span>epochs)

<span style="color:#f92672">from</span> gensim.test.test_doc2vec <span style="color:#f92672">import</span> ConcatenatedDoc2Vec
new_model <span style="color:#f92672">=</span> ConcatenatedDoc2Vec((models[<span style="color:#ae81ff">0</span>], models[<span style="color:#ae81ff">1</span>]))

inferred_vector <span style="color:#f92672">=</span> model<span style="color:#f92672">.</span>infer_vector(train_corpus[<span style="color:#ae81ff">0</span>]<span style="color:#f92672">.</span>words)
sims <span style="color:#f92672">=</span> model<span style="color:#f92672">.</span>docvecs<span style="color:#f92672">.</span>most_similar([inferred_vector])
<span style="color:#66d9ef">print</span>(sims)</code></pre></div>
<pre><code>&gt; In practice, no need to test for most similar vectors on training set - this is to illustrate
1. Note list of doc most similar to doc 0 ID 0 shows up first - more interesting to check 48th or 255th doc
</code></pre>

<blockquote>
<p><strong>Context captured perfectly by Doc2Vec, simply searched up the most similar doc - imagine the power it brings if used in tandem with clustering and classifying doc ! Instead of TF-IDF or TM as previously presented</strong> !!</p>

<p><strong>Such is Vectorisation with SEMANTIC understanding both words and documents</strong></p>
</blockquote>

<h3 id="other-word-embeddings">Other Word Embeddings</h3>

<ul>
<li><p>GENSIM wraps most of popular methods</p>

<blockquote>
<p>WORDRANK, VAREMBED, FASTTEXT, POINCARE EMBEDDINGS</p>
</blockquote></li>

<li><p>Neat script to use <strong>GloVe embeddings</strong> useful in comparing between diff kinds of embeddings</p></li>

<li><p><code>KeyedVectors</code> class is base to use all word embeddgins</p></li>

<li><p>Key to note is RUN <code>word_vectors = model.wv</code> AFTER done training model</p></li>

<li><p>Also, continue using <code>word_vectors</code> for all tasks - for most similar words, most dissimilar and running tests for word embeddings - <a href="https://github.com/RaRe-Technologies/gensim/blob/develop/gensim/models/keyedvectors.ipynb">source code of KeyedVectors.py</a></p></li>
</ul>

<h4 id="glove">GloVe</h4>

<ul>
<li>Training done on aggregated global word-word co-occurrence stats from a corpus - like Word2Vec, using context to decipher and create word representations</li>
<li>Developed by NLPL Stanford and <a href="https://nlp.stanford.edu/pubs/glove.pdf">paper</a> worth reading as it illustrates some of the pitfalls of LSA and Word2Vec</li>
<li>Many implementations and even in Python system - not training here but using (training need to tweek <a href="https://github.com/maciejkula/glove-python">glove_python</a> or just <a href="https://github.com/JonathanRaiman/glove">glove</a> or look at <a href="https://github.com/stanfordnlp/GloVe">source</a></li>
<li>GENSIM

<ol>
<li>download or train GloVe vectors - save - convert format to Word2Vec for futher usg in GENSIM API</li>
<li>Download <a href="https://nlp.stanford.edu/projects/glove">page</a></li>
</ol></li>
</ul>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-python" data-lang="python"><span style="color:#f92672">from</span> gensim.scripts.glove2word2vec <span style="color:#f92672">import</span> glove2word2vec
glove_input_file <span style="color:#f92672">=</span> <span style="color:#e6db74">&#39;glove.6B.100d.txt&#39;</span>
word2vec_output_file <span style="color:#f92672">=</span> <span style="color:#e6db74">&#39;glove.6B.100d.txt.word2vec&#39;</span>
glove2word2vec(glove_input_file, word2vec_output_file)

<span style="color:#f92672">from</span> gensim.models <span style="color:#f92672">import</span> KeyedVectors
filename <span style="color:#f92672">=</span> <span style="color:#e6db74">&#39;glove.6B.100d.txt.word2vec&#39;</span>
model <span style="color:#f92672">=</span> KeyedVectors<span style="color:#f92672">.</span>load_word2vec_format(filename, binary<span style="color:#f92672">=</span>False)

model<span style="color:#f92672">.</span>most_similar(positive<span style="color:#f92672">=</span>[<span style="color:#e6db74">&#39;woman&#39;</span>, <span style="color:#e6db74">&#39;king&#39;</span>], negative<span style="color:#f92672">=</span>[<span style="color:#e6db74">&#39;man&#39;</span>], topn<span style="color:#f92672">=</span><span style="color:#ae81ff">1</span>)</code></pre></div>
<h4 id="fasttext">FastText</h4>

<ul>
<li>Dev by Facebook AI, fast and efficient due to morphological details learning</li>
<li>Unique in deriving word vectors for unknown owrds from morphological char of words, creating word vector for unseen</li>
<li>Intriguing in some langugage, English e.g. &lsquo;ly&rsquo; <code>embedding(strang) - embedding(strangely) ~= embedding(charming) - embedding(charmingly)</code></li>
<li>Performs better for structure or syntax, while Word2Vec for semantic tasks</li>
<li><a href="https://github.com/RaRe-Technologies/gensim/blob/develop/docs/notebooks/Word2Vc_FastText_Comparison.ipynb">Notebook</a> and <a href="https://radimrehurek.com/gensim/models/fastext.html#module-gensim.models.fasttext">Doc</a></li>
<li>Possible to use C++ via <a href="https://radimrehurek.com/gensim/models/wrappers/fasttext.html">wrapper</a></li>
<li><a href="https://github.com/RaRe-Technologies/gensim/blob/develop/docs/notebooks/FastText_Tutorial.ipynb">Notebook1</a></li>
</ul>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-python" data-lang="python"><span style="color:#f92672">from</span> gensim.models.wrappers.fasttext <span style="color:#f92672">import</span> FastText

<span style="color:#75715e"># Set FastText home to the path to the FastText executable</span>
ft_home <span style="color:#f92672">=</span> <span style="color:#e6db74">&#39;/home/bhargav/Gensim/fastText/fasttext&#39;</span>
<span style="color:#75715e"># train the model</span>
model_wrapper <span style="color:#f92672">=</span> FastText<span style="color:#f92672">.</span>train(ft_home, train_file)

<span style="color:#66d9ef">print</span>(<span style="color:#e6db74">&#39;dog&#39;</span> <span style="color:#f92672">in</span> model<span style="color:#f92672">.</span>wv<span style="color:#f92672">.</span>vocab)
<span style="color:#66d9ef">print</span>(<span style="color:#e6db74">&#39;dogs&#39;</span> <span style="color:#f92672">in</span> model<span style="color:#f92672">.</span>wv<span style="color:#f92672">.</span>vocab)

<span style="color:#66d9ef">print</span>(<span style="color:#e6db74">&#39;dog&#39;</span> <span style="color:#f92672">in</span> model)
<span style="color:#66d9ef">print</span>(<span style="color:#e6db74">&#39;dogs&#39;</span> <span style="color:#f92672">in</span> model)</code></pre></div>
<h4 id="wordrank">WordRank</h4>

<ul>
<li>Embedding as Ranking - similar to GloVe in using global co-occurences of words to generate</li>
<li><a href="https://bitbucket.org/shihaoji/wordrank">code</a> and <a href="https://github.com/shihaoji/wordrank">github</a></li>
<li>GENSIM API - beware of <code>dump_period</code> and <code>iter</code> param needed to be sync as it dumps file with start of next iteration <a href="https://github.com/RaRe-Technologies/gensim/blob/develop/docs/notebooks/WordRank_wrapper_quick-start.ipynb">Tutorial</a></li>
<li><strong>CAVEAT</strong> window size of 15 performed with optimum results, and 100 epochs is better than 500, quite long.</li>
<li>GOOD COMPARISON betwee FastText, word2vec and WorkRank <a href="https://rare-technologies.com/wordrank-embedding-crowned-is-most-similar-to-king-not-word2vecs-canute">blog</a> and <a href="https://github.com/RaRe-Technologies/gensim/blob/develop/docs/notebooks/Wordrank_comparisons.ipynb">Notebook</a></li>
</ul>

<h4 id="varembed">Varembed</h4>

<ul>
<li>Like FastText it takes morphological info to generate word vectors</li>
<li>Similar to GloVe, cannot update model with new words and need to train a new model <a href="https://github.com/rguthrie3/MorphologicalPriorsForWrodEmeddings">code</a></li>
</ul>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-python" data-lang="python"><span style="color:#f92672">from</span> gensim.models.wrappers <span style="color:#f92672">import</span> varembed
varembed_vectors <span style="color:#f92672">=</span> <span style="color:#e6db74">&#39;../../gensim/test/test_data/varembed_leecorpus_vectors.pkl&#39;</span>
model <span style="color:#f92672">=</span> varembed<span style="color:#f92672">.</span>VarEmbed<span style="color:#f92672">.</span>load_varembed_format(vectors<span style="color:#f92672">=</span>varembed_vectors)


morfessors <span style="color:#f92672">=</span> <span style="color:#e6db74">&#39;../../gensim/test/test_data/varembed_leecorpus_morfessor.bin&#39;</span>
model <span style="color:#f92672">=</span> varembed<span style="color:#f92672">.</span>VarEmbed<span style="color:#f92672">.</span>load_varembed_format(vectors<span style="color:#f92672">=</span>varembed_vectors, morfessor_model<span style="color:#f92672">=</span>morfessors)</code></pre></div>
<h4 id="poincare">Poincare</h4>

<ul>
<li>Also dev by FB - using graphical repr of words to decipher relationship between words to generate vector</li>
<li>Also captures hiearchical info computed by hyperbolic space not traditioanl Norm2 allowing for hierarchy info</li>
<li><a href="https://arxiv.org/pdf/1705.08039.pdf">Poincaré Embeddings for Learning Hiearchical Represenstaions</a></li>
</ul>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-python" data-lang="python"><span style="color:#f92672">import</span> os

poincare_directory <span style="color:#f92672">=</span> os<span style="color:#f92672">.</span>path<span style="color:#f92672">.</span>join(os<span style="color:#f92672">.</span>getcwd(), <span style="color:#e6db74">&#39;docs&#39;</span>, <span style="color:#e6db74">&#39;notebooks&#39;</span>, <span style="color:#e6db74">&#39;poincare&#39;</span>)
data_directory <span style="color:#f92672">=</span> os<span style="color:#f92672">.</span>path<span style="color:#f92672">.</span>join(poincare_directory, <span style="color:#e6db74">&#39;data&#39;</span>)
wordnet_mammal_file <span style="color:#f92672">=</span> os<span style="color:#f92672">.</span>path<span style="color:#f92672">.</span>join(data_directory, <span style="color:#e6db74">&#39;wordnet_mammal_hypernyms.tsv&#39;</span>)

<span style="color:#75715e"># Training process</span>
<span style="color:#f92672">from</span> gensim.models.poincare <span style="color:#f92672">import</span> PoincareModel, PoincareKeyedVectors, PoincareRelations
relations <span style="color:#f92672">=</span> PoincareRelations(file_path<span style="color:#f92672">=</span>wordnet_mammal_file, delimiter<span style="color:#f92672">=</span><span style="color:#e6db74">&#39;</span><span style="color:#ae81ff">\t</span><span style="color:#e6db74">&#39;</span>)
model <span style="color:#f92672">=</span> PoincareModel(train_data<span style="color:#f92672">=</span>relations, size<span style="color:#f92672">=</span><span style="color:#ae81ff">2</span>, burn_in<span style="color:#f92672">=</span><span style="color:#ae81ff">0</span>)
model<span style="color:#f92672">.</span>train(epochs<span style="color:#f92672">=</span><span style="color:#ae81ff">1</span>, print_every<span style="color:#f92672">=</span><span style="color:#ae81ff">500</span>)

<span style="color:#75715e"># Also use own iterable of relations to train model</span>
<span style="color:#75715e"># each relation is just a pair of nodes</span>
<span style="color:#75715e"># GENSIM also has pre-trained models as follows</span>

models_directory <span style="color:#f92672">=</span> os<span style="color:#f92672">.</span>path<span style="color:#f92672">.</span>join(poincare_directory, <span style="color:#e6db74">&#39;models&#39;</span>)
test_model_path <span style="color:#f92672">=</span> os<span style="color:#f92672">.</span>path<span style="color:#f92672">.</span>join(models_directory, <span style="color:#e6db74">&#39;gensim_model_batch_size_10_burn_in_0_epochs_50_neg_20_dim_50&#39;</span>)
model <span style="color:#f92672">=</span> PoincareModel<span style="color:#f92672">.</span>load(test_model_path)</code></pre></div>
<ul>
<li><a href="https://github.com/RaRe-Technologies/gensim/blob/develop/docs/notebooks/Poincare%20Tutorial.ipynb">Training</a> and <a href="https://rare-technologies.com/implementing-poincare-embeddings">Blog</a></li>
</ul>

<h2 id="deep-learning-for-text">Deep Learning for Text</h2>

<h3 id="generating-text">Generating Text</h3>

<ul>
<li><a href="http://colah.github.io/posts/2015-08-Understanding-LSTMs">Understanding LSTM Networks</a></li>
<li><a href="http://karpathy.github.io/2015/05/21/rnn-effectiveness">Unreasonable effectiveness of NN</a></li>
<li><a href="https://github.com/kirit93/Personal/blob/master/text_generation_keras/text_generation.ipynb">Notebook</a></li>
</ul>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-python" data-lang="python"><span style="color:#f92672">import</span> keras
<span style="color:#f92672">from</span> keras.models <span style="color:#f92672">import</span> Sequential
<span style="color:#f92672">from</span> keras.layers <span style="color:#f92672">import</span> LSTM, Dense, Dropout
<span style="color:#f92672">from</span> keras.callbacks <span style="color:#f92672">import</span> ModelCheckpoint
<span style="color:#f92672">from</span> keras.utils <span style="color:#f92672">import</span> np_utils
<span style="color:#f92672">import</span> numpy <span style="color:#f92672">as</span> np

<span style="color:#75715e"># Any text source as input based on what kind of data to generate</span>
<span style="color:#75715e"># CREATEIVE! HERE - RNN to write poetry if enough data</span>
<span style="color:#75715e"># Need to generate MAPPING of all distinct characters inf book (LSTM is char-level model)</span>

filename    <span style="color:#f92672">=</span> <span style="color:#e6db74">&#39;data/source_data.txt&#39;</span>
data        <span style="color:#f92672">=</span> open(filename)<span style="color:#f92672">.</span>read()
data        <span style="color:#f92672">=</span> data<span style="color:#f92672">.</span>lower()
<span style="color:#75715e"># Find all the unique characters</span>
chars       <span style="color:#f92672">=</span> sorted(list(set(data)))
char_to_int <span style="color:#f92672">=</span> dict((c, i) <span style="color:#66d9ef">for</span> i, c <span style="color:#f92672">in</span> enumerate(chars))
ix_to_char  <span style="color:#f92672">=</span> dict((i, c) <span style="color:#66d9ef">for</span> i, c <span style="color:#f92672">in</span> enumerate(chars))
vocab_size  <span style="color:#f92672">=</span> len(chars)

<span style="color:#75715e"># The 2 dicts pass char to model and in generating</span>
<span style="color:#75715e"># RNN accepts seq of char as input and ouput such similar seq - to break up into seq</span>
seq_length <span style="color:#f92672">=</span> <span style="color:#ae81ff">100</span>
list_X <span style="color:#f92672">=</span> [ ]
list_Y <span style="color:#f92672">=</span> [ ]
<span style="color:#66d9ef">for</span> i <span style="color:#f92672">in</span> range(<span style="color:#ae81ff">0</span>, len(chars) <span style="color:#f92672">-</span> seq_length, <span style="color:#ae81ff">1</span>):
	seq_in <span style="color:#f92672">=</span> raw_text[i:i <span style="color:#f92672">+</span> seq_length]
	seq_out <span style="color:#f92672">=</span> raw_text[i <span style="color:#f92672">+</span> seq_length]
	list_X<span style="color:#f92672">.</span>append([char_to_int[char] <span style="color:#66d9ef">for</span> char <span style="color:#f92672">in</span> seq_in])
	list_Y<span style="color:#f92672">.</span>append(char_to_int[seq_out])
n_patterns <span style="color:#f92672">=</span> len(dataX)

X  <span style="color:#f92672">=</span> np<span style="color:#f92672">.</span>reshape(list_X, (n_patterns, seq_length, <span style="color:#ae81ff">1</span>)) 
<span style="color:#75715e"># Encode output as one-hot vector</span>
Y  <span style="color:#f92672">=</span> np_utils<span style="color:#f92672">.</span>to_categorical(list_Y)

model <span style="color:#f92672">=</span> Sequential()
model<span style="color:#f92672">.</span>add(LSTM(<span style="color:#ae81ff">256</span>, input_shape<span style="color:#f92672">=</span>(X<span style="color:#f92672">.</span>shape[<span style="color:#ae81ff">1</span>], X<span style="color:#f92672">.</span>shape[<span style="color:#ae81ff">2</span>])))
model<span style="color:#f92672">.</span>add(Dropout(<span style="color:#ae81ff">0.2</span>))
model<span style="color:#f92672">.</span>add(Dense(y<span style="color:#f92672">.</span>shape[<span style="color:#ae81ff">1</span>], activation<span style="color:#f92672">=</span><span style="color:#e6db74">&#39;softmax&#39;</span>))
model<span style="color:#f92672">.</span>compile(loss<span style="color:#f92672">=</span><span style="color:#e6db74">&#39;categorical_crossentropy&#39;</span>, optimizer<span style="color:#f92672">=</span><span style="color:#e6db74">&#39;adam&#39;</span>)

<span style="color:#75715e"># Dropout to control overfitting </span>

filepath<span style="color:#f92672">=</span><span style="color:#e6db74">&#34;weights-improvement-{epoch:02d}-{loss:.4f}.hdf5&#34;</span>
checkpoint <span style="color:#f92672">=</span> ModelCheckpoint(filepath, monitor<span style="color:#f92672">=</span><span style="color:#e6db74">&#39;loss&#39;</span>, verbose<span style="color:#f92672">=</span><span style="color:#ae81ff">1</span>, save_best_only<span style="color:#f92672">=</span>True, mode<span style="color:#f92672">=</span><span style="color:#e6db74">&#39;min&#39;</span>)
callbacks_list <span style="color:#f92672">=</span> [checkpoint]
<span style="color:#75715e"># fit the model</span>
model<span style="color:#f92672">.</span>fit(X, y, epochs<span style="color:#f92672">=</span><span style="color:#ae81ff">20</span>, batch_size<span style="color:#f92672">=</span><span style="color:#ae81ff">128</span>, callbacks<span style="color:#f92672">=</span>callbacks_list)

<span style="color:#75715e"># callback save weights to fiile at whenever improvement</span>

<span style="color:#75715e"># OR transfer learning</span>
filename <span style="color:#f92672">=</span> <span style="color:#e6db74">&#34;weights.hdf5&#34;</span>
model<span style="color:#f92672">.</span>load_weights(filename)
model<span style="color:#f92672">.</span>compile(loss<span style="color:#f92672">=</span><span style="color:#e6db74">&#39;categorical_crossentropy&#39;</span>, optimizer<span style="color:#f92672">=</span><span style="color:#e6db74">&#39;adam&#39;</span>)

<span style="color:#75715e"># generate text start text randomly</span>
start   <span style="color:#f92672">=</span> np<span style="color:#f92672">.</span>random<span style="color:#f92672">.</span>randint(<span style="color:#ae81ff">0</span>, len(X) <span style="color:#f92672">-</span> <span style="color:#ae81ff">1</span>)
pattern <span style="color:#f92672">=</span> np<span style="color:#f92672">.</span>ravel(X[start])<span style="color:#f92672">.</span>tolist()

output <span style="color:#f92672">=</span> []
<span style="color:#66d9ef">for</span> i <span style="color:#f92672">in</span> range(<span style="color:#ae81ff">250</span>):
    x           <span style="color:#f92672">=</span> np<span style="color:#f92672">.</span>reshape(pattern, (<span style="color:#ae81ff">1</span>, len(pattern), <span style="color:#ae81ff">1</span>))
    x           <span style="color:#f92672">=</span> x <span style="color:#f92672">/</span> float(vocab_size)
    prediction  <span style="color:#f92672">=</span> model<span style="color:#f92672">.</span>predict(x, verbose <span style="color:#f92672">=</span> <span style="color:#ae81ff">0</span>)
    index       <span style="color:#f92672">=</span> np<span style="color:#f92672">.</span>argmax(prediction)
    result      <span style="color:#f92672">=</span> index
    output<span style="color:#f92672">.</span>append(result)
    pattern<span style="color:#f92672">.</span>append(index)
    pattern <span style="color:#f92672">=</span> pattern[<span style="color:#ae81ff">1</span> : len(pattern)]

<span style="color:#66d9ef">print</span> (<span style="color:#e6db74">&#34;</span><span style="color:#ae81ff">\&#34;</span><span style="color:#e6db74">&#34;</span>, <span style="color:#e6db74">&#39;&#39;</span><span style="color:#f92672">.</span>join([ix_to_char[value] <span style="color:#66d9ef">for</span> value <span style="color:#f92672">in</span> output]), <span style="color:#e6db74">&#34;</span><span style="color:#ae81ff">\&#34;</span><span style="color:#e6db74">&#34;</span>)

<span style="color:#75715e"># IDEA: based on X input, choose highest porba for next char using argmax, convert that index to a char, append it to output list, loops = iterations in output</span></code></pre></div>
<ul>
<li>Resources

<ol>
<li><a href="http://ruder.io/deep-learning-nlp-best-practices/index.html#bestpractices">NLP Best Practice</a></li>
<li><a href="http://colah.github.io/posts/2014-07-NLP-RNNs-Representations">Deep Learning and Representation</a></li>
<li><a href="https://tryolabs.com/blog/2017/12/12/deep-learning-for-nlp-advancements-and-trends-in-2017">Best of 2017 for NLP and DL</a></li>
</ol></li>
</ul>

<h2 id="keras-and-spacy-for-dl">Keras and SpaCy for DL</h2>

<ul>
<li><a href="https://keras.io/getting-started/sequential-model-guide">Keras Sequential Model</a></li>
<li><a href="https://github.com/keras-team/keras/blob/master/examples/imdb_cnn_lstm.py">Keras CNN LSTM</a></li>
<li><a href="https://blog.keras.io/using-pre-trained-word-embeddings-in-a-keras-model.html">Pre-trained word embeddings</a></li>
</ul>

<h4 id="keras-and-spacy">Keras and SpaCy</h4>

<ul>
<li><a href="https://keras.io/preprocessing/text">Keras Submodule Text Preprocess</a></li>
<li><strong>KERAS KEN</strong>

<ol>
<li><a href="https://keras.io/models/about-keras-models">About Keras Models: explains various kinds of NN in Keras</a></li>
<li><a href="https://keras.io/layers/about-keras-layers">About Keras Layers</a></li>
<li><a href="https://keras.io/layers/core">Core Layers</a></li>
<li><a href="https://keras.io/datasets">Keras Datasets</a></li>
<li><a href="https://keras.io/layers/recurrent/#lstm">LSTM</a></li>
<li><a href="https://keras.io/layers/convolutional">CNN</a>
&gt; SpaCy <code>TextCategorizer</code> trains similar to other components as POS and NER, also integrating wiht other word embeddings such as GENSIM Word2Vec or GloVe, plus plug-in to Keras model; <strong>Combine SpaCy and Keras allows powerful classification machien</strong></li>
</ol></li>
</ul>

<h4 id="classification-with-keras">Classification with Keras</h4>

<ul>
<li><strong>Small dataset such as IMDB might get better result using simply BOW + SVM</strong></li>
</ul>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-python" data-lang="python"><span style="color:#f92672">from</span> keras.preprocessing <span style="color:#f92672">import</span> sequence
<span style="color:#f92672">from</span> keras.models <span style="color:#f92672">import</span> Sequential
<span style="color:#f92672">from</span> keras.layers <span style="color:#f92672">import</span> Dense, Embedding
<span style="color:#f92672">from</span> keras.layers <span style="color:#f92672">import</span> LSTM
<span style="color:#f92672">from</span> keras.datasets <span style="color:#f92672">import</span> imdb</code></pre></div>
<blockquote>
<p>Notes
    1. Not using text preprocess moduels as IMDB dataset already cleaned
    2. LSTM for classification, a variant of RNN
    3. LSTM is mere Layer inside <code>Sequential</code> model</p>
</blockquote>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-python" data-lang="python">max_features <span style="color:#f92672">=</span> <span style="color:#ae81ff">20000</span>
maxlen <span style="color:#f92672">=</span> <span style="color:#ae81ff">80</span>  <span style="color:#75715e"># cut texts after this number of words (among top max_features most common words)</span>
batch_size <span style="color:#f92672">=</span> <span style="color:#ae81ff">32</span>

<span style="color:#66d9ef">print</span>(<span style="color:#e6db74">&#39;Loading data...&#39;</span>)
(x_train, y_train), (x_test, y_test) <span style="color:#f92672">=</span> imdb<span style="color:#f92672">.</span>load_data(num_words<span style="color:#f92672">=</span>max_features)
<span style="color:#66d9ef">print</span>(len(x_train), <span style="color:#e6db74">&#39;train sequences&#39;</span>)
<span style="color:#66d9ef">print</span>(len(x_test), <span style="color:#e6db74">&#39;test sequences&#39;</span>)

<span style="color:#66d9ef">print</span>(<span style="color:#e6db74">&#39;Pad sequences (samples x time)&#39;</span>)
x_train <span style="color:#f92672">=</span> sequence<span style="color:#f92672">.</span>pad_sequences(x_train, maxlen<span style="color:#f92672">=</span>maxlen)
x_test <span style="color:#f92672">=</span> sequence<span style="color:#f92672">.</span>pad_sequences(x_test, maxlen<span style="color:#f92672">=</span>maxlen)
<span style="color:#66d9ef">print</span>(<span style="color:#e6db74">&#39;x_train shape:&#39;</span>, x_train<span style="color:#f92672">.</span>shape)
<span style="color:#66d9ef">print</span>(<span style="color:#e6db74">&#39;x_test shape:&#39;</span>, x_test<span style="color:#f92672">.</span>shape)</code></pre></div>
<blockquote>
<p><code>max_features</code> refers to top words wishe to use limited to 20k words; similar to ridding of least used words; <code>maxlen</code> used for fix length as NN accpets a FIED LEN input; <code>batch_size</code> used later to specify batches trained</p>
</blockquote>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-python" data-lang="python"><span style="color:#66d9ef">print</span>(<span style="color:#e6db74">&#39;Build model...&#39;</span>)
model <span style="color:#f92672">=</span> Sequential()
model<span style="color:#f92672">.</span>add(Embedding(max_features, <span style="color:#ae81ff">128</span>))
model<span style="color:#f92672">.</span>add(LSTM(<span style="color:#ae81ff">128</span>, dropout<span style="color:#f92672">=</span><span style="color:#ae81ff">0.2</span>, recurrent_dropout<span style="color:#f92672">=</span><span style="color:#ae81ff">0.2</span>))
model<span style="color:#f92672">.</span>add(Dense(<span style="color:#ae81ff">1</span>, activation<span style="color:#f92672">=</span><span style="color:#e6db74">&#39;sigmoid&#39;</span>))</code></pre></div>
<blockquote>
<p>Setup Seq-model, stacked on layers of word-embeddings (20k features), dropped down to 128 <strong>Option to use other embedders</strong> - LSTM 128 number of Dimensions</p>
</blockquote>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-python" data-lang="python"><span style="color:#75715e"># try using different optimizers and different optimizer configs</span>
model<span style="color:#f92672">.</span>compile(loss<span style="color:#f92672">=</span><span style="color:#e6db74">&#39;binary_crossentropy&#39;</span>,
              optimizer<span style="color:#f92672">=</span><span style="color:#e6db74">&#39;adam&#39;</span>,
              metrics<span style="color:#f92672">=</span>[<span style="color:#e6db74">&#39;accuracy&#39;</span>])

<span style="color:#66d9ef">print</span>(<span style="color:#e6db74">&#39;Train...&#39;</span>)
model<span style="color:#f92672">.</span>fit(x_train, y_train,
          batch_size<span style="color:#f92672">=</span>batch_size,
          epochs<span style="color:#f92672">=</span><span style="color:#ae81ff">15</span>,
          validation_data<span style="color:#f92672">=</span>(x_test, y_test))


score, acc <span style="color:#f92672">=</span> model<span style="color:#f92672">.</span>evaluate(x_test, y_test,
                            batch_size<span style="color:#f92672">=</span>batch_size)
<span style="color:#66d9ef">print</span>(<span style="color:#e6db74">&#39;Test score:&#39;</span>, score)
<span style="color:#66d9ef">print</span>(<span style="color:#e6db74">&#39;Test accuracy:&#39;</span>, acc)</code></pre></div>
<ul>
<li>CNN need a few more params to tune</li>
</ul>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-python" data-lang="python"><span style="color:#f92672">from</span> keras.preprocessing <span style="color:#f92672">import</span> sequence
<span style="color:#f92672">from</span> keras.models <span style="color:#f92672">import</span> Sequential
<span style="color:#f92672">from</span> keras.layers <span style="color:#f92672">import</span> Dense, Dropout, Activation
<span style="color:#f92672">from</span> keras.layers <span style="color:#f92672">import</span> Embedding
<span style="color:#f92672">from</span> keras.layers <span style="color:#f92672">import</span> LSTM
<span style="color:#f92672">from</span> keras.layers <span style="color:#f92672">import</span> Conv1D, MaxPooling1D
<span style="color:#f92672">from</span> keras.datasets <span style="color:#f92672">import</span> imdb

<span style="color:#75715e"># Convolution</span>
kernel_size <span style="color:#f92672">=</span> <span style="color:#ae81ff">5</span>
filters <span style="color:#f92672">=</span> <span style="color:#ae81ff">64</span>
pool_size <span style="color:#f92672">=</span> <span style="color:#ae81ff">4</span>

<span style="color:#75715e"># Embedding</span>
max_features <span style="color:#f92672">=</span> <span style="color:#ae81ff">20000</span>
maxlen <span style="color:#f92672">=</span> <span style="color:#ae81ff">100</span>
embedding_size <span style="color:#f92672">=</span> <span style="color:#ae81ff">128</span>

<span style="color:#75715e"># LSTM</span>
lstm_output_size <span style="color:#f92672">=</span> <span style="color:#ae81ff">70</span>

<span style="color:#75715e"># Training</span>
batch_size <span style="color:#f92672">=</span> <span style="color:#ae81ff">30</span>
epochs <span style="color:#f92672">=</span> <span style="color:#ae81ff">2</span></code></pre></div>
<blockquote>
<p>Above params affects training heavily and are empirically derived after experiemtns</p>
</blockquote>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-python" data-lang="python"><span style="color:#66d9ef">print</span>(<span style="color:#e6db74">&#39;Build model...&#39;</span>)

model <span style="color:#f92672">=</span> Sequential()
model<span style="color:#f92672">.</span>add(Embedding(max_features, embedding_size, input_length<span style="color:#f92672">=</span>maxlen))
model<span style="color:#f92672">.</span>add(Dropout(<span style="color:#ae81ff">0.25</span>))
model<span style="color:#f92672">.</span>add(Conv1D(filters,
                 kernel_size,
                 padding<span style="color:#f92672">=</span><span style="color:#e6db74">&#39;valid&#39;</span>,
                 activation<span style="color:#f92672">=</span><span style="color:#e6db74">&#39;relu&#39;</span>,
                 strides<span style="color:#f92672">=</span><span style="color:#ae81ff">1</span>))
model<span style="color:#f92672">.</span>add(MaxPooling1D(pool_size<span style="color:#f92672">=</span>pool_size))
model<span style="color:#f92672">.</span>add(LSTM(lstm_output_size))
model<span style="color:#f92672">.</span>add(Dense(<span style="color:#ae81ff">1</span>))
model<span style="color:#f92672">.</span>add(Activation(<span style="color:#e6db74">&#39;sigmoid&#39;</span>))</code></pre></div>
<blockquote>
<p>7 payers, Pooling layer to progressively reduce spatial size to reduce params hence controlling overfitting;</p>
</blockquote>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-python" data-lang="python">model<span style="color:#f92672">.</span>compile(loss<span style="color:#f92672">=</span><span style="color:#e6db74">&#39;binary_crossentropy&#39;</span>,
              optimizer<span style="color:#f92672">=</span><span style="color:#e6db74">&#39;adam&#39;</span>,
              metrics<span style="color:#f92672">=</span>[<span style="color:#e6db74">&#39;accuracy&#39;</span>])

<span style="color:#66d9ef">print</span>(<span style="color:#e6db74">&#39;Train...&#39;</span>)
model<span style="color:#f92672">.</span>fit(x_train, y_train,
          batch_size<span style="color:#f92672">=</span>batch_size,
          epochs<span style="color:#f92672">=</span>epochs,
          validation_data<span style="color:#f92672">=</span>(x_test, y_test))
score, acc <span style="color:#f92672">=</span> model<span style="color:#f92672">.</span>evaluate(x_test, y_test, batch_size<span style="color:#f92672">=</span>batch_size)
<span style="color:#66d9ef">print</span>(<span style="color:#e6db74">&#39;Test score:&#39;</span>, score)
<span style="color:#66d9ef">print</span>(<span style="color:#e6db74">&#39;Test accuracy:&#39;</span>, acc)</code></pre></div>
<blockquote>
<p>Below use pretrained word embeddings in classifier to improve results</p>
</blockquote>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-python" data-lang="python">BASE_DIR <span style="color:#f92672">=</span> <span style="color:#e6db74">&#39;&#39;</span>
GLOVE_DIR <span style="color:#f92672">=</span> os<span style="color:#f92672">.</span>path<span style="color:#f92672">.</span>join(BASE_DIR, <span style="color:#e6db74">&#39;glove.6B&#39;</span>)
MAX_SEQUENCE_LENGTH <span style="color:#f92672">=</span> <span style="color:#ae81ff">1000</span>
MAX_NUM_WORDS <span style="color:#f92672">=</span> <span style="color:#ae81ff">20000</span>
EMBEDDING_DIM <span style="color:#f92672">=</span> <span style="color:#ae81ff">100</span>

<span style="color:#75715e"># using preceding var/arg to load word embeddgins</span>
<span style="color:#66d9ef">print</span>(<span style="color:#e6db74">&#39;Indexing word vectors.&#39;</span>)

embeddings_index <span style="color:#f92672">=</span> {}
<span style="color:#66d9ef">with</span> open(os<span style="color:#f92672">.</span>path<span style="color:#f92672">.</span>join(GLOVE_DIR, <span style="color:#e6db74">&#39;glove.6B.100d.txt&#39;</span>)) <span style="color:#66d9ef">as</span> f:
    <span style="color:#66d9ef">for</span> line <span style="color:#f92672">in</span> f:
        values <span style="color:#f92672">=</span> line<span style="color:#f92672">.</span>split()
        word <span style="color:#f92672">=</span> values[<span style="color:#ae81ff">0</span>]
        coefs <span style="color:#f92672">=</span> np<span style="color:#f92672">.</span>asarray(values[<span style="color:#ae81ff">1</span>:], dtype<span style="color:#f92672">=</span><span style="color:#e6db74">&#39;float32&#39;</span>)
        embeddings_index[word] <span style="color:#f92672">=</span> coefs

<span style="color:#66d9ef">print</span>(<span style="color:#e6db74">&#39;Found </span><span style="color:#e6db74">%s</span><span style="color:#e6db74"> word vectors.&#39;</span> <span style="color:#f92672">%</span> len(embeddings_index))

<span style="color:#75715e"># Simple loop through files </span>
<span style="color:#66d9ef">print</span>(<span style="color:#e6db74">&#39;Preparing embedding matrix.&#39;</span>)

<span style="color:#75715e"># prepare embedding matrix</span>
num_words <span style="color:#f92672">=</span> min(MAX_NUM_WORDS, len(word_index) <span style="color:#f92672">+</span> <span style="color:#ae81ff">1</span>)
embedding_matrix <span style="color:#f92672">=</span> np<span style="color:#f92672">.</span>zeros((num_words, EMBEDDING_DIM))
<span style="color:#66d9ef">for</span> word, i <span style="color:#f92672">in</span> word_index<span style="color:#f92672">.</span>items():
    <span style="color:#66d9ef">if</span> i <span style="color:#f92672">&gt;=</span> MAX_NUM_WORDS:
        <span style="color:#66d9ef">continue</span>
    embedding_vector <span style="color:#f92672">=</span> embeddings_index<span style="color:#f92672">.</span>get(word)
    <span style="color:#66d9ef">if</span> embedding_vector <span style="color:#f92672">is</span> <span style="color:#f92672">not</span> None:
        <span style="color:#75715e"># words not found in embedding index will be all-zeros.</span>
        embedding_matrix[i] <span style="color:#f92672">=</span> embedding_vector

<span style="color:#75715e"># Make sure set training argument to false so to use word vectors as is</span>
embedding_layer <span style="color:#f92672">=</span> Embedding(num_words,
                            EMBEDDING_DIM,
                            weights<span style="color:#f92672">=</span>[embedding_matrix],
                            input_length<span style="color:#f92672">=</span>MAX_SEQUENCE_LENGTH,
                            trainable<span style="color:#f92672">=</span>False)

<span style="color:#66d9ef">print</span>(<span style="color:#e6db74">&#39;Training model.&#39;</span>)

<span style="color:#75715e"># train a 1D convnet with global maxpooling</span>
sequence_input <span style="color:#f92672">=</span> Input(shape<span style="color:#f92672">=</span>(MAX_SEQUENCE_LENGTH,), dtype<span style="color:#f92672">=</span><span style="color:#e6db74">&#39;int32&#39;</span>)
embedded_sequences <span style="color:#f92672">=</span> embedding_layer(sequence_input)
x <span style="color:#f92672">=</span> Conv1D(<span style="color:#ae81ff">128</span>, <span style="color:#ae81ff">5</span>, activation<span style="color:#f92672">=</span><span style="color:#e6db74">&#39;relu&#39;</span>)(embedded_sequences)
x <span style="color:#f92672">=</span> MaxPooling1D(<span style="color:#ae81ff">5</span>)(x)
x <span style="color:#f92672">=</span> Conv1D(<span style="color:#ae81ff">128</span>, <span style="color:#ae81ff">5</span>, activation<span style="color:#f92672">=</span><span style="color:#e6db74">&#39;relu&#39;</span>)(x)
x <span style="color:#f92672">=</span> MaxPooling1D(<span style="color:#ae81ff">5</span>)(x)
x <span style="color:#f92672">=</span> Conv1D(<span style="color:#ae81ff">128</span>, <span style="color:#ae81ff">5</span>, activation<span style="color:#f92672">=</span><span style="color:#e6db74">&#39;relu&#39;</span>)(x)
x <span style="color:#f92672">=</span> GlobalMaxPooling1D()(x)
x <span style="color:#f92672">=</span> Dense(<span style="color:#ae81ff">128</span>, activation<span style="color:#f92672">=</span><span style="color:#e6db74">&#39;relu&#39;</span>)(x)
preds <span style="color:#f92672">=</span> Dense(len(labels_index), activation<span style="color:#f92672">=</span><span style="color:#e6db74">&#39;softmax&#39;</span>)(x)

<span style="color:#75715e"># Layers stacked differently with x var holding each layer</span>
model <span style="color:#f92672">=</span> Model(sequence_input, preds)
model<span style="color:#f92672">.</span>compile(loss<span style="color:#f92672">=</span><span style="color:#e6db74">&#39;categorical_crossentropy&#39;</span>,
              optimizer<span style="color:#f92672">=</span><span style="color:#e6db74">&#39;rmsprop&#39;</span>,
              metrics<span style="color:#f92672">=</span>[<span style="color:#e6db74">&#39;acc&#39;</span>])

model<span style="color:#f92672">.</span>fit(x_train, y_train,
          batch_size<span style="color:#f92672">=</span><span style="color:#ae81ff">128</span>,
          epochs<span style="color:#f92672">=</span><span style="color:#ae81ff">10</span>,
          validation_data<span style="color:#f92672">=</span>(x_val, y_val))</code></pre></div>
<blockquote>
<p>Here used different measure for calcu loss; above illustrates basic LSTM, a CNN and a CNN using pretrained word embeddings
    1. See progressive rise in performance of each networkds
    2. Embeddings are esp. useful when not much data
    3. CNN generally perform better than Sequential, those using word-embedding even better
    4. Useful to train and compare with Non-NN model such as NB or SVM</p>
</blockquote>

<h4 id="classification-with-spacy">Classification with SpaCy</h4>

<ul>
<li>While keras works esp. well in standalone text classification, it might be useuful to use Keras plus spaCy</li>
<li>2 Ways to Text Classification in SpaCy

<ol>
<li>Own NN library <strong>THINC</strong></li>
<li>Keras</li>
</ol></li>
<li>Example 1 <a href="https://github.com/explosion/spaCy/blob/master/examples/deep_learning_keras.py">code</a>
&gt; Set up
&gt;     1. This example shows how to use an LSTM sentiment classification model trained using Keras in spaCy. spaCy splits the document into sentences, and each sentence is classified using the LSTM. The scores for the sentences are then aggregated to give the document score.
&gt;     2. This kind of hierarchical model is quite difficult in &ldquo;pure&rdquo; Keras or Tensorflow, but it&rsquo;s very effective. The Keras example on this dataset performs quite poorly, because it cuts off the documents so that they&rsquo;re a fixed size. This hurts review accuracy a lot, because people often summarise their rating in the final sentence
&gt;     3. Prerequesit: spacy download en_vectors_web_lg / pip install keras==2.0.9 / Compatible with: spaCy v2.0.0+</li>
</ul>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-python" data-lang="python"><span style="color:#f92672">import</span> plac
<span style="color:#f92672">import</span> random
<span style="color:#f92672">import</span> pathlib
<span style="color:#f92672">import</span> cytoolz
<span style="color:#f92672">import</span> numpy
<span style="color:#f92672">from</span> keras.models <span style="color:#f92672">import</span> Sequential, model_from_json
<span style="color:#f92672">from</span> keras.layers <span style="color:#f92672">import</span> LSTM, Dense, Embedding, Bidirectional
<span style="color:#f92672">from</span> keras.layers <span style="color:#f92672">import</span> TimeDistributed
<span style="color:#f92672">from</span> keras.optimizers <span style="color:#f92672">import</span> Adam
<span style="color:#f92672">import</span> thinc.extra.datasets
<span style="color:#f92672">from</span> spacy.compat <span style="color:#f92672">import</span> pickle
<span style="color:#f92672">import</span> spacy

<span style="color:#66d9ef">class</span> <span style="color:#a6e22e">SentimentAnalyser</span>(object):
    <span style="color:#a6e22e">@classmethod</span>
    <span style="color:#66d9ef">def</span> <span style="color:#a6e22e">load</span>(cls, path, nlp, max_length<span style="color:#f92672">=</span><span style="color:#ae81ff">100</span>):
        <span style="color:#66d9ef">with</span> (path <span style="color:#f92672">/</span> <span style="color:#e6db74">&#39;config.json&#39;</span>)<span style="color:#f92672">.</span>open() <span style="color:#66d9ef">as</span> file_:
            model <span style="color:#f92672">=</span> model_from_json(file_<span style="color:#f92672">.</span>read())
        <span style="color:#66d9ef">with</span> (path <span style="color:#f92672">/</span> <span style="color:#e6db74">&#39;model&#39;</span>)<span style="color:#f92672">.</span>open(<span style="color:#e6db74">&#39;rb&#39;</span>) <span style="color:#66d9ef">as</span> file_:
            lstm_weights <span style="color:#f92672">=</span> pickle<span style="color:#f92672">.</span>load(file_)
        embeddings <span style="color:#f92672">=</span> get_embeddings(nlp<span style="color:#f92672">.</span>vocab)
        model<span style="color:#f92672">.</span>set_weights([embeddings] <span style="color:#f92672">+</span> lstm_weights)
        <span style="color:#66d9ef">return</span> cls(model, max_length<span style="color:#f92672">=</span>max_length)

    <span style="color:#66d9ef">def</span> __init__(self, model, max_length<span style="color:#f92672">=</span><span style="color:#ae81ff">100</span>):
        self<span style="color:#f92672">.</span>_model <span style="color:#f92672">=</span> model
        self<span style="color:#f92672">.</span>max_length <span style="color:#f92672">=</span> max_length

    <span style="color:#66d9ef">def</span> __call__(self, doc):
        X <span style="color:#f92672">=</span> get_features([doc], self<span style="color:#f92672">.</span>max_length)
        y <span style="color:#f92672">=</span> self<span style="color:#f92672">.</span>_model<span style="color:#f92672">.</span>predict(X)
        self<span style="color:#f92672">.</span>set_sentiment(doc, y)
        
    <span style="color:#75715e"># set up class and how to load model and embedding weights</span>
    <span style="color:#75715e"># INIT model, max length, instructions to predict</span>
    <span style="color:#75715e"># Load method returns model to use in eval</span>
    <span style="color:#75715e"># call gets features and pred</span>

    <span style="color:#66d9ef">def</span> <span style="color:#a6e22e">pipe</span>(self, docs, batch_size<span style="color:#f92672">=</span><span style="color:#ae81ff">1000</span>, n_threads<span style="color:#f92672">=</span><span style="color:#ae81ff">2</span>):
        <span style="color:#66d9ef">for</span> minibatch <span style="color:#f92672">in</span> cytoolz<span style="color:#f92672">.</span>partition_all(batch_size, docs):
            minibatch <span style="color:#f92672">=</span> list(minibatch)
            sentences <span style="color:#f92672">=</span> []
            <span style="color:#66d9ef">for</span> doc <span style="color:#f92672">in</span> minibatch:
                sentences<span style="color:#f92672">.</span>extend(doc<span style="color:#f92672">.</span>sents)
            Xs <span style="color:#f92672">=</span> get_features(sentences, self<span style="color:#f92672">.</span>max_length)
            ys <span style="color:#f92672">=</span> self<span style="color:#f92672">.</span>_model<span style="color:#f92672">.</span>predict(Xs)
            <span style="color:#66d9ef">for</span> sent, label <span style="color:#f92672">in</span> zip(sentences, ys):
                sent<span style="color:#f92672">.</span>doc<span style="color:#f92672">.</span>sentiment <span style="color:#f92672">+=</span> label <span style="color:#f92672">-</span> <span style="color:#ae81ff">0.5</span>
            <span style="color:#66d9ef">for</span> doc <span style="color:#f92672">in</span> minibatch:
                <span style="color:#66d9ef">yield</span> doc

    <span style="color:#66d9ef">def</span> <span style="color:#a6e22e">set_sentiment</span>(self, doc, y):
        doc<span style="color:#f92672">.</span>sentiment <span style="color:#f92672">=</span> float(y[<span style="color:#ae81ff">0</span>])
        <span style="color:#75715e"># Sentiment has a native slot for a single float.</span>
        <span style="color:#75715e"># For arbitrary data storage, there&#39;s:</span>
        <span style="color:#75715e"># doc.user_data[&#39;my_data&#39;] = y</span>
        
<span style="color:#75715e"># </span>
<span style="color:#66d9ef">def</span> <span style="color:#a6e22e">get_labelled_sentences</span>(docs, doc_labels):
    labels <span style="color:#f92672">=</span> []
    sentences <span style="color:#f92672">=</span> []
    <span style="color:#66d9ef">for</span> doc, y <span style="color:#f92672">in</span> zip(docs, doc_labels):
        <span style="color:#66d9ef">for</span> sent <span style="color:#f92672">in</span> doc<span style="color:#f92672">.</span>sents:
            sentences<span style="color:#f92672">.</span>append(sent)
            labels<span style="color:#f92672">.</span>append(y)
    <span style="color:#66d9ef">return</span> sentences, numpy<span style="color:#f92672">.</span>asarray(labels, dtype<span style="color:#f92672">=</span><span style="color:#e6db74">&#39;int32&#39;</span>)


<span style="color:#66d9ef">def</span> <span style="color:#a6e22e">get_features</span>(docs, max_length):
    docs <span style="color:#f92672">=</span> list(docs)
    Xs <span style="color:#f92672">=</span> numpy<span style="color:#f92672">.</span>zeros((len(docs), max_length), dtype<span style="color:#f92672">=</span><span style="color:#e6db74">&#39;int32&#39;</span>)
    <span style="color:#66d9ef">for</span> i, doc <span style="color:#f92672">in</span> enumerate(docs):
        j <span style="color:#f92672">=</span> <span style="color:#ae81ff">0</span>
        <span style="color:#66d9ef">for</span> token <span style="color:#f92672">in</span> doc:
            vector_id <span style="color:#f92672">=</span> token<span style="color:#f92672">.</span>vocab<span style="color:#f92672">.</span>vectors<span style="color:#f92672">.</span>find(key<span style="color:#f92672">=</span>token<span style="color:#f92672">.</span>orth)
            <span style="color:#66d9ef">if</span> vector_id <span style="color:#f92672">&gt;=</span> <span style="color:#ae81ff">0</span>:
                Xs[i, j] <span style="color:#f92672">=</span> vector_id
            <span style="color:#66d9ef">else</span>:
                Xs[i, j] <span style="color:#f92672">=</span> <span style="color:#ae81ff">0</span>
            j <span style="color:#f92672">+=</span> <span style="color:#ae81ff">1</span>
            <span style="color:#66d9ef">if</span> j <span style="color:#f92672">&gt;=</span> max_length:
                <span style="color:#66d9ef">break</span>
    <span style="color:#66d9ef">return</span> Xs

<span style="color:#75715e"># Below is where all heavy lifting place - NOTE lines involving spacy&#39;s pipeline of sentencizer</span>
<span style="color:#66d9ef">def</span> <span style="color:#a6e22e">train</span>(train_texts, train_labels, dev_texts, dev_labels,
          lstm_shape, lstm_settings, lstm_optimizer, batch_size<span style="color:#f92672">=</span><span style="color:#ae81ff">100</span>,
          nb_epoch<span style="color:#f92672">=</span><span style="color:#ae81ff">5</span>, by_sentence<span style="color:#f92672">=</span>True):
    <span style="color:#66d9ef">print</span>(<span style="color:#e6db74">&#34;Loading spaCy&#34;</span>)
    nlp <span style="color:#f92672">=</span> spacy<span style="color:#f92672">.</span>load(<span style="color:#e6db74">&#39;en_vectors_web_lg&#39;</span>)
    nlp<span style="color:#f92672">.</span>add_pipe(nlp<span style="color:#f92672">.</span>create_pipe(<span style="color:#e6db74">&#39;sentencizer&#39;</span>))
    embeddings <span style="color:#f92672">=</span> get_embeddings(nlp<span style="color:#f92672">.</span>vocab)
    model <span style="color:#f92672">=</span> compile_lstm(embeddings, lstm_shape, lstm_settings)
    <span style="color:#66d9ef">print</span>(<span style="color:#e6db74">&#34;Parsing texts...&#34;</span>)
    train_docs <span style="color:#f92672">=</span> list(nlp<span style="color:#f92672">.</span>pipe(train_texts))
    dev_docs <span style="color:#f92672">=</span> list(nlp<span style="color:#f92672">.</span>pipe(dev_texts))
    <span style="color:#66d9ef">if</span> by_sentence:
        train_docs, train_labels <span style="color:#f92672">=</span> get_labelled_sentences(train_docs, train_labels)
        dev_docs, dev_labels <span style="color:#f92672">=</span> get_labelled_sentences(dev_docs, dev_labels)

    train_X <span style="color:#f92672">=</span> get_features(train_docs, lstm_shape[<span style="color:#e6db74">&#39;max_length&#39;</span>])
    dev_X <span style="color:#f92672">=</span> get_features(dev_docs, lstm_shape[<span style="color:#e6db74">&#39;max_length&#39;</span>])
    model<span style="color:#f92672">.</span>fit(train_X, train_labels, validation_data<span style="color:#f92672">=</span>(dev_X, dev_labels),
              nb_epoch<span style="color:#f92672">=</span>nb_epoch, batch_size<span style="color:#f92672">=</span>batch_size)
    <span style="color:#66d9ef">return</span> model

<span style="color:#75715e"># Like previously, set up each layers and stack up</span>
<span style="color:#75715e"># Any Keras model would do, here bidriectional LSTM</span>
<span style="color:#66d9ef">def</span> <span style="color:#a6e22e">compile_lstm</span>(embeddings, shape, settings):
    model <span style="color:#f92672">=</span> Sequential()
    model<span style="color:#f92672">.</span>add(
        Embedding(
            embeddings<span style="color:#f92672">.</span>shape[<span style="color:#ae81ff">0</span>],
            embeddings<span style="color:#f92672">.</span>shape[<span style="color:#ae81ff">1</span>],
            input_length<span style="color:#f92672">=</span>shape[<span style="color:#e6db74">&#39;max_length&#39;</span>],
            trainable<span style="color:#f92672">=</span>False,
            weights<span style="color:#f92672">=</span>[embeddings],
            mask_zero<span style="color:#f92672">=</span>True
        )
    )
    model<span style="color:#f92672">.</span>add(TimeDistributed(Dense(shape[<span style="color:#e6db74">&#39;nr_hidden&#39;</span>], use_bias<span style="color:#f92672">=</span>False)))
    model<span style="color:#f92672">.</span>add(Bidirectional(LSTM(shape[<span style="color:#e6db74">&#39;nr_hidden&#39;</span>],
                                 recurrent_dropout<span style="color:#f92672">=</span>settings[<span style="color:#e6db74">&#39;dropout&#39;</span>],
                                 dropout<span style="color:#f92672">=</span>settings[<span style="color:#e6db74">&#39;dropout&#39;</span>])))
    model<span style="color:#f92672">.</span>add(Dense(shape[<span style="color:#e6db74">&#39;nr_class&#39;</span>], activation<span style="color:#f92672">=</span><span style="color:#e6db74">&#39;sigmoid&#39;</span>))
    model<span style="color:#f92672">.</span>compile(optimizer<span style="color:#f92672">=</span>Adam(lr<span style="color:#f92672">=</span>settings[<span style="color:#e6db74">&#39;lr&#39;</span>]), loss<span style="color:#f92672">=</span><span style="color:#e6db74">&#39;binary_crossentropy&#39;</span>,
          metrics<span style="color:#f92672">=</span>[<span style="color:#e6db74">&#39;accuracy&#39;</span>])
    <span style="color:#66d9ef">return</span> model

<span style="color:#75715e"># Eval method retunrs a score of how well model performed; checks assigned sentiment score with label of document</span>
<span style="color:#66d9ef">def</span> <span style="color:#a6e22e">get_embeddings</span>(vocab):
    <span style="color:#66d9ef">return</span> vocab<span style="color:#f92672">.</span>vectors<span style="color:#f92672">.</span>data


<span style="color:#66d9ef">def</span> <span style="color:#a6e22e">evaluate</span>(model_dir, texts, labels, max_length<span style="color:#f92672">=</span><span style="color:#ae81ff">100</span>):
    <span style="color:#66d9ef">def</span> <span style="color:#a6e22e">create_pipeline</span>(nlp):
        <span style="color:#e6db74">&#39;&#39;&#39;
</span><span style="color:#e6db74">        This could be a lambda, but named functions are easier to read in Python.
</span><span style="color:#e6db74">        &#39;&#39;&#39;</span>
        <span style="color:#66d9ef">return</span> [nlp<span style="color:#f92672">.</span>tagger, nlp<span style="color:#f92672">.</span>parser, SentimentAnalyser<span style="color:#f92672">.</span>load(model_dir, nlp,
                                                               max_length<span style="color:#f92672">=</span>max_length)]

    nlp <span style="color:#f92672">=</span> spacy<span style="color:#f92672">.</span>load(<span style="color:#e6db74">&#39;en&#39;</span>)
    nlp<span style="color:#f92672">.</span>pipeline <span style="color:#f92672">=</span> create_pipeline(nlp)

    correct <span style="color:#f92672">=</span> <span style="color:#ae81ff">0</span>
    i <span style="color:#f92672">=</span> <span style="color:#ae81ff">0</span>
    <span style="color:#66d9ef">for</span> doc <span style="color:#f92672">in</span> nlp<span style="color:#f92672">.</span>pipe(texts, batch_size<span style="color:#f92672">=</span><span style="color:#ae81ff">1000</span>, n_threads<span style="color:#f92672">=</span><span style="color:#ae81ff">4</span>):
        correct <span style="color:#f92672">+=</span> bool(doc<span style="color:#f92672">.</span>sentiment <span style="color:#f92672">&gt;=</span> <span style="color:#ae81ff">0.5</span>) <span style="color:#f92672">==</span> bool(labels[i])
        i <span style="color:#f92672">+=</span> <span style="color:#ae81ff">1</span>
    <span style="color:#66d9ef">return</span> float(correct) <span style="color:#f92672">/</span> i

<span style="color:#75715e"># Using IMBD sentiment analysis datteset, this method is an API to access data</span>
<span style="color:#66d9ef">def</span> <span style="color:#a6e22e">read_data</span>(data_dir, limit<span style="color:#f92672">=</span><span style="color:#ae81ff">0</span>):
    examples <span style="color:#f92672">=</span> []
    <span style="color:#66d9ef">for</span> subdir, label <span style="color:#f92672">in</span> ((<span style="color:#e6db74">&#39;pos&#39;</span>, <span style="color:#ae81ff">1</span>), (<span style="color:#e6db74">&#39;neg&#39;</span>, <span style="color:#ae81ff">0</span>)):
        <span style="color:#66d9ef">for</span> filename <span style="color:#f92672">in</span> (data_dir <span style="color:#f92672">/</span> subdir)<span style="color:#f92672">.</span>iterdir():
            <span style="color:#66d9ef">with</span> filename<span style="color:#f92672">.</span>open() <span style="color:#66d9ef">as</span> file_:
                text <span style="color:#f92672">=</span> file_<span style="color:#f92672">.</span>read()
            examples<span style="color:#f92672">.</span>append((text, label))
    random<span style="color:#f92672">.</span>shuffle(examples)
    <span style="color:#66d9ef">if</span> limit <span style="color:#f92672">&gt;=</span> <span style="color:#ae81ff">1</span>:
        examples <span style="color:#f92672">=</span> examples[:limit]
    <span style="color:#66d9ef">return</span> zip(<span style="color:#f92672">*</span>examples) <span style="color:#75715e"># Unzips into two lists</span>

<span style="color:#75715e"># Annotations set up options setting various model directories, runtime, and params </span>
<span style="color:#a6e22e">@plac.annotations</span>(
    train_dir<span style="color:#f92672">=</span>(<span style="color:#e6db74">&#34;Location of training file or directory&#34;</span>),
    dev_dir<span style="color:#f92672">=</span>(<span style="color:#e6db74">&#34;Location of development file or directory&#34;</span>),
    model_dir<span style="color:#f92672">=</span>(<span style="color:#e6db74">&#34;Location of output model directory&#34;</span>,),
    is_runtime<span style="color:#f92672">=</span>(<span style="color:#e6db74">&#34;Demonstrate run-time usage&#34;</span>, <span style="color:#e6db74">&#34;flag&#34;</span>, <span style="color:#e6db74">&#34;r&#34;</span>, bool),
    nr_hidden<span style="color:#f92672">=</span>(<span style="color:#e6db74">&#34;Number of hidden units&#34;</span>, <span style="color:#e6db74">&#34;option&#34;</span>, <span style="color:#e6db74">&#34;H&#34;</span>, int),
    max_length<span style="color:#f92672">=</span>(<span style="color:#e6db74">&#34;Maximum sentence length&#34;</span>, <span style="color:#e6db74">&#34;option&#34;</span>, <span style="color:#e6db74">&#34;L&#34;</span>, int),
    dropout<span style="color:#f92672">=</span>(<span style="color:#e6db74">&#34;Dropout&#34;</span>, <span style="color:#e6db74">&#34;option&#34;</span>, <span style="color:#e6db74">&#34;d&#34;</span>, float),
    learn_rate<span style="color:#f92672">=</span>(<span style="color:#e6db74">&#34;Learn rate&#34;</span>, <span style="color:#e6db74">&#34;option&#34;</span>, <span style="color:#e6db74">&#34;e&#34;</span>, float),
    nb_epoch<span style="color:#f92672">=</span>(<span style="color:#e6db74">&#34;Number of training epochs&#34;</span>, <span style="color:#e6db74">&#34;option&#34;</span>, <span style="color:#e6db74">&#34;i&#34;</span>, int),
    batch_size<span style="color:#f92672">=</span>(<span style="color:#e6db74">&#34;Size of minibatches for training LSTM&#34;</span>, <span style="color:#e6db74">&#34;option&#34;</span>, <span style="color:#e6db74">&#34;b&#34;</span>, int),
    nr_examples<span style="color:#f92672">=</span>(<span style="color:#e6db74">&#34;Limit to N examples&#34;</span>, <span style="color:#e6db74">&#34;option&#34;</span>, <span style="color:#e6db74">&#34;n&#34;</span>, int)
)


<span style="color:#75715e"># Now the main functions</span>
<span style="color:#66d9ef">def</span> <span style="color:#a6e22e">main</span>(model_dir<span style="color:#f92672">=</span>None, train_dir<span style="color:#f92672">=</span>None, dev_dir<span style="color:#f92672">=</span>None,
         is_runtime<span style="color:#f92672">=</span>False,
         nr_hidden<span style="color:#f92672">=</span><span style="color:#ae81ff">64</span>, max_length<span style="color:#f92672">=</span><span style="color:#ae81ff">100</span>, <span style="color:#75715e"># Shape</span>
         dropout<span style="color:#f92672">=</span><span style="color:#ae81ff">0.5</span>, learn_rate<span style="color:#f92672">=</span><span style="color:#ae81ff">0.001</span>, <span style="color:#75715e"># General NN config</span>
         nb_epoch<span style="color:#f92672">=</span><span style="color:#ae81ff">5</span>, batch_size<span style="color:#f92672">=</span><span style="color:#ae81ff">100</span>, nr_examples<span style="color:#f92672">=-</span><span style="color:#ae81ff">1</span>):  <span style="color:#75715e"># Training params</span>
    <span style="color:#66d9ef">if</span> model_dir <span style="color:#f92672">is</span> <span style="color:#f92672">not</span> None:
        model_dir <span style="color:#f92672">=</span> pathlib<span style="color:#f92672">.</span>Path(model_dir)
    <span style="color:#66d9ef">if</span> train_dir <span style="color:#f92672">is</span> None <span style="color:#f92672">or</span> dev_dir <span style="color:#f92672">is</span> None:
        imdb_data <span style="color:#f92672">=</span> thinc<span style="color:#f92672">.</span>extra<span style="color:#f92672">.</span>datasets<span style="color:#f92672">.</span>imdb()
    <span style="color:#66d9ef">if</span> is_runtime:
        <span style="color:#66d9ef">if</span> dev_dir <span style="color:#f92672">is</span> None:
            dev_texts, dev_labels <span style="color:#f92672">=</span> zip(<span style="color:#f92672">*</span>imdb_data[<span style="color:#ae81ff">1</span>])
        <span style="color:#66d9ef">else</span>:
            dev_texts, dev_labels <span style="color:#f92672">=</span> read_data(dev_dir)
        acc <span style="color:#f92672">=</span> evaluate(model_dir, dev_texts, dev_labels, max_length<span style="color:#f92672">=</span>max_length)
        <span style="color:#66d9ef">print</span>(acc)
    <span style="color:#66d9ef">else</span>:
        <span style="color:#66d9ef">if</span> train_dir <span style="color:#f92672">is</span> None:
            train_texts, train_labels <span style="color:#f92672">=</span> zip(<span style="color:#f92672">*</span>imdb_data[<span style="color:#ae81ff">0</span>])
        <span style="color:#66d9ef">else</span>:
            <span style="color:#66d9ef">print</span>(<span style="color:#e6db74">&#34;Read data&#34;</span>)
            train_texts, train_labels <span style="color:#f92672">=</span> read_data(train_dir, limit<span style="color:#f92672">=</span>nr_examples)
        <span style="color:#66d9ef">if</span> dev_dir <span style="color:#f92672">is</span> None:
            dev_texts, dev_labels <span style="color:#f92672">=</span> zip(<span style="color:#f92672">*</span>imdb_data[<span style="color:#ae81ff">1</span>])
        <span style="color:#66d9ef">else</span>:
            dev_texts, dev_labels <span style="color:#f92672">=</span> read_data(dev_dir, imdb_data, limit<span style="color:#f92672">=</span>nr_examples)
        train_labels <span style="color:#f92672">=</span> numpy<span style="color:#f92672">.</span>asarray(train_labels, dtype<span style="color:#f92672">=</span><span style="color:#e6db74">&#39;int32&#39;</span>)
        dev_labels <span style="color:#f92672">=</span> numpy<span style="color:#f92672">.</span>asarray(dev_labels, dtype<span style="color:#f92672">=</span><span style="color:#e6db74">&#39;int32&#39;</span>)
        lstm <span style="color:#f92672">=</span> train(train_texts, train_labels, dev_texts, dev_labels,
                     {<span style="color:#e6db74">&#39;nr_hidden&#39;</span>: nr_hidden, <span style="color:#e6db74">&#39;max_length&#39;</span>: max_length, <span style="color:#e6db74">&#39;nr_class&#39;</span>: <span style="color:#ae81ff">1</span>},
                     {<span style="color:#e6db74">&#39;dropout&#39;</span>: dropout, <span style="color:#e6db74">&#39;lr&#39;</span>: learn_rate},
                     {},
                     nb_epoch<span style="color:#f92672">=</span>nb_epoch, batch_size<span style="color:#f92672">=</span>batch_size)
        weights <span style="color:#f92672">=</span> lstm<span style="color:#f92672">.</span>get_weights()
        <span style="color:#66d9ef">if</span> model_dir <span style="color:#f92672">is</span> <span style="color:#f92672">not</span> None:
            <span style="color:#66d9ef">with</span> (model_dir <span style="color:#f92672">/</span> <span style="color:#e6db74">&#39;model&#39;</span>)<span style="color:#f92672">.</span>open(<span style="color:#e6db74">&#39;wb&#39;</span>) <span style="color:#66d9ef">as</span> file_:
                pickle<span style="color:#f92672">.</span>dump(weights[<span style="color:#ae81ff">1</span>:], file_)
            <span style="color:#66d9ef">with</span> (model_dir <span style="color:#f92672">/</span> <span style="color:#e6db74">&#39;config.json&#39;</span>)<span style="color:#f92672">.</span>open(<span style="color:#e6db74">&#39;wb&#39;</span>) <span style="color:#66d9ef">as</span> file_:
                file_<span style="color:#f92672">.</span>write(lstm<span style="color:#f92672">.</span>to_json())


<span style="color:#66d9ef">if</span> __name__ <span style="color:#f92672">==</span> <span style="color:#e6db74">&#39;__main__&#39;</span>:
    plac<span style="color:#f92672">.</span>call(main)</code></pre></div>
<blockquote>
<p>First few lines set up model folder and load dataset / then check print run time info, if not training is not complete proceeding to train, train and save model
    1. Running, saving using model in pipelines is huge motif behind Keras and SpaCy in such a way
    2. KEY here updating <code>sentiment</code> attri for each doc (how is optional)
    3. SpaCy GOOD at not removing or truncating input - as users tend to sum up review in last sentence of documents with a lot of sentiment inferred
    4. HOW to USE? model adds one more attribute to doc, <code>doc.sentiment</code> caputuring
    5. Verify by loading saved model and run any document through pipelien the same way through prevous POS, NER and Dep-parsing <code>doc = nlp(document)</code>
    6. <code>nlp</code> is pipeline obj of loaded model trained, <code>docuemnt</code> any unicode text wish to analyse</p>
</blockquote>

<ul>
<li>Non-NN classifier

<ol>
<li>proba of duc belonging to particular class</li>
<li>simple, use <code>update</code> <a href="https://spacy.io/usage/training#section-textcat">code</a> and <a href="https://github.com/explosion/spacy/blob/master/examples/training/train_textcat.py">github</a></li>
</ol></li>
<li>Example below to run at once</li>
</ul>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-python" data-lang="python"><span style="color:#f92672">from</span> __future__ <span style="color:#f92672">import</span> unicode_literals, print_function
<span style="color:#f92672">import</span> plac
<span style="color:#f92672">import</span> random
<span style="color:#f92672">from</span> pathlib <span style="color:#f92672">import</span> Path
<span style="color:#f92672">import</span> thinc.extra.datasets

<span style="color:#f92672">import</span> spacy
<span style="color:#f92672">from</span> spacy.util <span style="color:#f92672">import</span> minibatch, compounding

<span style="color:#75715e"># Not Keras, but SpaCy&#39;s THINC</span>

<span style="color:#a6e22e">@plac.annotations</span>(
    model<span style="color:#f92672">=</span>(<span style="color:#e6db74">&#34;Model name. Defaults to blank &#39;en&#39; model.&#34;</span>, <span style="color:#e6db74">&#34;option&#34;</span>, <span style="color:#e6db74">&#34;m&#34;</span>, str),
    output_dir<span style="color:#f92672">=</span>(<span style="color:#e6db74">&#34;Optional output directory&#34;</span>, <span style="color:#e6db74">&#34;option&#34;</span>, <span style="color:#e6db74">&#34;o&#34;</span>, Path),
    n_texts<span style="color:#f92672">=</span>(<span style="color:#e6db74">&#34;Number of texts to train from&#34;</span>, <span style="color:#e6db74">&#34;option&#34;</span>, <span style="color:#e6db74">&#34;t&#34;</span>, int),
    n_iter<span style="color:#f92672">=</span>(<span style="color:#e6db74">&#34;Number of training iterations&#34;</span>, <span style="color:#e6db74">&#34;option&#34;</span>, <span style="color:#e6db74">&#34;n&#34;</span>, int))
<span style="color:#66d9ef">def</span> <span style="color:#a6e22e">main</span>(model<span style="color:#f92672">=</span>None, output_dir<span style="color:#f92672">=</span>None, n_iter<span style="color:#f92672">=</span><span style="color:#ae81ff">20</span>, n_texts<span style="color:#f92672">=</span><span style="color:#ae81ff">2000</span>):
    <span style="color:#66d9ef">if</span> model <span style="color:#f92672">is</span> <span style="color:#f92672">not</span> None:
        nlp <span style="color:#f92672">=</span> spacy<span style="color:#f92672">.</span>load(model)  <span style="color:#75715e"># load existing spaCy model</span>
        <span style="color:#66d9ef">print</span>(<span style="color:#e6db74">&#34;Loaded model &#39;</span><span style="color:#e6db74">%s</span><span style="color:#e6db74">&#39;&#34;</span> <span style="color:#f92672">%</span> model)
    <span style="color:#66d9ef">else</span>:
        nlp <span style="color:#f92672">=</span> spacy<span style="color:#f92672">.</span>blank(<span style="color:#e6db74">&#39;en&#39;</span>)  <span style="color:#75715e"># create blank Language class</span>
        <span style="color:#66d9ef">print</span>(<span style="color:#e6db74">&#34;Created blank &#39;en&#39; model&#34;</span>)

    <span style="color:#75715e"># add the text classifier to the pipeline if it doesn&#39;t exist</span>
    <span style="color:#75715e"># nlp.create_pipe works for built-ins that are registered with spaCy</span>
    <span style="color:#66d9ef">if</span> <span style="color:#e6db74">&#39;textcat&#39;</span> <span style="color:#f92672">not</span> <span style="color:#f92672">in</span> nlp<span style="color:#f92672">.</span>pipe_names:
        textcat <span style="color:#f92672">=</span> nlp<span style="color:#f92672">.</span>create_pipe(<span style="color:#e6db74">&#39;textcat&#39;</span>)
        nlp<span style="color:#f92672">.</span>add_pipe(textcat, last<span style="color:#f92672">=</span>True)
    <span style="color:#75715e"># otherwise, get it, so we can add labels to it</span>
    <span style="color:#66d9ef">else</span>:
        textcat <span style="color:#f92672">=</span> nlp<span style="color:#f92672">.</span>get_pipe(<span style="color:#e6db74">&#39;textcat&#39;</span>)

    <span style="color:#75715e"># add label to text classifier</span>
    textcat<span style="color:#f92672">.</span>add_label(<span style="color:#e6db74">&#39;POSITIVE&#39;</span>)

    <span style="color:#75715e"># load the IMDB dataset</span>
    <span style="color:#66d9ef">print</span>(<span style="color:#e6db74">&#34;Loading IMDB data...&#34;</span>)
    (train_texts, train_cats), (dev_texts, dev_cats) <span style="color:#f92672">=</span> load_data(limit<span style="color:#f92672">=</span>n_texts)
    <span style="color:#66d9ef">print</span>(<span style="color:#e6db74">&#34;Using {} examples ({} training, {} evaluation)&#34;</span>
          <span style="color:#f92672">.</span>format(n_texts, len(train_texts), len(dev_texts)))
    train_data <span style="color:#f92672">=</span> list(zip(train_texts,
                          [{<span style="color:#e6db74">&#39;cats&#39;</span>: cats} <span style="color:#66d9ef">for</span> cats <span style="color:#f92672">in</span> train_cats]))

    <span style="color:#75715e"># get names of other pipes to disable them during training</span>
    other_pipes <span style="color:#f92672">=</span> [pipe <span style="color:#66d9ef">for</span> pipe <span style="color:#f92672">in</span> nlp<span style="color:#f92672">.</span>pipe_names <span style="color:#66d9ef">if</span> pipe <span style="color:#f92672">!=</span> <span style="color:#e6db74">&#39;textcat&#39;</span>]
    <span style="color:#66d9ef">with</span> nlp<span style="color:#f92672">.</span>disable_pipes(<span style="color:#f92672">*</span>other_pipes):  <span style="color:#75715e"># only train textcat</span>
        optimizer <span style="color:#f92672">=</span> nlp<span style="color:#f92672">.</span>begin_training()
        <span style="color:#66d9ef">print</span>(<span style="color:#e6db74">&#34;Training the model...&#34;</span>)
        <span style="color:#66d9ef">print</span>(<span style="color:#e6db74">&#39;{:^5}</span><span style="color:#ae81ff">\t</span><span style="color:#e6db74">{:^5}</span><span style="color:#ae81ff">\t</span><span style="color:#e6db74">{:^5}</span><span style="color:#ae81ff">\t</span><span style="color:#e6db74">{:^5}&#39;</span><span style="color:#f92672">.</span>format(<span style="color:#e6db74">&#39;LOSS&#39;</span>, <span style="color:#e6db74">&#39;P&#39;</span>, <span style="color:#e6db74">&#39;R&#39;</span>, <span style="color:#e6db74">&#39;F&#39;</span>))
        <span style="color:#66d9ef">for</span> i <span style="color:#f92672">in</span> range(n_iter):
            losses <span style="color:#f92672">=</span> {}
            <span style="color:#75715e"># batch up the examples using spaCy&#39;s minibatch</span>
            batches <span style="color:#f92672">=</span> minibatch(train_data, size<span style="color:#f92672">=</span>compounding(<span style="color:#ae81ff">4.</span>, <span style="color:#ae81ff">32.</span>, <span style="color:#ae81ff">1.001</span>))
            <span style="color:#66d9ef">for</span> batch <span style="color:#f92672">in</span> batches:
                texts, annotations <span style="color:#f92672">=</span> zip(<span style="color:#f92672">*</span>batch)
                nlp<span style="color:#f92672">.</span>update(texts, annotations, sgd<span style="color:#f92672">=</span>optimizer, drop<span style="color:#f92672">=</span><span style="color:#ae81ff">0.2</span>,
                           losses<span style="color:#f92672">=</span>losses)
            <span style="color:#66d9ef">with</span> textcat<span style="color:#f92672">.</span>model<span style="color:#f92672">.</span>use_params(optimizer<span style="color:#f92672">.</span>averages):
                <span style="color:#75715e"># evaluate on the dev data split off in load_data()</span>
                scores <span style="color:#f92672">=</span> evaluate(nlp<span style="color:#f92672">.</span>tokenizer, textcat, dev_texts, dev_cats)
            <span style="color:#66d9ef">print</span>(<span style="color:#e6db74">&#39;{0:.3f}</span><span style="color:#ae81ff">\t</span><span style="color:#e6db74">{1:.3f}</span><span style="color:#ae81ff">\t</span><span style="color:#e6db74">{2:.3f}</span><span style="color:#ae81ff">\t</span><span style="color:#e6db74">{3:.3f}&#39;</span>  <span style="color:#75715e"># print a simple table</span>
                  <span style="color:#f92672">.</span>format(losses[<span style="color:#e6db74">&#39;textcat&#39;</span>], scores[<span style="color:#e6db74">&#39;textcat_p&#39;</span>],
                          scores[<span style="color:#e6db74">&#39;textcat_r&#39;</span>], scores[<span style="color:#e6db74">&#39;textcat_f&#39;</span>]))

    <span style="color:#75715e"># test the trained model</span>
    test_text <span style="color:#f92672">=</span> <span style="color:#e6db74">&#34;This movie sucked&#34;</span>
    doc <span style="color:#f92672">=</span> nlp(test_text)
    <span style="color:#66d9ef">print</span>(test_text, doc<span style="color:#f92672">.</span>cats)

    <span style="color:#66d9ef">if</span> output_dir <span style="color:#f92672">is</span> <span style="color:#f92672">not</span> None:
        output_dir <span style="color:#f92672">=</span> Path(output_dir)
        <span style="color:#66d9ef">if</span> <span style="color:#f92672">not</span> output_dir<span style="color:#f92672">.</span>exists():
            output_dir<span style="color:#f92672">.</span>mkdir()
        nlp<span style="color:#f92672">.</span>to_disk(output_dir)
        <span style="color:#66d9ef">print</span>(<span style="color:#e6db74">&#34;Saved model to&#34;</span>, output_dir)

        <span style="color:#75715e"># test the saved model</span>
        <span style="color:#66d9ef">print</span>(<span style="color:#e6db74">&#34;Loading from&#34;</span>, output_dir)
        nlp2 <span style="color:#f92672">=</span> spacy<span style="color:#f92672">.</span>load(output_dir)
        doc2 <span style="color:#f92672">=</span> nlp2(test_text)
        <span style="color:#66d9ef">print</span>(test_text, doc2<span style="color:#f92672">.</span>cats)

<span style="color:#75715e"># test model with eval method, calcu precision, recall F-score, last part saving trained model in output dir</span>

<span style="color:#66d9ef">def</span> <span style="color:#a6e22e">load_data</span>(limit<span style="color:#f92672">=</span><span style="color:#ae81ff">0</span>, split<span style="color:#f92672">=</span><span style="color:#ae81ff">0.8</span>):
    <span style="color:#e6db74">&#34;&#34;&#34;Load data from the IMDB dataset.&#34;&#34;&#34;</span>
    <span style="color:#75715e"># Partition off part of the train data for evaluation</span>
    train_data, _ <span style="color:#f92672">=</span> thinc<span style="color:#f92672">.</span>extra<span style="color:#f92672">.</span>datasets<span style="color:#f92672">.</span>imdb()
    random<span style="color:#f92672">.</span>shuffle(train_data)
    train_data <span style="color:#f92672">=</span> train_data[<span style="color:#f92672">-</span>limit:]
    texts, labels <span style="color:#f92672">=</span> zip(<span style="color:#f92672">*</span>train_data)
    cats <span style="color:#f92672">=</span> [{<span style="color:#e6db74">&#39;POSITIVE&#39;</span>: bool(y)} <span style="color:#66d9ef">for</span> y <span style="color:#f92672">in</span> labels]
    split <span style="color:#f92672">=</span> int(len(train_data) <span style="color:#f92672">*</span> split)
    <span style="color:#66d9ef">return</span> (texts[:split], cats[:split]), (texts[split:], cats[split:])


<span style="color:#66d9ef">def</span> <span style="color:#a6e22e">evaluate</span>(tokenizer, textcat, texts, cats):
    docs <span style="color:#f92672">=</span> (tokenizer(text) <span style="color:#66d9ef">for</span> text <span style="color:#f92672">in</span> texts)
    tp <span style="color:#f92672">=</span> <span style="color:#ae81ff">1e-8</span>  <span style="color:#75715e"># True positives</span>
    fp <span style="color:#f92672">=</span> <span style="color:#ae81ff">1e-8</span>  <span style="color:#75715e"># False positives</span>
    fn <span style="color:#f92672">=</span> <span style="color:#ae81ff">1e-8</span>  <span style="color:#75715e"># False negatives</span>
    tn <span style="color:#f92672">=</span> <span style="color:#ae81ff">1e-8</span>  <span style="color:#75715e"># True negatives</span>
    <span style="color:#66d9ef">for</span> i, doc <span style="color:#f92672">in</span> enumerate(textcat<span style="color:#f92672">.</span>pipe(docs)):
        gold <span style="color:#f92672">=</span> cats[i]
        <span style="color:#66d9ef">for</span> label, score <span style="color:#f92672">in</span> doc<span style="color:#f92672">.</span>cats<span style="color:#f92672">.</span>items():
            <span style="color:#66d9ef">if</span> label <span style="color:#f92672">not</span> <span style="color:#f92672">in</span> gold:
                <span style="color:#66d9ef">continue</span>
            <span style="color:#66d9ef">if</span> score <span style="color:#f92672">&gt;=</span> <span style="color:#ae81ff">0.5</span> <span style="color:#f92672">and</span> gold[label] <span style="color:#f92672">&gt;=</span> <span style="color:#ae81ff">0.5</span>:
                tp <span style="color:#f92672">+=</span> <span style="color:#ae81ff">1.</span>
            <span style="color:#66d9ef">elif</span> score <span style="color:#f92672">&gt;=</span> <span style="color:#ae81ff">0.5</span> <span style="color:#f92672">and</span> gold[label] <span style="color:#f92672">&lt;</span> <span style="color:#ae81ff">0.5</span>:
                fp <span style="color:#f92672">+=</span> <span style="color:#ae81ff">1.</span>
            <span style="color:#66d9ef">elif</span> score <span style="color:#f92672">&lt;</span> <span style="color:#ae81ff">0.5</span> <span style="color:#f92672">and</span> gold[label] <span style="color:#f92672">&lt;</span> <span style="color:#ae81ff">0.5</span>:
                tn <span style="color:#f92672">+=</span> <span style="color:#ae81ff">1</span>
            <span style="color:#66d9ef">elif</span> score <span style="color:#f92672">&lt;</span> <span style="color:#ae81ff">0.5</span> <span style="color:#f92672">and</span> gold[label] <span style="color:#f92672">&gt;=</span> <span style="color:#ae81ff">0.5</span>:
                fn <span style="color:#f92672">+=</span> <span style="color:#ae81ff">1</span>
    precision <span style="color:#f92672">=</span> tp <span style="color:#f92672">/</span> (tp <span style="color:#f92672">+</span> fp)
    recall <span style="color:#f92672">=</span> tp <span style="color:#f92672">/</span> (tp <span style="color:#f92672">+</span> fn)
    f_score <span style="color:#f92672">=</span> <span style="color:#ae81ff">2</span> <span style="color:#f92672">*</span> (precision <span style="color:#f92672">*</span> recall) <span style="color:#f92672">/</span> (precision <span style="color:#f92672">+</span> recall)
    <span style="color:#66d9ef">return</span> {<span style="color:#e6db74">&#39;textcat_p&#39;</span>: precision, <span style="color:#e6db74">&#39;textcat_r&#39;</span>: recall, <span style="color:#e6db74">&#39;textcat_f&#39;</span>: f_score}


<span style="color:#66d9ef">if</span> __name__ <span style="color:#f92672">==</span> <span style="color:#e6db74">&#39;__main__&#39;</span>:
    plac<span style="color:#f92672">.</span>call(main)</code></pre></div>
<blockquote>
<p>Final methods similar to before in MAIN fucn; one is to load data, other to eval; return data appropriately shuffled and split, eval func calcu true neatves, TP, FN and FP to create confusion matrix</p>
</blockquote>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-python" data-lang="python">test_text <span style="color:#f92672">=</span> <span style="color:#e6db74">&#34;This movie disappointed me severely&#34;</span>
doc <span style="color:#f92672">=</span> nlp(test_text)
<span style="color:#66d9ef">print</span>(test_text, doc<span style="color:#f92672">.</span>cats)</code></pre></div>
<blockquote>
<p><code>doc.cats</code> gives result of classifcation, i.e. negative sentiments</p>

<p><strong>Such is final setp - test model on sample, also see one of main pros of spaCy for DL - fits seamlessly in PIPELINE, and classifaction or sentiment score ends up being antoehr attribute of document - quite differente to Keras, whose purpose is to EITHER generating text OR to output proba-Vectors (vector in vector out); Possible to leverage this info as part of text analysis pipeline BUT spaCy does training under hood and learns attributes to doc makes easy to include info as part of any text analysis PIPELINE</strong></p>
</blockquote>

<h2 id="ideas-for-project">Ideas for Project</h2>

<h4 id="reddit-sense2vec-spacy">Reddit Sense2Vec SpaCy</h4>

<ul>
<li><a href="https://github.com/explosion/sense2vec">Code</a> Semantic analysis modifiable in source data and <strong>Semantics</strong> and web app! Visualisation</li>
</ul>

<h4 id="twitter-mining">Twitter Mining</h4>

<ul>
<li><a href="http://thinknook.com/twitter-sentiment-analysis-training-corpus-dataset-2012-09-22">Label Dataset</a> and <a href="https://www.kaggle.com/c/si650winter11">UMichigan Kaggle</a> and <a href="http://help.sentiment140.com/for-students">Sentiment140 dataset</a></li>
</ul>

<h4 id="chatbot">Chatbot</h4>

<ul>
<li><a href="https://arxiv.org/pdf/1506.05869v1.pdf">A Neural Conversational Model - Vinyal and Lee</a></li>
<li>Production-grade API <strong>RASA NLU</strong> and <strong>ChatterBot</strong></li>
<li>RASA <a href="https://github.com/RASAHQ/rasa_nlu/blob/master/data/examples/rasa/demo-rasa.json">JSON-data Example</a>
&gt; adding more entites and intent, model laerns more context better decipher questions - one of backend is SpaCy and SKL
&gt;     1. UnderHood, Word2Vec for intent, spaCy clean up text, SLK build models - <a href="https://medium.com/rasa-blog/do-it-yourself-nlp-for-bot-developers-2e2da2817f3f">detail</a>
&gt;     2. One of which involves being able to write own parts of bot instead of API
&gt;     3. JSON entry/data to train RASA (see elsewhere)</li>
<li><a href="https://core.rasa.com">Front-end</a> and <a href="https://core.rasa.com/tutorial_basics.html">Tutorial</a></li>
<li>Non-AI but Learn-Concept at <a href="https://apps.worldwritable.com/tutorials/chatbot">Chatbot Fundamentals</a> and <a href="https://github.com/lizadaly/brobot">Brobot</a></li>
<li>Recall the Concept of chatbot

<ol>
<li>Take Input</li>
<li>Classify intent (question, statement, greeting)</li>
<li>If greeting -</li>
<li>If Question (query simialr questions from dataset, do sentence analysis, response e.e. based on Reddit/Food or Twitter conversion)</li>
<li>If statement/conversion (generative model)</li>
<li>Closure</li>
</ol></li>
</ul>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-python" data-lang="python"><span style="color:#f92672">import</span> spacy

<span style="color:#75715e"># init English model (download required for other langues)</span>
nlp <span style="color:#f92672">=</span> spacy<span style="color:#f92672">.</span>load(<span style="color:#e6db74">&#39;en&#39;</span>, tagger<span style="color:#f92672">=</span>False,
                parser<span style="color:#f92672">=</span>False, matcher<span style="color:#f92672">=</span>False)

doc <span style="color:#f92672">=</span> nlp(article_uber)

<span style="color:#66d9ef">for</span> entity <span style="color:#f92672">in</span> doc<span style="color:#f92672">.</span>ents:
    <span style="color:#66d9ef">print</span>(entity<span style="color:#f92672">.</span>label_, entity<span style="color:#f92672">.</span>text)</code></pre></div>
<pre><code>/Users/Ocean/anaconda3/lib/python3.6/importlib/_bootstrap.py:219: RuntimeWarning: numpy.dtype size changed, may indicate binary incompatibility. Expected 96, got 88
  return f(*args, **kwds)
/Users/Ocean/anaconda3/lib/python3.6/importlib/_bootstrap.py:219: RuntimeWarning: numpy.ufunc size changed, may indicate binary incompatibility. Expected 192, got 176
  return f(*args, **kwds)


ORG Uber
ORG Uber
ORG Apple
ORG Uber
PERSON Travis Kalanick
ORG Uber
PERSON Tim Cook
ORG Apple
ORG Uber
GPE drivers’
LOC Silicon Valley’s
ORG Yahoo
PERSON Marissa Mayer
MONEY $186m
</code></pre>

<h1 id="polyglot-ner-polygplot">Polyglot NER <code>polygplot</code></h1>

<ul>
<li>Vector word</li>
<li>Why? main is language&hellip;. over 130</li>
<li>e.g. transliteration</li>
<li>practice Spanish NER with polyglot</li>
<li>auto-detect once init &lsquo;langue&rsquo;</li>
</ul>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-python" data-lang="python"><span style="color:#f92672">from</span> polyglot.text <span style="color:#f92672">import</span> Text

txt <span style="color:#f92672">=</span> Text(article_uber)</code></pre></div>
<pre><code>---------------------------------------------------------------------------

ModuleNotFoundError                       Traceback (most recent call last)

&lt;ipython-input-154-47870b8e6243&gt; in &lt;module&gt;()
----&gt; 1 from polyglot.text import Text
      2 
      3 txt = Text(article_uber)


~/anaconda3/lib/python3.6/site-packages/polyglot/text.py in &lt;module&gt;()
      7 
      8 from polyglot.base import Sequence, TextFile, TextFiles
----&gt; 9 from polyglot.detect import Detector, Language
     10 from polyglot.decorators import cached_property
     11 from polyglot.downloader import Downloader


~/anaconda3/lib/python3.6/site-packages/polyglot/detect/__init__.py in &lt;module&gt;()
----&gt; 1 from .base import Detector, Language
      2 
      3 __all__ = ['Detector', 'Language']


~/anaconda3/lib/python3.6/site-packages/polyglot/detect/base.py in &lt;module&gt;()
      9 
     10 
---&gt; 11 from icu import Locale
     12 import pycld2 as cld2
     13 


ModuleNotFoundError: No module named 'icu'
</code></pre>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-python" data-lang="python"><span style="color:#75715e"># Create a new text object using Polyglot&#39;s Text class: txt</span>
txt <span style="color:#f92672">=</span> Text(article)

<span style="color:#75715e"># Print each of the entities found</span>
<span style="color:#66d9ef">for</span> ent <span style="color:#f92672">in</span> txt<span style="color:#f92672">.</span>entities:
    <span style="color:#66d9ef">print</span>(ent)
    
<span style="color:#75715e"># Print the type of ent</span>
<span style="color:#66d9ef">print</span>(type(ent))


<span style="color:#75715e"># Create the list of tuples: entities</span>
entities <span style="color:#f92672">=</span> [(ent<span style="color:#f92672">.</span>tag, <span style="color:#e6db74">&#39; &#39;</span><span style="color:#f92672">.</span>join(ent)) <span style="color:#66d9ef">for</span> ent <span style="color:#f92672">in</span> txt<span style="color:#f92672">.</span>entities]

<span style="color:#75715e"># Print entities</span>
<span style="color:#66d9ef">print</span>(entities)</code></pre></div>
<h3 id="spanish-ner">Spanish NER</h3>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-python" data-lang="python"><span style="color:#75715e"># Initialize the count variable: count</span>
count <span style="color:#f92672">=</span> <span style="color:#ae81ff">0</span>

<span style="color:#75715e"># Iterate over all the entities</span>
<span style="color:#66d9ef">for</span> ent <span style="color:#f92672">in</span> txt<span style="color:#f92672">.</span>entities:
    <span style="color:#75715e"># Check whether the entity contains &#39;Márquez&#39; or &#39;Gabo&#39;</span>
    <span style="color:#66d9ef">if</span> <span style="color:#e6db74">&#34;Márquez&#34;</span> <span style="color:#f92672">in</span> ent <span style="color:#f92672">or</span> <span style="color:#e6db74">&#34;Gabo&#34;</span> <span style="color:#f92672">in</span> ent:
        <span style="color:#75715e"># Increment count</span>
        count <span style="color:#f92672">+=</span> <span style="color:#ae81ff">1</span>

<span style="color:#75715e"># Print count</span>
<span style="color:#66d9ef">print</span>(count)

<span style="color:#75715e"># Calculate the percentage of entities that refer to &#34;Gabo&#34;: percentage</span>
percentage <span style="color:#f92672">=</span> count <span style="color:#f92672">/</span> len(txt<span style="color:#f92672">.</span>entities)
<span style="color:#66d9ef">print</span>(percentage)</code></pre></div>
<h1 id="sl-with-nlp">SL with NLP</h1>

<ul>
<li>Classification of <strong>fake news</strong></li>
<li>Use language au lieu de Features</li>
<li>Creating train data from text

<ol>
<li>BOW or tf-idf as <strong>feature</strong></li>
</ol></li>
</ul>

<h2 id="imdb-movie-example">IMDB Movie Example</h2>

<ul>
<li>Plot <strong>text data</strong></li>
<li>Type of Film <strong>MultiCAT</strong></li>

<li><p>Target: Predict genre by plot</p>

<h2 id="possible-features-in-text-classification">Possible Features in Text-Classification</h2></li>

<li><p>Frequency (BOW or tf-idf)</p></li>

<li><p>Topic (Named Entities)</p></li>

<li><p>Language</p></li>
</ul>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-python" data-lang="python">df_movie <span style="color:#f92672">=</span> pd<span style="color:#f92672">.</span>read_csv(<span style="color:#e6db74">&#39;Data_Folder/TxT/fake_or_real_news.csv&#39;</span>)
df_movie<span style="color:#f92672">.</span>head()
df_movie<span style="color:#f92672">.</span>label[:<span style="color:#ae81ff">5</span>]

<span style="color:#f92672">from</span> sklearn.feature_extraction.text <span style="color:#f92672">import</span> CountVectorizer
<span style="color:#f92672">from</span> sklearn.model_selection <span style="color:#f92672">import</span> train_test_split

y <span style="color:#f92672">=</span> df_movie<span style="color:#f92672">.</span>label

X_train, X_test, y_train, y_test <span style="color:#f92672">=</span> train_test_split(df_movie<span style="color:#f92672">.</span>text, y, test_size<span style="color:#f92672">=</span><span style="color:#ae81ff">0.33</span>, random_state<span style="color:#f92672">=</span><span style="color:#ae81ff">53</span>)

count_vectorizer <span style="color:#f92672">=</span> CountVectorizer(stop_words<span style="color:#f92672">=</span><span style="color:#e6db74">&#39;english&#39;</span>)

count_train <span style="color:#f92672">=</span> count_vectorizer<span style="color:#f92672">.</span>fit_transform(X_train)

count_test <span style="color:#f92672">=</span> count_vectorizer<span style="color:#f92672">.</span>transform(X_test)

<span style="color:#66d9ef">print</span>(count_vectorizer<span style="color:#f92672">.</span>get_feature_names()[:<span style="color:#ae81ff">10</span>])</code></pre></div>
<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }
    
    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>Unnamed: 0</th>
      <th>title</th>
      <th>text</th>
      <th>label</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>8476</td>
      <td>You Can Smell Hillary’s Fear</td>
      <td>Daniel Greenfield, a Shillman Journalism Fello...</td>
      <td>FAKE</td>
    </tr>
    <tr>
      <th>1</th>
      <td>10294</td>
      <td>Watch The Exact Moment Paul Ryan Committed Pol...</td>
      <td>Google Pinterest Digg Linkedin Reddit Stumbleu...</td>
      <td>FAKE</td>
    </tr>
    <tr>
      <th>2</th>
      <td>3608</td>
      <td>Kerry to go to Paris in gesture of sympathy</td>
      <td>U.S. Secretary of State John F. Kerry said Mon...</td>
      <td>REAL</td>
    </tr>
    <tr>
      <th>3</th>
      <td>10142</td>
      <td>Bernie supporters on Twitter erupt in anger ag...</td>
      <td>— Kaydee King (@KaydeeKing) November 9, 2016 T...</td>
      <td>FAKE</td>
    </tr>
    <tr>
      <th>4</th>
      <td>875</td>
      <td>The Battle of New York: Why This Primary Matters</td>
      <td>It's primary day in New York and front-runners...</td>
      <td>REAL</td>
    </tr>
  </tbody>
</table>
</div>

<pre><code>0    FAKE
1    FAKE
2    REAL
3    FAKE
4    REAL
Name: label, dtype: object



['00', '000', '0000', '00000031', '000035', '00006', '0001', '0001pt', '000ft', '000km']
</code></pre>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-python" data-lang="python"><span style="color:#75715e"># similar to sparse CountVectorizer, create tf-idf vectors</span>

<span style="color:#f92672">from</span> sklearn.feature_extraction.text <span style="color:#f92672">import</span> TfidfVectorizer

tfidf_vectorizer <span style="color:#f92672">=</span> TfidfVectorizer(stop_words<span style="color:#f92672">=</span><span style="color:#e6db74">&#39;english&#39;</span>, max_df<span style="color:#f92672">=</span><span style="color:#ae81ff">0.7</span>)

tfidf_train <span style="color:#f92672">=</span> tfidf_vectorizer<span style="color:#f92672">.</span>fit_transform(X_train)

tfidf_test <span style="color:#f92672">=</span> tfidf_vectorizer<span style="color:#f92672">.</span>transform(X_test)

<span style="color:#66d9ef">print</span>(tfidf_vectorizer<span style="color:#f92672">.</span>get_feature_names()[:<span style="color:#ae81ff">10</span>])

<span style="color:#66d9ef">print</span>(tfidf_train<span style="color:#f92672">.</span>A[:<span style="color:#ae81ff">5</span>])</code></pre></div>
<pre><code>['00', '000', '0000', '00000031', '000035', '00006', '0001', '0001pt', '000ft', '000km']
[[0. 0. 0. ... 0. 0. 0.]
 [0. 0. 0. ... 0. 0. 0.]
 [0. 0. 0. ... 0. 0. 0.]
 [0. 0. 0. ... 0. 0. 0.]
 [0. 0. 0. ... 0. 0. 0.]]
</code></pre>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-python" data-lang="python"><span style="color:#75715e"># some inspection</span>

count_df <span style="color:#f92672">=</span> pd<span style="color:#f92672">.</span>DataFrame(count_train<span style="color:#f92672">.</span>A, columns<span style="color:#f92672">=</span>count_vectorizer<span style="color:#f92672">.</span>get_feature_names())
tfidf_df <span style="color:#f92672">=</span> pd<span style="color:#f92672">.</span>DataFrame(tfidf_train<span style="color:#f92672">.</span>A, columns<span style="color:#f92672">=</span>tfidf_vectorizer<span style="color:#f92672">.</span>get_feature_names())

<span style="color:#66d9ef">print</span>(count_df<span style="color:#f92672">.</span>head(), <span style="color:#e6db74">&#39;</span><span style="color:#ae81ff">\n</span><span style="color:#e6db74">&#39;</span>, tfidf_df<span style="color:#f92672">.</span>head(), <span style="color:#e6db74">&#39;</span><span style="color:#ae81ff">\n</span><span style="color:#e6db74">&#39;</span>)

difference <span style="color:#f92672">=</span> set(count_df<span style="color:#f92672">.</span>columns) <span style="color:#f92672">-</span> set(tfidf_df<span style="color:#f92672">.</span>columns)
<span style="color:#66d9ef">print</span>(difference, <span style="color:#e6db74">&#39;</span><span style="color:#ae81ff">\n</span><span style="color:#e6db74">&#39;</span>)

<span style="color:#66d9ef">print</span>(count_df<span style="color:#f92672">.</span>equals(tfidf_df))</code></pre></div>
<pre><code>   00  000  0000  00000031  000035  00006  0001  0001pt  000ft  000km  ...    \
0   0    0     0         0       0      0     0       0      0      0  ...     
1   0    0     0         0       0      0     0       0      0      0  ...     
2   0    0     0         0       0      0     0       0      0      0  ...     
3   0    0     0         0       0      0     0       0      0      0  ...     
4   0    0     0         0       0      0     0       0      0      0  ...     

   حلب  عربي  عن  لم  ما  محاولات  من  هذا  والمرضى  ยงade  
0    0     0   0   0   0        0   0    0        0      0  
1    0     0   0   0   0        0   0    0        0      0  
2    0     0   0   0   0        0   0    0        0      0  
3    0     0   0   0   0        0   0    0        0      0  
4    0     0   0   0   0        0   0    0        0      0  

[5 rows x 56922 columns] 
     00  000  0000  00000031  000035  00006  0001  0001pt  000ft  000km  ...    \
0  0.0  0.0   0.0       0.0     0.0    0.0   0.0     0.0    0.0    0.0  ...     
1  0.0  0.0   0.0       0.0     0.0    0.0   0.0     0.0    0.0    0.0  ...     
2  0.0  0.0   0.0       0.0     0.0    0.0   0.0     0.0    0.0    0.0  ...     
3  0.0  0.0   0.0       0.0     0.0    0.0   0.0     0.0    0.0    0.0  ...     
4  0.0  0.0   0.0       0.0     0.0    0.0   0.0     0.0    0.0    0.0  ...     

   حلب  عربي   عن   لم   ما  محاولات   من  هذا  والمرضى  ยงade  
0  0.0   0.0  0.0  0.0  0.0      0.0  0.0  0.0      0.0    0.0  
1  0.0   0.0  0.0  0.0  0.0      0.0  0.0  0.0      0.0    0.0  
2  0.0   0.0  0.0  0.0  0.0      0.0  0.0  0.0      0.0    0.0  
3  0.0   0.0  0.0  0.0  0.0      0.0  0.0  0.0      0.0    0.0  
4  0.0   0.0  0.0  0.0  0.0      0.0  0.0  0.0      0.0    0.0  

[5 rows x 56922 columns] 

set() 

False
</code></pre>

<h1 id="testing-using-naive-bayes-classifier">Testing using Naive Bayes Classifier</h1>

<ul>
<li>NB Model commonly used for testing NLP classificaiton problems</li>
<li>basis in probability</li>
<li>likelihood estimation</li>
<li>Conditional Probability of token</li>
<li>Not working well with float like tf-idf</li>
</ul>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-python" data-lang="python"><span style="color:#f92672">from</span> sklearn.naive_bayes <span style="color:#f92672">import</span> MultinomialNB
<span style="color:#f92672">from</span> sklearn <span style="color:#f92672">import</span> metrics

<span style="color:#75715e"># Instantiate a Multinomial Naive Bayes classifier: nb_classifier</span>
nb_classifier <span style="color:#f92672">=</span> MultinomialNB()

nb_classifier<span style="color:#f92672">.</span>fit(count_train, y_train)

pred <span style="color:#f92672">=</span> nb_classifier<span style="color:#f92672">.</span>predict(count_test)

score <span style="color:#f92672">=</span> metrics<span style="color:#f92672">.</span>accuracy_score(y_test, pred)
<span style="color:#66d9ef">print</span>(score)

cm <span style="color:#f92672">=</span> metrics<span style="color:#f92672">.</span>confusion_matrix(y_test, pred, labels<span style="color:#f92672">=</span>[<span style="color:#e6db74">&#39;FAKE&#39;</span>, <span style="color:#e6db74">&#39;REAL&#39;</span>])
<span style="color:#66d9ef">print</span>(cm)</code></pre></div>
<pre><code>MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True)



0.893352462936394
[[ 865  143]
 [  80 1003]]
</code></pre>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-python" data-lang="python"><span style="color:#75715e"># NB on tf-idf</span>
nb_classifier_tfidf <span style="color:#f92672">=</span> MultinomialNB()

nb_classifier_tfidf<span style="color:#f92672">.</span>fit(tfidf_train, y_train)

pred_tfidf <span style="color:#f92672">=</span> nb_classifier<span style="color:#f92672">.</span>predict(tfidf_test)

score_tfidf <span style="color:#f92672">=</span> metrics<span style="color:#f92672">.</span>accuracy_score(y_test, pred)
<span style="color:#66d9ef">print</span>(score_tfidf)

cm_tfidf <span style="color:#f92672">=</span> metrics<span style="color:#f92672">.</span>confusion_matrix(y_test, pred, labels<span style="color:#f92672">=</span>[<span style="color:#e6db74">&#39;FAKE&#39;</span>, <span style="color:#e6db74">&#39;REAL&#39;</span>])
<span style="color:#66d9ef">print</span>(cm_tfidf)</code></pre></div>
<pre><code>MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True)



0.893352462936394
[[ 865  143]
 [  80 1003]]
</code></pre>

<h1 id="simple-nlp-complex-problems">Simple NLP, Complex Problems</h1>

<ul>
<li>Translation, grammar in languages</li>
<li>Word complexity in other languages</li>
<li>Sentiment shift</li>
<li>Semantics</li>
<li>Custom in gender, meaning, etc</li>
</ul>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-python" data-lang="python">alphas <span style="color:#f92672">=</span> np<span style="color:#f92672">.</span>arange(<span style="color:#ae81ff">0</span>, <span style="color:#ae81ff">1</span>, <span style="color:#f92672">.</span><span style="color:#ae81ff">1</span>)

<span style="color:#66d9ef">def</span> <span style="color:#a6e22e">train_and_predict</span>(alpha):

    nb_classifier <span style="color:#f92672">=</span> MultinomialNB(alpha<span style="color:#f92672">=</span>alpha)

    nb_classifier<span style="color:#f92672">.</span>fit(tfidf_train, y_train)

    pred <span style="color:#f92672">=</span> nb_classifier<span style="color:#f92672">.</span>predict(tfidf_test)

    score <span style="color:#f92672">=</span> metrics<span style="color:#f92672">.</span>accuracy_score(y_test, pred)
    <span style="color:#66d9ef">return</span> score


<span style="color:#66d9ef">for</span> alpha <span style="color:#f92672">in</span> alphas:
    <span style="color:#66d9ef">print</span>(<span style="color:#e6db74">&#39;Alpha: &#39;</span>, alpha)
    <span style="color:#66d9ef">print</span>(<span style="color:#e6db74">&#39;Score: &#39;</span>, train_and_predict(alpha))
    <span style="color:#66d9ef">print</span>()
    </code></pre></div>
<pre><code>Alpha:  0.0
Score:  0.8813964610234337

Alpha:  0.1
Score:  0.8976566236250598

Alpha:  0.2
Score:  0.8938307030129125

Alpha:  0.30000000000000004
Score:  0.8900047824007652

Alpha:  0.4


/Users/Ocean/anaconda3/lib/python3.6/site-packages/sklearn/naive_bayes.py:472: UserWarning: alpha too small will result in numeric errors, setting alpha = 1.0e-10
  'setting alpha = %.1e' % _ALPHA_MIN)


Score:  0.8857006217120995

Alpha:  0.5
Score:  0.8842659014825442

Alpha:  0.6000000000000001
Score:  0.874701099952176

Alpha:  0.7000000000000001
Score:  0.8703969392635102

Alpha:  0.8
Score:  0.8660927785748446

Alpha:  0.9
Score:  0.8589191774270684
</code></pre>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-python" data-lang="python">class_labels <span style="color:#f92672">=</span> nb_classifier<span style="color:#f92672">.</span>classes_

feature_names <span style="color:#f92672">=</span> tfidf_vectorizer<span style="color:#f92672">.</span>get_feature_names()

<span style="color:#75715e"># Zip the feature names together with the coefficient array and sort by weights: feat_with_weights</span>
feat_with_weights <span style="color:#f92672">=</span> sorted(zip(nb_classifier<span style="color:#f92672">.</span>coef_[<span style="color:#ae81ff">0</span>], feature_names))

<span style="color:#75715e"># Print the first class label and the top 20 feat_with_weights entries</span>
<span style="color:#66d9ef">print</span>(class_labels[<span style="color:#ae81ff">0</span>], feat_with_weights[:<span style="color:#ae81ff">20</span>], <span style="color:#e6db74">&#39;</span><span style="color:#ae81ff">\n</span><span style="color:#e6db74">&#39;</span>)

<span style="color:#75715e"># Print the second class label and the bottom 20 feat_with_weights entries</span>
<span style="color:#66d9ef">print</span>(class_labels[<span style="color:#ae81ff">1</span>], feat_with_weights[<span style="color:#f92672">-</span><span style="color:#ae81ff">20</span>:])</code></pre></div>
<pre><code>FAKE [(-13.817639290604365, '0000'), (-13.817639290604365, '000035'), (-13.817639290604365, '0001'), (-13.817639290604365, '0001pt'), (-13.817639290604365, '000km'), (-13.817639290604365, '0011'), (-13.817639290604365, '006s'), (-13.817639290604365, '007'), (-13.817639290604365, '007s'), (-13.817639290604365, '008s'), (-13.817639290604365, '0099'), (-13.817639290604365, '00am'), (-13.817639290604365, '00p'), (-13.817639290604365, '00pm'), (-13.817639290604365, '014'), (-13.817639290604365, '015'), (-13.817639290604365, '018'), (-13.817639290604365, '01am'), (-13.817639290604365, '020'), (-13.817639290604365, '023')] 

REAL [(-6.172241591175732, 'republicans'), (-6.126896127062493, 'percent'), (-6.115534950553315, 'political'), (-6.067024557833956, 'house'), (-5.9903983888515535, 'like'), (-5.986816295469049, 'just'), (-5.97418288622825, 'time'), (-5.964034477506528, 'states'), (-5.949002396420198, 'sanders'), (-5.844483857160232, 'party'), (-5.728156816243612, 'republican'), (-5.63452121120962, 'campaign'), (-5.5727798946931095, 'new'), (-5.515621480853161, 'state'), (-5.511414074572205, 'obama'), (-5.482207812723569, 'president'), (-5.455931002028523, 'people'), (-4.98170150128453, 'clinton'), (-4.5936919152219655, 'trump'), (-4.477148234163137, 'said')]
</code></pre>

<h1 id="chatbot-1">ChatBot</h1>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-python" data-lang="python">bot_template <span style="color:#f92672">=</span> <span style="color:#e6db74">&#34;BOT : {0}&#34;</span>
user_template <span style="color:#f92672">=</span> <span style="color:#e6db74">&#34;USER : {0}&#34;</span>

<span style="color:#75715e"># Define a function that responds to a user&#39;s message: respond</span>
<span style="color:#66d9ef">def</span> <span style="color:#a6e22e">respond</span>(message):
    <span style="color:#75715e"># Concatenate the user&#39;s message to the end of a standard bot respone</span>
    bot_message <span style="color:#f92672">=</span> <span style="color:#e6db74">&#34;I can hear you! You said: &#34;</span> <span style="color:#f92672">+</span> message
    <span style="color:#75715e"># Return the result</span>
    <span style="color:#66d9ef">return</span> message

<span style="color:#f92672">import</span> time

<span style="color:#75715e"># Define a function that sends a message to the bot: send_message</span>
<span style="color:#66d9ef">def</span> <span style="color:#a6e22e">send_message</span>(message):
    <span style="color:#75715e"># Print user_template including the user_message</span>
    <span style="color:#66d9ef">print</span>(user_template<span style="color:#f92672">.</span>format(message))
    <span style="color:#75715e"># Get the bot&#39;s response to the message</span>
    response <span style="color:#f92672">=</span> respond(message)

    time<span style="color:#f92672">.</span>sleep(<span style="color:#ae81ff">1.5</span>) <span style="color:#75715e"># artificial delay minicking natural</span>
    <span style="color:#66d9ef">print</span>(bot_template<span style="color:#f92672">.</span>format(response))</code></pre></div><div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-python" data-lang="python"><span style="color:#75715e"># Send a message to the bot</span>
send_message(<span style="color:#e6db74">&#34;hello&#34;</span>)
send_message(<span style="color:#e6db74">&#34;What did I say?&#34;</span>)</code></pre></div>
<pre><code>USER : hello
BOT : hello
USER : What did I say?
BOT : What did I say?
</code></pre>

<h2 id="personification">Personification</h2>

<ul>
<li>chatbot not command line</li>
<li>fun and use</li>
<li>python module Smaltalk</li>
<li>Simple: dict{key:response} , exact match-only</li>
<li>Variable: response = dict{options}</li>
<li>Asking Questions: input</li>
</ul>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-python" data-lang="python"><span style="color:#f92672">import</span> random

name <span style="color:#f92672">=</span> <span style="color:#e6db74">&#34;Greg&#34;</span>
weather <span style="color:#f92672">=</span> <span style="color:#e6db74">&#34;cloudy&#34;</span>

<span style="color:#75715e"># Define a dictionary containing a list of responses for each message</span>
responses <span style="color:#f92672">=</span> {
  <span style="color:#e6db74">&#34;what&#39;s your name?&#34;</span>: [
      <span style="color:#e6db74">&#34;my name is {0}&#34;</span><span style="color:#f92672">.</span>format(name),
      <span style="color:#e6db74">&#34;they call me {0}&#34;</span><span style="color:#f92672">.</span>format(name),
      <span style="color:#e6db74">&#34;I go by {0}&#34;</span><span style="color:#f92672">.</span>format(name)
   ],
  <span style="color:#e6db74">&#34;what&#39;s today&#39;s weather?&#34;</span>: [
      <span style="color:#e6db74">&#34;the weather is {0}&#34;</span><span style="color:#f92672">.</span>format(weather),
      <span style="color:#e6db74">&#34;it&#39;s {0} today&#34;</span><span style="color:#f92672">.</span>format(weather)
    ],
  <span style="color:#e6db74">&#34;default&#34;</span>: [<span style="color:#e6db74">&#34;default message&#34;</span>]
}

<span style="color:#75715e"># Use random.choice() to choose a matching response</span>
<span style="color:#66d9ef">def</span> <span style="color:#a6e22e">respond</span>(message):
    <span style="color:#75715e"># Check if the message is in the responses</span>
    <span style="color:#66d9ef">if</span> message <span style="color:#f92672">in</span> responses:
        <span style="color:#75715e"># Return a random matching response</span>
        bot_message <span style="color:#f92672">=</span> random<span style="color:#f92672">.</span>choice(responses[message])
    <span style="color:#66d9ef">else</span>:
        <span style="color:#75715e"># Return a random &#34;default&#34; response</span>
        bot_message <span style="color:#f92672">=</span> random<span style="color:#f92672">.</span>choice(responses[<span style="color:#e6db74">&#34;default&#34;</span>])
    <span style="color:#66d9ef">return</span> bot_message</code></pre></div><div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-python" data-lang="python"><span style="color:#f92672">import</span> random

responses_question <span style="color:#f92672">=</span> {<span style="color:#e6db74">&#39;question&#39;</span>: [<span style="color:#e6db74">&#34;I don&#39;t know :(&#34;</span>, 
                                   <span style="color:#e6db74">&#39;you tell me!&#39;</span>],
                      <span style="color:#e6db74">&#39;statement&#39;</span>: 
                      [<span style="color:#e6db74">&#39;tell me more!&#39;</span>, <span style="color:#e6db74">&#39;why do you think that?&#39;</span>,
                       <span style="color:#e6db74">&#39;how long have you felt this way?&#39;</span>,
                       <span style="color:#e6db74">&#39;I find that extremely interesting&#39;</span>,
                       <span style="color:#e6db74">&#39;can you back that up?&#39;</span>,<span style="color:#e6db74">&#39;oh wow!&#39;</span>,
                       <span style="color:#e6db74">&#39;:)&#39;</span>]}
<span style="color:#66d9ef">def</span> <span style="color:#a6e22e">respond</span>(message):
    <span style="color:#75715e"># Check for a question mark</span>
    <span style="color:#66d9ef">if</span> message<span style="color:#f92672">.</span>endswith(<span style="color:#e6db74">&#34;?&#34;</span>):
        <span style="color:#75715e"># Return a random question</span>
        <span style="color:#66d9ef">return</span> random<span style="color:#f92672">.</span>choice(responses_question[<span style="color:#e6db74">&#34;question&#34;</span>])
    <span style="color:#75715e"># Return a random statement</span>
    <span style="color:#66d9ef">return</span> random<span style="color:#f92672">.</span>choice(responses_question[<span style="color:#e6db74">&#34;statement&#34;</span>])


<span style="color:#75715e"># Send messages ending in a question mark</span>
send_message(<span style="color:#e6db74">&#34;what&#39;s today&#39;s weather?&#34;</span>)
send_message(<span style="color:#e6db74">&#34;what&#39;s today&#39;s weather?&#34;</span>)

<span style="color:#75715e"># Send messages which don&#39;t end with a question mark</span>
send_message(<span style="color:#e6db74">&#34;I love building chatbots&#34;</span>)
send_message(<span style="color:#e6db74">&#34;I love building chatbots&#34;</span>)</code></pre></div>
<pre><code>USER : what's today's weather?
BOT : I don't know :(
USER : what's today's weather?
BOT : I don't know :(
USER : I love building chatbots
BOT : I find that extremely interesting
USER : I love building chatbots
BOT : tell me more!
</code></pre>

<h3 id="using-regex-to-match-pattern-and-respond">Using Regex to Match Pattern and Respond</h3>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-python" data-lang="python"><span style="color:#f92672">import</span> re

rules <span style="color:#f92672">=</span> {<span style="color:#e6db74">&#39;I want (.*)&#39;</span>: 
             [<span style="color:#e6db74">&#39;What would it mean if you got {0}&#39;</span>,
              <span style="color:#e6db74">&#39;Why do you want {0}&#39;</span>,
              <span style="color:#e6db74">&#34;What&#39;s stopping you from getting {0}&#34;</span>],
         <span style="color:#e6db74">&#39;do you remember (.*)&#39;</span>: 
             [<span style="color:#e6db74">&#39;Did you think I would forget {0}&#39;</span>,
              <span style="color:#e6db74">&#34;Why haven&#39;t you been able to forget {0}&#34;</span>,
              <span style="color:#e6db74">&#39;What about {0}&#39;</span>,
              <span style="color:#e6db74">&#39;Yes .. and?&#39;</span>],
         <span style="color:#e6db74">&#39;do you think (.*)&#39;</span>: 
             [<span style="color:#e6db74">&#39;if {0}? Absolutely.&#39;</span>, <span style="color:#e6db74">&#39;No chance&#39;</span>],
         <span style="color:#e6db74">&#39;if (.*)&#39;</span>: 
             [<span style="color:#e6db74">&#34;Do you really think it&#39;s likely that {0}&#34;</span>,
              <span style="color:#e6db74">&#39;Do you wish that {0}&#39;</span>,
              <span style="color:#e6db74">&#39;What do you think about {0}&#39;</span>,
              <span style="color:#e6db74">&#39;Really--if {0}&#39;</span>]}

<span style="color:#66d9ef">def</span> <span style="color:#a6e22e">match_rule</span>(rules, message):
    response, phrase <span style="color:#f92672">=</span> <span style="color:#e6db74">&#34;default&#34;</span>, None
    
    <span style="color:#66d9ef">for</span> pattern, responses <span style="color:#f92672">in</span> rules<span style="color:#f92672">.</span>items():
        match <span style="color:#f92672">=</span> re<span style="color:#f92672">.</span>search(pattern, message)
        <span style="color:#66d9ef">if</span> match <span style="color:#f92672">is</span> <span style="color:#f92672">not</span> None:
            response <span style="color:#f92672">=</span> random<span style="color:#f92672">.</span>choice(responses)
            <span style="color:#66d9ef">if</span> <span style="color:#e6db74">&#39;{0}&#39;</span> <span style="color:#f92672">in</span> response:
                phrase <span style="color:#f92672">=</span> match<span style="color:#f92672">.</span>group(<span style="color:#ae81ff">1</span>)
    <span style="color:#66d9ef">return</span> response, phrase</code></pre></div><div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-python" data-lang="python"><span style="color:#66d9ef">print</span>(match_rule(rules, <span style="color:#e6db74">&#34;nice&#34;</span>))</code></pre></div>
<pre><code>('default', None)
</code></pre>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-python" data-lang="python"><span style="color:#75715e"># Grammar, Pronouns change</span>

<span style="color:#66d9ef">def</span> <span style="color:#a6e22e">replace_pronouns</span>(message):
    
    message <span style="color:#f92672">=</span> message<span style="color:#f92672">.</span>lower()
    <span style="color:#66d9ef">if</span> <span style="color:#e6db74">&#39;me&#39;</span> <span style="color:#f92672">in</span> message:
        <span style="color:#66d9ef">return</span> re<span style="color:#f92672">.</span>sub(<span style="color:#e6db74">&#39;me&#39;</span>, <span style="color:#e6db74">&#39;you&#39;</span>, message)
    <span style="color:#66d9ef">if</span> <span style="color:#e6db74">&#39;my&#39;</span> <span style="color:#f92672">in</span> message:
        <span style="color:#66d9ef">return</span> re<span style="color:#f92672">.</span>sub(<span style="color:#e6db74">&#39;my&#39;</span>, <span style="color:#e6db74">&#39;your&#39;</span>, message)
    <span style="color:#66d9ef">if</span> <span style="color:#e6db74">&#39;your&#39;</span> <span style="color:#f92672">in</span> message:
        <span style="color:#66d9ef">return</span> re<span style="color:#f92672">.</span>sub(<span style="color:#e6db74">&#39;your&#39;</span>, <span style="color:#e6db74">&#39;my&#39;</span>, message)
    <span style="color:#66d9ef">if</span> <span style="color:#e6db74">&#39;you&#39;</span> <span style="color:#f92672">in</span> message:
        <span style="color:#66d9ef">return</span> re<span style="color:#f92672">.</span>sub(<span style="color:#e6db74">&#39;you&#39;</span>, <span style="color:#e6db74">&#39;me&#39;</span>, message)
    
    <span style="color:#66d9ef">return</span> message</code></pre></div><div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-python" data-lang="python"><span style="color:#66d9ef">print</span>(replace_pronouns(<span style="color:#e6db74">&#34;my last birthday&#34;</span>))
<span style="color:#66d9ef">print</span>(replace_pronouns(<span style="color:#e6db74">&#34;when you went to Florida&#34;</span>))
<span style="color:#66d9ef">print</span>(replace_pronouns(<span style="color:#e6db74">&#34;I had my own castle&#34;</span>))</code></pre></div>
<pre><code>your last birthday
when me went to florida
i had your own castle
</code></pre>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-python" data-lang="python"><span style="color:#75715e"># Combining previous two functions</span>

<span style="color:#66d9ef">def</span> <span style="color:#a6e22e">respond</span>(message):

    response, phrase <span style="color:#f92672">=</span> match_rule(rules, message)
    <span style="color:#66d9ef">if</span> <span style="color:#e6db74">&#39;{0}&#39;</span> <span style="color:#f92672">in</span> response:

        phrase <span style="color:#f92672">=</span> replace_pronouns(phrase)
        <span style="color:#75715e"># Include the phrase in the response</span>
        response <span style="color:#f92672">=</span> response<span style="color:#f92672">.</span>format(phrase)
    <span style="color:#66d9ef">return</span> response


send_message(<span style="color:#e6db74">&#34;do you remember your last birthday&#34;</span>)
send_message(<span style="color:#e6db74">&#34;do you think humans should be worried about AI&#34;</span>)
send_message(<span style="color:#e6db74">&#34;I want a robot friend&#34;</span>)
send_message(<span style="color:#e6db74">&#34;what if you could be anything you wanted&#34;</span>)</code></pre></div>
<pre><code>USER : do you remember your last birthday
BOT : What about my last birthday
USER : do you think humans should be worried about AI
BOT : if humans should be worried about ai? Absolutely.
USER : I want a robot friend
BOT : What's stopping you from getting a robot friend
USER : what if you could be anything you wanted
BOT : Really--if me could be anything me wanted
</code></pre>

<h1 id="nlu-sub-nlp">NLU - Sub-NLP</h1>

<ul>
<li>Logic Loop

<ol>
<li>&ldquo;I&rsquo;m look for a Mexico restaurant in the center of town&rdquo;</li>
<li>NLU model</li>
<li><strong>intent</strong> - restaurant_search</li>
<li><strong>entities</strong> - cuisine: Mexican, area: center</li>
<li>Database</li>
<li>&ldquo;Sure! what about Pepe&rsquo;s Buritos on Main St?&rdquo;</li>
</ol></li>
</ul>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-python" data-lang="python">keywords <span style="color:#f92672">=</span> {<span style="color:#e6db74">&#39;goodbye&#39;</span>: [<span style="color:#e6db74">&#39;bye&#39;</span>, <span style="color:#e6db74">&#39;farewell&#39;</span>],
 <span style="color:#e6db74">&#39;greet&#39;</span>: [<span style="color:#e6db74">&#39;hello&#39;</span>, <span style="color:#e6db74">&#39;hi&#39;</span>, <span style="color:#e6db74">&#39;hey&#39;</span>],
 <span style="color:#e6db74">&#39;thankyou&#39;</span>: [<span style="color:#e6db74">&#39;thank&#39;</span>, <span style="color:#e6db74">&#39;thx&#39;</span>]}

patterns <span style="color:#f92672">=</span> {}

<span style="color:#66d9ef">for</span> intent, keys <span style="color:#f92672">in</span> keywords<span style="color:#f92672">.</span>items():
    patterns[intent] <span style="color:#f92672">=</span> re<span style="color:#f92672">.</span>compile(<span style="color:#e6db74">&#39;|&#39;</span><span style="color:#f92672">.</span>join(keys))
    
<span style="color:#66d9ef">print</span>(patterns)</code></pre></div>
<pre><code>{'goodbye': re.compile('bye|farewell'), 'greet': re.compile('hello|hi|hey'), 'thankyou': re.compile('thank|thx')}
</code></pre>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-python" data-lang="python"><span style="color:#75715e"># Define a function to find the intent of a message</span>
<span style="color:#66d9ef">def</span> <span style="color:#a6e22e">match_intent</span>(message):
    matched_intent <span style="color:#f92672">=</span> None
    
    <span style="color:#66d9ef">for</span> intent, pattern <span style="color:#f92672">in</span> patterns<span style="color:#f92672">.</span>items():
        <span style="color:#66d9ef">if</span> pattern<span style="color:#f92672">.</span>search(message):
            matched_intent <span style="color:#f92672">=</span> intent
    <span style="color:#66d9ef">return</span> matched_intent

responses_intent <span style="color:#f92672">=</span> {<span style="color:#e6db74">&#39;default&#39;</span>: <span style="color:#e6db74">&#39;default message&#39;</span>,
 <span style="color:#e6db74">&#39;goodbye&#39;</span>: <span style="color:#e6db74">&#39;goodbye for now&#39;</span>,
 <span style="color:#e6db74">&#39;greet&#39;</span>: <span style="color:#e6db74">&#39;Hello you! :)&#39;</span>,
 <span style="color:#e6db74">&#39;thankyou&#39;</span>: <span style="color:#e6db74">&#39;you are very welcome&#39;</span>}

<span style="color:#66d9ef">def</span> <span style="color:#a6e22e">respond</span>(message):

    intent <span style="color:#f92672">=</span> match_intent(message)
    key <span style="color:#f92672">=</span> <span style="color:#e6db74">&#34;default&#34;</span>
    <span style="color:#66d9ef">if</span> intent <span style="color:#f92672">in</span> responses_intent:
        key <span style="color:#f92672">=</span> intent
    <span style="color:#66d9ef">return</span> responses_intent[key]


send_message(<span style="color:#e6db74">&#34;hello!&#34;</span>)
send_message(<span style="color:#e6db74">&#34;bye byeee&#34;</span>)
send_message(<span style="color:#e6db74">&#34;thanks very much!&#34;</span>)</code></pre></div>
<pre><code>USER : hello!
BOT : Hello you! :)
USER : bye byeee
BOT : goodbye for now
USER : thanks very much!
BOT : you are very welcome
</code></pre>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-python" data-lang="python"><span style="color:#66d9ef">def</span> <span style="color:#a6e22e">find_name</span>(message):
    name <span style="color:#f92672">=</span> None
    <span style="color:#75715e"># Create a pattern for checking if the keywords occur</span>
    name_keyword <span style="color:#f92672">=</span> re<span style="color:#f92672">.</span>compile(<span style="color:#e6db74">&#34;name|call&#34;</span>)
    <span style="color:#75715e"># Create a pattern for finding capitalized words</span>
    name_pattern <span style="color:#f92672">=</span> re<span style="color:#f92672">.</span>compile(<span style="color:#e6db74">&#39;[A-Z]{1}[a-z]*&#39;</span>)
    <span style="color:#66d9ef">if</span> name_keyword<span style="color:#f92672">.</span>search(message):
        <span style="color:#75715e"># Get the matching words in the string</span>
        name_words <span style="color:#f92672">=</span> name_pattern<span style="color:#f92672">.</span>findall(message)
        <span style="color:#66d9ef">if</span> len(name_words) <span style="color:#f92672">&gt;</span> <span style="color:#ae81ff">0</span>:
            <span style="color:#75715e"># Return the name if the keywords are present</span>
            name <span style="color:#f92672">=</span> <span style="color:#e6db74">&#39; &#39;</span><span style="color:#f92672">.</span>join(name_words)
    <span style="color:#66d9ef">return</span> name


<span style="color:#66d9ef">def</span> <span style="color:#a6e22e">respond</span>(message):

    name <span style="color:#f92672">=</span> find_name(message)
    <span style="color:#66d9ef">if</span> name <span style="color:#f92672">is</span> None:
        <span style="color:#66d9ef">return</span> <span style="color:#e6db74">&#34;Hi there!&#34;</span>
    <span style="color:#66d9ef">else</span>:
        <span style="color:#66d9ef">return</span> <span style="color:#e6db74">&#34;Hello, {0}!&#34;</span><span style="color:#f92672">.</span>format(name)


send_message(<span style="color:#e6db74">&#34;my name is David Copperfield&#34;</span>)
send_message(<span style="color:#e6db74">&#34;call me Ishmael&#34;</span>)
send_message(<span style="color:#e6db74">&#34;People call me Cassandra&#34;</span>)</code></pre></div>
<pre><code>USER : my name is David Copperfield
BOT : Hello, David Copperfield!
USER : call me Ishmael
BOT : Hello, Ishmael!
USER : People call me Cassandra
BOT : Hello, People Cassandra!
</code></pre>

<h1 id="ml-in-chatbot">ML in Chatbot</h1>

<ul>
<li>Predict(Intent)</li>
<li>Many ways of Vectorisation of text</li>
<li>Here word-vector to repr meaning of similar context</li>

<li><p><strong>Open source of high quality word-vectors</strong></p>

<h2 id="spacy-module">SpaCy module</h2></li>

<li><p>word-vector hundreds of elements</p></li>

<li><p>Similarity measured by <strong>angle</strong> between vectors <strong>distance</strong>
<strong>Cosine Similarity</strong></p>

<ul>
<li>1 if same direction</li>
<li>0 if orthogonal</li>
<li>-1 if opposite</li>
</ul></li>
</ul>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-python" data-lang="python"><span style="color:#75715e"># dataset: flight booking system interaction from ATIS</span>

sentences_demo <span style="color:#f92672">=</span> [<span style="color:#e6db74">&#39; i want to fly from boston at 838 am and arrive in denver at 1110 in the morning&#39;</span>,
 <span style="color:#e6db74">&#39; what flights are available from pittsburgh to baltimore on thursday morning&#39;</span>,
 <span style="color:#e6db74">&#39; what is the arrival time in san francisco for the 755 am flight leaving washington&#39;</span>,
 <span style="color:#e6db74">&#39; cheapest airfare from tacoma to orlando&#39;</span>,
 <span style="color:#e6db74">&#39; round trip fares from pittsburgh to philadelphia under 1000 dollars&#39;</span>,
 <span style="color:#e6db74">&#39; i need a flight tomorrow from columbus to minneapolis&#39;</span>,
 <span style="color:#e6db74">&#39; what kind of aircraft is used on a flight from cleveland to dallas&#39;</span>,
 <span style="color:#e6db74">&#39; show me the flights from pittsburgh to los angeles on thursday&#39;</span>,
 <span style="color:#e6db74">&#39; all flights from boston to washington&#39;</span>,
 <span style="color:#e6db74">&#39; what kind of ground transportation is available in denver&#39;</span>,
 <span style="color:#e6db74">&#39; show me the flights from dallas to san francisco&#39;</span>,
 <span style="color:#e6db74">&#39; show me the flights from san diego to newark by way of houston&#39;</span>,
 <span style="color:#e6db74">&#39; what is the cheapest flight from boston to bwi&#39;</span>,
 <span style="color:#e6db74">&#39; all flights to baltimore after 6 pm&#39;</span>,
 <span style="color:#e6db74">&#39; show me the first class fares from boston to denver&#39;</span>,
 <span style="color:#e6db74">&#39; show me the ground transportation in denver&#39;</span>,
 <span style="color:#e6db74">&#39; all flights from denver to pittsburgh leaving after 6 pm and before 7 pm&#39;</span>,
 <span style="color:#e6db74">&#39; i need information on flights for tuesday leaving baltimore for dallas dallas to boston and boston to baltimore&#39;</span>,
 <span style="color:#e6db74">&#39; please give me the flights from boston to pittsburgh on thursday of next week&#39;</span>,
 <span style="color:#e6db74">&#39; i would like to fly from denver to pittsburgh on united airlines&#39;</span>,
 <span style="color:#e6db74">&#39; show me the flights from san diego to newark&#39;</span>,
 <span style="color:#e6db74">&#39; please list all first class flights on united from denver to baltimore&#39;</span>,
 <span style="color:#e6db74">&#39; what kinds of planes are used by american airlines&#39;</span>,
 <span style="color:#e6db74">&#34; i&#39;d like to have some information on a ticket from denver to pittsburgh and atlanta&#34;</span>,
 <span style="color:#e6db74">&#34; i&#39;d like to book a flight from atlanta to denver&#34;</span>,
 <span style="color:#e6db74">&#39; which airline serves denver pittsburgh and atlanta&#39;</span>,
 <span style="color:#e6db74">&#34; show me all flights from boston to pittsburgh on wednesday of next week which leave boston after 2 o&#39;clock pm&#34;</span>,
 <span style="color:#e6db74">&#39; atlanta ground transportation&#39;</span>,
 <span style="color:#e6db74">&#39; i also need service from dallas to boston arriving by noon&#39;</span>,
 <span style="color:#e6db74">&#39; show me the cheapest round trip fare from baltimore to dallas&#39;</span>]</code></pre></div><div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-python" data-lang="python"><span style="color:#f92672">import</span> spacy

nlp <span style="color:#f92672">=</span> spacy<span style="color:#f92672">.</span>load(<span style="color:#e6db74">&#39;en_core_web_lg&#39;</span>) <span style="color:#75715e"># 1G size of library</span>

n_sentences_demo <span style="color:#f92672">=</span> len(sentences_demo)

embedding_dim <span style="color:#f92672">=</span> nlp<span style="color:#f92672">.</span>vocab<span style="color:#f92672">.</span>vectors_length</code></pre></div><div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-python" data-lang="python">X <span style="color:#f92672">=</span> np<span style="color:#f92672">.</span>zeros((n_sentences_demo, embedding_dim))

<span style="color:#66d9ef">for</span> idx, sentence <span style="color:#f92672">in</span> enumerate(sentences_demo):
    doc <span style="color:#f92672">=</span> nlp(sentence) <span style="color:#75715e"># pass each sent to nlp obj</span>
    X[idx, :] <span style="color:#f92672">=</span> doc<span style="color:#f92672">.</span>vector <span style="color:#75715e"># pass .vector attr to row in X</span></code></pre></div>
<h2 id="from-word-vector-to-ml-intent-and-classification">From Word-Vector to ML Intent and Classification</h2>

<ul>
<li>&lsquo;msg&rsquo; to intent recognition</li>
<li>KNN is simple, Support Vector is better</li>
</ul>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-python" data-lang="python">df_intent_train <span style="color:#f92672">=</span> pd<span style="color:#f92672">.</span>read_csv(<span style="color:#e6db74">&#39;Data_Folder/atis/atis_intents_train.csv&#39;</span>, header<span style="color:#f92672">=</span>None)
df_intent_test <span style="color:#f92672">=</span> pd<span style="color:#f92672">.</span>read_csv(<span style="color:#e6db74">&#39;Data_Folder/atis/atis_intents_test.csv&#39;</span>, header<span style="color:#f92672">=</span>None)

intent_X_train, intent_y_train <span style="color:#f92672">=</span> df_intent_train[:][<span style="color:#ae81ff">1</span>], df_intent_train[:][<span style="color:#ae81ff">0</span>]
intent_X_test, intent_y_test <span style="color:#f92672">=</span> df_intent_test[:][<span style="color:#ae81ff">1</span>], df_intent_test[:][<span style="color:#ae81ff">0</span>]                                                             </code></pre></div><div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-python" data-lang="python">X_train <span style="color:#f92672">=</span> np<span style="color:#f92672">.</span>zeros((len(intent_X_train), nlp<span style="color:#f92672">.</span>vocab<span style="color:#f92672">.</span>vectors_length))

<span style="color:#66d9ef">for</span> idx, sent <span style="color:#f92672">in</span> enumerate(intent_X_train):
    X_train[idx,:] <span style="color:#f92672">=</span> nlp(sent)<span style="color:#f92672">.</span>vector

<span style="color:#75715e"># long time......</span></code></pre></div><div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-python" data-lang="python">X_test <span style="color:#f92672">=</span> np<span style="color:#f92672">.</span>zeros</code></pre></div><div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-python" data-lang="python"><span style="color:#75715e"># simple method using label as intent via cosine similarity in sent-vectors</span>

<span style="color:#f92672">from</span> sklearn.metrics.pairwise <span style="color:#f92672">import</span> cosine_similarity

X_test <span style="color:#f92672">=</span> nlp(intent_X_test[<span style="color:#ae81ff">0</span>][<span style="color:#ae81ff">1</span>])<span style="color:#f92672">.</span>vector

scores <span style="color:#f92672">=</span> [cosine_similarity(X_train[i,:]<span style="color:#f92672">.</span>reshape(<span style="color:#f92672">-</span><span style="color:#ae81ff">1</span>,<span style="color:#ae81ff">1</span>), X_test<span style="color:#f92672">.</span>reshape(<span style="color:#f92672">-</span><span style="color:#ae81ff">1</span>,<span style="color:#ae81ff">1</span>)) <span style="color:#66d9ef">for</span> i <span style="color:#f92672">in</span> range(len(intent_X_train))]

intent_y_train[np<span style="color:#f92672">.</span>argmax(scores)]</code></pre></div>
<pre><code>'atis_flight'
</code></pre>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-python" data-lang="python"><span style="color:#75715e"># using Full X_test vectors for SVM</span>

X_test <span style="color:#f92672">=</span> np<span style="color:#f92672">.</span>zeros((len(intent_X_test), nlp<span style="color:#f92672">.</span>vocab<span style="color:#f92672">.</span>vectors_length))

<span style="color:#66d9ef">for</span> idx, sent <span style="color:#f92672">in</span> enumerate(intent_X_test):
    X_test[idx,:] <span style="color:#f92672">=</span> nlp(sent)<span style="color:#f92672">.</span>vector</code></pre></div><div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-python" data-lang="python"><span style="color:#75715e"># SVC</span>

<span style="color:#f92672">from</span> sklearn.svm <span style="color:#f92672">import</span> SVC

clf <span style="color:#f92672">=</span> SVC(C<span style="color:#f92672">=</span><span style="color:#ae81ff">1</span>)

clf<span style="color:#f92672">.</span>fit(X_train, intent_y_train)

y_pred <span style="color:#f92672">=</span> clf<span style="color:#f92672">.</span>predict(X_test)

n_correct <span style="color:#f92672">=</span> <span style="color:#ae81ff">0</span>
<span style="color:#66d9ef">for</span> i <span style="color:#f92672">in</span> range(len(intent_y_test)):
    <span style="color:#66d9ef">if</span> y_pred[i] <span style="color:#f92672">==</span> intent_y_test[i]:
        n_correct <span style="color:#f92672">+=</span> <span style="color:#ae81ff">1</span>

<span style="color:#66d9ef">print</span>(<span style="color:#e6db74">&#34;Predicted {0} correctly out of {1} test examples&#34;</span><span style="color:#f92672">.</span>format(n_correct, len(intent_y_test)))</code></pre></div>
<pre><code>SVC(C=1, cache_size=200, class_weight=None, coef0=0.0,
  decision_function_shape='ovr', degree=3, gamma='auto', kernel='rbf',
  max_iter=-1, probability=False, random_state=None, shrinking=True,
  tol=0.001, verbose=False)



Predicted 689 correctly out of 800 test examples
</code></pre>

<h2 id="entity-extraction">Entity Extraction</h2>

<ul>
<li>Unseen ER is tricky</li>
<li>Generalisation - <strong>pattern</strong> and contextual cues:

<ol>
<li>spelling</li>
<li>capitalisation</li>
<li>sequence of word pairs</li>
</ol></li>
<li>Pre-built NER

<ol>
<li>places, dates, orgs, etc</li>
</ol></li>

<li><p>Roles</p>

<ul>
<li>xxx from x to x ; xxx to x from x</li>
<li><code>re.compile('.* from (.*) to (.*)')</code></li>

<li><p><code>re.compile('.* to (.*) from (.*)')</code></p>

<h3 id="dependency-parsing-is-complex-topic">Dependency Parsing is complex topic</h3></li>
</ul></li>

<li><p>spacy attr of token <code>token.ancestors</code> for parent token of word token</p></li>
</ul>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-python" data-lang="python"><span style="color:#75715e"># Define included entities</span>
include_entities <span style="color:#f92672">=</span> [<span style="color:#e6db74">&#39;DATE&#39;</span>, <span style="color:#e6db74">&#39;ORG&#39;</span>, <span style="color:#e6db74">&#39;PERSON&#39;</span>]

<span style="color:#75715e"># Define extract_entities()</span>
<span style="color:#66d9ef">def</span> <span style="color:#a6e22e">extract_entities</span>(message):
    
    ents <span style="color:#f92672">=</span> dict<span style="color:#f92672">.</span>fromkeys(include_entities) <span style="color:#75715e"># useful dict.fromkeys func</span>

    doc <span style="color:#f92672">=</span> nlp(message)
    <span style="color:#66d9ef">for</span> ent <span style="color:#f92672">in</span> doc<span style="color:#f92672">.</span>ents:
        <span style="color:#66d9ef">if</span> ent<span style="color:#f92672">.</span>label_ <span style="color:#f92672">in</span> include_entities:
            <span style="color:#75715e"># Save interesting entities</span>
            ents[ent<span style="color:#f92672">.</span>label_] <span style="color:#f92672">=</span> ent<span style="color:#f92672">.</span>text
    <span style="color:#66d9ef">return</span> ents

<span style="color:#66d9ef">print</span>(extract_entities(<span style="color:#e6db74">&#39;friends called Mary who have worked at Google since 2010&#39;</span>))
<span style="color:#66d9ef">print</span>(extract_entities(<span style="color:#e6db74">&#39;people who graduated from MIT in 1999&#39;</span>))</code></pre></div>
<pre><code>{'DATE': '2010', 'ORG': 'Google', 'PERSON': 'Mary'}
{'DATE': '1999', 'ORG': 'MIT', 'PERSON': None}
</code></pre>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-python" data-lang="python"><span style="color:#75715e"># SpaCy&#39;s powerful syntax parser to assign ROLES to entities in message</span>

doc <span style="color:#f92672">=</span> nlp(<span style="color:#e6db74">&#34;let&#39;s see that jacket in red and some blue jeans&#34;</span>)

colors <span style="color:#f92672">=</span> [<span style="color:#e6db74">&#39;black&#39;</span>, <span style="color:#e6db74">&#39;red&#39;</span>, <span style="color:#e6db74">&#39;blue&#39;</span>]
items <span style="color:#f92672">=</span> [<span style="color:#e6db74">&#39;shoes&#39;</span>, <span style="color:#e6db74">&#39;handback&#39;</span>, <span style="color:#e6db74">&#39;jacket&#39;</span>, <span style="color:#e6db74">&#39;jeans&#39;</span>]

<span style="color:#66d9ef">def</span> <span style="color:#a6e22e">entity_type</span>(word):
    _type <span style="color:#f92672">=</span> None
    <span style="color:#66d9ef">if</span> word<span style="color:#f92672">.</span>text <span style="color:#f92672">in</span> colors:
        _type <span style="color:#f92672">=</span> <span style="color:#e6db74">&#34;color&#34;</span>
    <span style="color:#66d9ef">elif</span> word<span style="color:#f92672">.</span>text <span style="color:#f92672">in</span> items:
        _type <span style="color:#f92672">=</span> <span style="color:#e6db74">&#34;item&#34;</span>
    <span style="color:#66d9ef">return</span> _type

<span style="color:#75715e"># Iterate over parents in parse tree until an item entity is found</span>
<span style="color:#66d9ef">def</span> <span style="color:#a6e22e">find_parent_item</span>(word):
    <span style="color:#75715e"># Iterate over the word&#39;s ancestors</span>
    <span style="color:#66d9ef">for</span> parent <span style="color:#f92672">in</span> word<span style="color:#f92672">.</span>ancestors:
        <span style="color:#75715e"># Check for an &#34;item&#34; entity</span>
        <span style="color:#66d9ef">if</span> entity_type(parent) <span style="color:#f92672">==</span> <span style="color:#e6db74">&#34;item&#34;</span>:
            <span style="color:#66d9ef">return</span> parent<span style="color:#f92672">.</span>text
    <span style="color:#66d9ef">return</span> None

<span style="color:#75715e"># For all color entities, find their parent item</span>
<span style="color:#66d9ef">def</span> <span style="color:#a6e22e">assign_colors</span>(doc):
    <span style="color:#75715e"># Iterate over the document</span>
    <span style="color:#66d9ef">for</span> word <span style="color:#f92672">in</span> doc:
        <span style="color:#75715e"># Check for &#34;color&#34; entities</span>
        <span style="color:#66d9ef">if</span> entity_type(word) <span style="color:#f92672">==</span> <span style="color:#e6db74">&#34;color&#34;</span>:
            <span style="color:#75715e"># Find the parent</span>
            item <span style="color:#f92672">=</span>  find_parent_item(word)
            <span style="color:#66d9ef">print</span>(<span style="color:#e6db74">&#34;item: {0} has color : {1}&#34;</span><span style="color:#f92672">.</span>format(item, word))

<span style="color:#75715e"># Assign the colors</span>
assign_colors(doc)</code></pre></div>
<pre><code>item: jacket has color : red
item: jeans has color : blue
</code></pre>

<h2 id="robust-nlu-with-rasa">Robust NLU with Rasa</h2>

<ul>
<li>high-level API for intent recogition &amp; entity extraction</li>
<li>Based on spaCy, scikit-learn, other lib</li>
<li>Built-in support for chatbot specific tasks</li>

<li><p>data <strong>json</strong> file</p>

<h3 id="special-predicting-typo-or-unseen-word">Special - predicting typo or unseen word</h3></li>

<li><p><code>'intent_featurizer_ngrams'</code> : predictive of ngram in sub-vectors</p></li>

<li><p>Ensure such config is required in context, i.e. enough data of such for learning</p></li>
</ul>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-python" data-lang="python"><span style="color:#75715e"># Import necessary modules</span>
<span style="color:#f92672">from</span> rasa_nlu.converters <span style="color:#f92672">import</span> load_data
<span style="color:#f92672">from</span> rasa_nlu.config <span style="color:#f92672">import</span> RasaNLUConfig
<span style="color:#f92672">from</span> rasa_nlu.model <span style="color:#f92672">import</span> Trainer

<span style="color:#75715e"># Create args dictionary</span>
args <span style="color:#f92672">=</span> {<span style="color:#e6db74">&#34;pipeline&#34;</span>: <span style="color:#e6db74">&#34;spacy_sklearn&#34;</span>}

<span style="color:#75715e"># Create a configuration and trainer</span>
config <span style="color:#f92672">=</span> RasaNLUConfig(cmdline_args<span style="color:#f92672">=</span>args)
trainer <span style="color:#f92672">=</span> Trainer(config)

<span style="color:#75715e"># Load the training data</span>
training_data <span style="color:#f92672">=</span> load_data(<span style="color:#e6db74">&#34;./training_data.json&#34;</span>)

<span style="color:#75715e"># Create an interpreter by training the model</span>
interpreter <span style="color:#f92672">=</span> trainer<span style="color:#f92672">.</span>train(training_data)

<span style="color:#75715e"># Try it out</span>
<span style="color:#66d9ef">print</span>(interpreter<span style="color:#f92672">.</span>parse(<span style="color:#e6db74">&#34;I&#39;m looking for a Mexican restaurant in the North of town&#34;</span>))</code></pre></div>
<h2 id="rasa">Rasa</h2>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-python" data-lang="python"><span style="color:#75715e"># Import necessary modules</span>
<span style="color:#f92672">from</span> rasa_nlu.config <span style="color:#f92672">import</span> RasaNLUConfig
<span style="color:#f92672">from</span> rasa_nlu.model <span style="color:#f92672">import</span> Trainer

pipeline <span style="color:#f92672">=</span> [
    <span style="color:#e6db74">&#34;nlp_spacy&#34;</span>,
    <span style="color:#e6db74">&#34;tokenizer_spacy&#34;</span>,
    <span style="color:#e6db74">&#34;ner_crf&#34;</span>
]

<span style="color:#75715e"># Create a config that uses this pipeline</span>
config <span style="color:#f92672">=</span> RasaNLUConfig(cmdline_args<span style="color:#f92672">=</span>{<span style="color:#e6db74">&#34;pipeline&#34;</span>: pipeline})

<span style="color:#75715e"># Create a trainer that uses this config</span>
trainer <span style="color:#f92672">=</span> Trainer(config)

<span style="color:#75715e"># Create an interpreter by training the model</span>
interpreter <span style="color:#f92672">=</span> trainer<span style="color:#f92672">.</span>train(training_data)

<span style="color:#75715e"># Parse some messages</span>
<span style="color:#66d9ef">print</span>(interpreter<span style="color:#f92672">.</span>parse(<span style="color:#e6db74">&#34;show me Chinese food in the centre of town&#34;</span>))
<span style="color:#66d9ef">print</span>(interpreter<span style="color:#f92672">.</span>parse(<span style="color:#e6db74">&#34;I want an Indian restaurant in the west&#34;</span>))
<span style="color:#66d9ef">print</span>(interpreter<span style="color:#f92672">.</span>parse(<span style="color:#e6db74">&#34;are there any good pizza places in the center?&#34;</span>))</code></pre></div>
<h2 id="virtual-assistance">Virtual Assistance</h2>

<ul>
<li>accessing SQL</li>
<li>bad practice to SQL injection, using python syntax like {} .foramt() on query</li>
<li>good practice: t= (area, price) ; c.execute(&ldquo;&hellip;.&rdquo;, t)</li>
</ul>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-python" data-lang="python"><span style="color:#f92672">import</span> sqlite3

conn <span style="color:#f92672">=</span> sqlite3<span style="color:#f92672">.</span>connect(<span style="color:#e6db74">&#39;Data_Folder/hotels.db&#39;</span>)

conn</code></pre></div>
<pre><code>&lt;sqlite3.Connection at 0x12a847c70&gt;
</code></pre>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-python" data-lang="python">c <span style="color:#f92672">=</span> conn<span style="color:#f92672">.</span>cursor()

c<span style="color:#f92672">.</span>execute(<span style="color:#e6db74">&#34;SELECT * FROM hotels WHERE area=&#39;south&#39; and price=&#39;hi&#39;&#34;</span>)</code></pre></div>
<pre><code>&lt;sqlite3.Cursor at 0x12a8339d0&gt;
</code></pre>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-python" data-lang="python">c<span style="color:#f92672">.</span>fetchall()</code></pre></div>
<pre><code>[('Grand Hotel', 'hi', 'south', 5)]
</code></pre>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-python" data-lang="python">area, price <span style="color:#f92672">=</span> <span style="color:#e6db74">&#34;south&#34;</span>, <span style="color:#e6db74">&#34;hi&#34;</span>
t <span style="color:#f92672">=</span> (area, price)

<span style="color:#75715e"># key</span>
c<span style="color:#f92672">.</span>execute(<span style="color:#e6db74">&#39;SELECT * FROM hotels WHERE area=? AND price=?&#39;</span>, t)

<span style="color:#66d9ef">print</span>(c<span style="color:#f92672">.</span>fetchall())</code></pre></div>
<pre><code>&lt;sqlite3.Cursor at 0x12a8339d0&gt;



[('Grand Hotel', 'hi', 'south', 5)]
</code></pre>

<h2 id="exploring-db-with-nl">Exploring DB with NL</h2>

<ul>
<li>Logic

<ol>
<li>using trained <strong>Rasa interpreter</strong> to <strong>parser</strong> message</li>
<li>result = <strong>entities dict</strong></li>
<li>define params = {} storing key-val of entities</li>
<li>feed query by filtering by params</li>
<li>execute query with condition AND (or .join(&lsquo;query&rsquo;)</li>
<li>Responses: default None; but result -&gt; &hellip;and or but&hellip;</li>
</ol></li>
</ul>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-python" data-lang="python"><span style="color:#75715e"># Define find_hotels()</span>
<span style="color:#66d9ef">def</span> <span style="color:#a6e22e">find_hotels</span>(params):
    <span style="color:#75715e"># Create the base query</span>
    query <span style="color:#f92672">=</span> <span style="color:#e6db74">&#39;SELECT * FROM hotels&#39;</span>
    <span style="color:#75715e"># Add filter clauses for each of the parameters</span>
    <span style="color:#66d9ef">if</span> len(params) <span style="color:#f92672">&gt;</span> <span style="color:#ae81ff">0</span>:
        filters <span style="color:#f92672">=</span> [<span style="color:#e6db74">&#34;{}=?&#34;</span><span style="color:#f92672">.</span>format(k) <span style="color:#66d9ef">for</span> k <span style="color:#f92672">in</span> params]
        query <span style="color:#f92672">+=</span> <span style="color:#e6db74">&#34; WHERE &#34;</span> <span style="color:#f92672">+</span> <span style="color:#e6db74">&#34; and &#34;</span><span style="color:#f92672">.</span>join(filters)
    <span style="color:#75715e"># Create the tuple of values</span>
    t <span style="color:#f92672">=</span> tuple(params<span style="color:#f92672">.</span>values())
    
    <span style="color:#75715e"># Open connection to DB</span>
    conn <span style="color:#f92672">=</span> sqlite3<span style="color:#f92672">.</span>connect(<span style="color:#e6db74">&#39;hotels.db&#39;</span>)
    <span style="color:#75715e"># Create a cursor</span>
    c <span style="color:#f92672">=</span> conn<span style="color:#f92672">.</span>cursor()
    <span style="color:#75715e"># Execute the query</span>
    c<span style="color:#f92672">.</span>execute(query, t)
    <span style="color:#75715e"># Return the results</span>
    <span style="color:#66d9ef">return</span> c<span style="color:#f92672">.</span>fetchall()</code></pre></div>
<h3 id="above-find-and-match-any-range-combination">Above find and match any range combination</h3>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-python" data-lang="python"><span style="color:#75715e"># Create the dictionary of column names and values</span>
params <span style="color:#f92672">=</span> {<span style="color:#e6db74">&#34;area&#34;</span>: <span style="color:#e6db74">&#34;south&#34;</span>, <span style="color:#e6db74">&#34;price&#34;</span>:<span style="color:#e6db74">&#34;lo&#34;</span>}

<span style="color:#75715e"># Find the hotels that match the parameters</span>
<span style="color:#66d9ef">print</span>(find_hotels(params))


<span style="color:#75715e"># Define respond()</span>
<span style="color:#66d9ef">def</span> <span style="color:#a6e22e">respond</span>(message):
    <span style="color:#75715e"># Extract the entities</span>
    entities <span style="color:#f92672">=</span> interpreter<span style="color:#f92672">.</span>parse(message)[<span style="color:#e6db74">&#34;entities&#34;</span>]
    <span style="color:#75715e"># Initialize an empty params dictionary</span>
    params <span style="color:#f92672">=</span> {}
    <span style="color:#75715e"># Fill the dictionary with entities</span>
    <span style="color:#66d9ef">for</span> ent <span style="color:#f92672">in</span> entities:
        params[ent[<span style="color:#e6db74">&#34;entity&#34;</span>]] <span style="color:#f92672">=</span> str(ent[<span style="color:#e6db74">&#34;value&#34;</span>])

    <span style="color:#75715e"># Find hotels that match the dictionary</span>
    results <span style="color:#f92672">=</span> find_hotels(params)
    <span style="color:#75715e"># Get the names of the hotels and index of the response</span>
    names <span style="color:#f92672">=</span> [r[<span style="color:#ae81ff">0</span>] <span style="color:#66d9ef">for</span> r <span style="color:#f92672">in</span> results]
    n <span style="color:#f92672">=</span> min(len(results),<span style="color:#ae81ff">3</span>)
    <span style="color:#75715e"># Select the nth element of the responses array</span>
    <span style="color:#66d9ef">return</span> responses[n]<span style="color:#f92672">.</span>format(<span style="color:#f92672">*</span>names)

<span style="color:#75715e"># Define respond()</span>
<span style="color:#66d9ef">def</span> <span style="color:#a6e22e">respond</span>(message):
    <span style="color:#75715e"># Extract the entities</span>
    entities <span style="color:#f92672">=</span> interpreter<span style="color:#f92672">.</span>parse(message)[<span style="color:#e6db74">&#34;entities&#34;</span>]
    <span style="color:#75715e"># Initialize an empty params dictionary</span>
    params <span style="color:#f92672">=</span> {}
    <span style="color:#75715e"># Fill the dictionary with entities</span>
    <span style="color:#66d9ef">for</span> ent <span style="color:#f92672">in</span> entities:
        params[ent[<span style="color:#e6db74">&#34;entity&#34;</span>]] <span style="color:#f92672">=</span> str(ent[<span style="color:#e6db74">&#34;value&#34;</span>])

    <span style="color:#75715e"># Find hotels that match the dictionary</span>
    results <span style="color:#f92672">=</span> find_hotels(params)
    <span style="color:#75715e"># Get the names of the hotels and index of the response</span>
    names <span style="color:#f92672">=</span> [r[<span style="color:#ae81ff">0</span>] <span style="color:#66d9ef">for</span> r <span style="color:#f92672">in</span> results]
    n <span style="color:#f92672">=</span> min(len(results),<span style="color:#ae81ff">3</span>)
    <span style="color:#75715e"># Select the nth element of the responses array</span>
    <span style="color:#66d9ef">return</span> responses[n]<span style="color:#f92672">.</span>format(<span style="color:#f92672">*</span>names)

<span style="color:#66d9ef">print</span>(respond(<span style="color:#e6db74">&#34;I want an expensive hotel in the south of town&#34;</span>))</code></pre></div>
<h2 id="incremental-slot-filling-and-negation">Incremental Slot Filling and Negation</h2>

<ul>
<li><strong>memory-filled response</strong> incrementally</li>
<li>Basic Memory - saving params in memory</li>
<li><strong>negation</strong> filtering response by negation or certainty</li>
<li>tricky topic</li>
<li>Negated entities - &ldquo;no, not, etc&rdquo; + NE

<ol>
<li>&lsquo;not sushi, maybe pizza?&rsquo;</li>
</ol></li>
</ul>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-python" data-lang="python"><span style="color:#75715e"># Define a respond function, taking the message and existing params as input</span>
<span style="color:#66d9ef">def</span> <span style="color:#a6e22e">respond</span>(message, params):
    <span style="color:#75715e"># Extract the entities</span>
    entities <span style="color:#f92672">=</span> interpreter<span style="color:#f92672">.</span>parse(message)[<span style="color:#e6db74">&#34;entities&#34;</span>]
    <span style="color:#75715e"># Fill the dictionary with entities</span>
    <span style="color:#66d9ef">for</span> ent <span style="color:#f92672">in</span> entities:
        params[ent[<span style="color:#e6db74">&#34;entity&#34;</span>]] <span style="color:#f92672">=</span> str(ent[<span style="color:#e6db74">&#34;value&#34;</span>])

    <span style="color:#75715e"># Find the hotels</span>
    results <span style="color:#f92672">=</span> find_hotels(params)
    names <span style="color:#f92672">=</span> [r[<span style="color:#ae81ff">0</span>] <span style="color:#66d9ef">for</span> r <span style="color:#f92672">in</span> results]
    n <span style="color:#f92672">=</span> min(len(results), <span style="color:#ae81ff">3</span>)
    <span style="color:#75715e"># Return the appropriate response</span>
    <span style="color:#66d9ef">return</span> responses[n]<span style="color:#f92672">.</span>format(<span style="color:#f92672">*</span>names), params

<span style="color:#75715e"># Initialize params dictionary</span>
params <span style="color:#f92672">=</span> {}

<span style="color:#75715e"># Pass the messages to the bot</span>
<span style="color:#66d9ef">for</span> message <span style="color:#f92672">in</span> [<span style="color:#e6db74">&#34;I want an expensive hotel&#34;</span>, <span style="color:#e6db74">&#34;in the north of town&#34;</span>]:
    <span style="color:#66d9ef">print</span>(<span style="color:#e6db74">&#34;USER: {}&#34;</span><span style="color:#f92672">.</span>format(message))
    response, params <span style="color:#f92672">=</span> respond(message, params)
    <span style="color:#66d9ef">print</span>(<span style="color:#e6db74">&#34;BOT: {}&#34;</span><span style="color:#f92672">.</span>format(response))</code></pre></div>
<h4 id="basic-negation">Basic Negation</h4>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-python" data-lang="python"><span style="color:#75715e"># Define negated_ents()</span>
<span style="color:#66d9ef">def</span> <span style="color:#a6e22e">negated_ents</span>(phrase):
    <span style="color:#75715e"># Extract the entities using keyword matching</span>
    ents <span style="color:#f92672">=</span> [e <span style="color:#66d9ef">for</span> e <span style="color:#f92672">in</span> [<span style="color:#e6db74">&#34;south&#34;</span>, <span style="color:#e6db74">&#34;north&#34;</span>] <span style="color:#66d9ef">if</span> e <span style="color:#f92672">in</span> phrase]
    <span style="color:#75715e"># Find the index of the final character of each entity</span>
    ends <span style="color:#f92672">=</span> sorted([phrase<span style="color:#f92672">.</span>index(e) <span style="color:#f92672">+</span> len(e) <span style="color:#66d9ef">for</span> e <span style="color:#f92672">in</span> ents])
    <span style="color:#75715e"># Initialise a list to store sentence chunks</span>
    chunks <span style="color:#f92672">=</span> []
    <span style="color:#75715e"># Take slices of the sentence up to and including each entitiy</span>
    start <span style="color:#f92672">=</span> <span style="color:#ae81ff">0</span>
    <span style="color:#66d9ef">for</span> end <span style="color:#f92672">in</span> ends:
        chunks<span style="color:#f92672">.</span>append(phrase[start:end])
        start <span style="color:#f92672">=</span> end
    result <span style="color:#f92672">=</span> {}
    <span style="color:#75715e"># Iterate over the chunks and look for entities</span>
    <span style="color:#66d9ef">for</span> chunk <span style="color:#f92672">in</span> chunks:
        <span style="color:#66d9ef">for</span> ent <span style="color:#f92672">in</span> ents:
            <span style="color:#66d9ef">if</span> ent <span style="color:#f92672">in</span> chunk:
                <span style="color:#75715e"># If the entity is preceeded by a negation, give it the key False</span>
                <span style="color:#66d9ef">if</span> <span style="color:#e6db74">&#34;not&#34;</span> <span style="color:#f92672">in</span> chunk <span style="color:#f92672">or</span> <span style="color:#e6db74">&#34;n&#39;t&#34;</span> <span style="color:#f92672">in</span> chunk:
                    result[ent] <span style="color:#f92672">=</span> False
                <span style="color:#66d9ef">else</span>:
                    result[ent] <span style="color:#f92672">=</span> True
    <span style="color:#66d9ef">return</span> result  

<span style="color:#75715e"># Check that the entities are correctly assigned as True or False</span>
<span style="color:#66d9ef">for</span> test <span style="color:#f92672">in</span> tests:
    <span style="color:#66d9ef">print</span>(negated_ents(test[<span style="color:#ae81ff">0</span>]) <span style="color:#f92672">==</span> test[<span style="color:#ae81ff">1</span>])</code></pre></div><div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-python" data-lang="python"><span style="color:#75715e"># Define the respond function</span>
<span style="color:#66d9ef">def</span> <span style="color:#a6e22e">respond</span>(message, params, neg_params):
    <span style="color:#75715e"># Extract the entities</span>
    entities <span style="color:#f92672">=</span> interpreter<span style="color:#f92672">.</span>parse(message)[<span style="color:#e6db74">&#34;entities&#34;</span>]
    ent_vals <span style="color:#f92672">=</span> [e[<span style="color:#e6db74">&#34;value&#34;</span>] <span style="color:#66d9ef">for</span> e <span style="color:#f92672">in</span> entities]
    <span style="color:#75715e"># Look for negated entities</span>
    negated <span style="color:#f92672">=</span> negated_ents(message, ent_vals)
    <span style="color:#66d9ef">for</span> ent <span style="color:#f92672">in</span> entities:
        <span style="color:#66d9ef">if</span> ent[<span style="color:#e6db74">&#34;value&#34;</span>] <span style="color:#f92672">in</span> negated <span style="color:#f92672">and</span> negated[ent[<span style="color:#e6db74">&#34;value&#34;</span>]]:
            neg_params[ent[<span style="color:#e6db74">&#34;entity&#34;</span>]] <span style="color:#f92672">=</span> str(ent[<span style="color:#e6db74">&#34;value&#34;</span>])
        <span style="color:#66d9ef">else</span>:
            params[ent[<span style="color:#e6db74">&#34;entity&#34;</span>]] <span style="color:#f92672">=</span> str(ent[<span style="color:#e6db74">&#34;value&#34;</span>])
    <span style="color:#75715e"># Find the hotels</span>
    results <span style="color:#f92672">=</span> find_hotels(params, neg_params)
    names <span style="color:#f92672">=</span> [r[<span style="color:#ae81ff">0</span>] <span style="color:#66d9ef">for</span> r <span style="color:#f92672">in</span> results]
    n <span style="color:#f92672">=</span> min(len(results),<span style="color:#ae81ff">3</span>)
    <span style="color:#75715e"># Return the correct response</span>
    <span style="color:#66d9ef">return</span> responses[n]<span style="color:#f92672">.</span>format(<span style="color:#f92672">*</span>names), params, neg_params

<span style="color:#75715e"># Initialize params and neg_params</span>
params <span style="color:#f92672">=</span> {}
neg_params <span style="color:#f92672">=</span> {}

<span style="color:#75715e"># Pass the messages to the bot</span>
<span style="color:#66d9ef">for</span> message <span style="color:#f92672">in</span> [<span style="color:#e6db74">&#34;I want a cheap hotel&#34;</span>, <span style="color:#e6db74">&#34;but not in the north of town&#34;</span>]:
    <span style="color:#66d9ef">print</span>(<span style="color:#e6db74">&#34;USER: {}&#34;</span><span style="color:#f92672">.</span>format(message))
    response, params, neg_params <span style="color:#f92672">=</span> respond(message, params, neg_params)
    <span style="color:#66d9ef">print</span>(<span style="color:#e6db74">&#34;BOT: {}&#34;</span><span style="color:#f92672">.</span>format(response))</code></pre></div>
<h2 id="statefulness">Statefulness</h2>

<ul>
<li>Policy mapping user input with action</li>
<li>Sophistication adding select action content-dependency</li>
<li>additional memory, <strong>state machine</strong> e.g. traffic light (3 states)

<ol>
<li>state dependency</li>
<li>e-commerce - browsing, info, order completion, questions</li>
<li>int used for states</li>
</ol></li>
<li>example: coffee ordering

<ol>
<li>INIT state - order intent -&gt; choose state</li>
<li>ORDER -&gt; take input from INIT and CHOOSE</li>
<li>sequential dict()</li>
</ol></li>
</ul>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-python" data-lang="python"><span style="color:#75715e"># Define the INIT state</span>
INIT <span style="color:#f92672">=</span> <span style="color:#ae81ff">0</span>

<span style="color:#75715e"># Define the CHOOSE_COFFEE state</span>
CHOOSE_COFFEE <span style="color:#f92672">=</span> <span style="color:#ae81ff">1</span>

<span style="color:#75715e"># Define the ORDERED state</span>
ORDERED <span style="color:#f92672">=</span> <span style="color:#ae81ff">2</span>

<span style="color:#75715e"># Define the policy rules</span>
policy <span style="color:#f92672">=</span> {
    (INIT, <span style="color:#e6db74">&#34;order&#34;</span>): (CHOOSE_COFFEE, <span style="color:#e6db74">&#34;ok, Columbian or Kenyan?&#34;</span>),
    (INIT, <span style="color:#e6db74">&#34;none&#34;</span>): (INIT, <span style="color:#e6db74">&#34;I&#39;m sorry - I&#39;m not sure how to help you&#34;</span>),
    (CHOOSE_COFFEE, <span style="color:#e6db74">&#34;specify_coffee&#34;</span>): (ORDERED, <span style="color:#e6db74">&#34;perfect, the beans are on their way!&#34;</span>),
    (CHOOSE_COFFEE, <span style="color:#e6db74">&#34;none&#34;</span>): (CHOOSE_COFFEE, <span style="color:#e6db74">&#34;I&#39;m sorry - would you like Colombian or Kenyan?&#34;</span>),
}

<span style="color:#75715e"># Create the list of messages</span>
messages <span style="color:#f92672">=</span> [
    <span style="color:#e6db74">&#34;I&#39;d like to become a professional dancer&#34;</span>,
    <span style="color:#e6db74">&#34;well then I&#39;d like to order some coffee&#34;</span>,
    <span style="color:#e6db74">&#34;my favourite animal is a zebra&#34;</span>,
    <span style="color:#e6db74">&#34;kenyan&#34;</span>
]

<span style="color:#75715e"># Call send_message() for each message</span>
state <span style="color:#f92672">=</span> INIT
<span style="color:#66d9ef">for</span> message <span style="color:#f92672">in</span> messages:    
    state <span style="color:#f92672">=</span> send_message(policy, state, message)</code></pre></div><div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-python" data-lang="python"><span style="color:#75715e"># Define the states</span>
INIT<span style="color:#f92672">=</span><span style="color:#ae81ff">0</span> 
CHOOSE_COFFEE<span style="color:#f92672">=</span><span style="color:#ae81ff">1</span>
ORDERED<span style="color:#f92672">=</span><span style="color:#ae81ff">2</span>

<span style="color:#75715e"># Define the policy rules dictionary</span>
policy_rules <span style="color:#f92672">=</span> {
    (INIT, <span style="color:#e6db74">&#34;ask_explanation&#34;</span>): (INIT, <span style="color:#e6db74">&#34;I&#39;m a bot to help you order coffee beans&#34;</span>),
    (INIT, <span style="color:#e6db74">&#34;order&#34;</span>): (CHOOSE_COFFEE, <span style="color:#e6db74">&#34;ok, Columbian or Kenyan?&#34;</span>),
    (CHOOSE_COFFEE, <span style="color:#e6db74">&#34;specify_coffee&#34;</span>): (ORDERED, <span style="color:#e6db74">&#34;perfect, the beans are on their way!&#34;</span>),
    (CHOOSE_COFFEE, <span style="color:#e6db74">&#34;ask_explanation&#34;</span>): (CHOOSE_COFFEE, <span style="color:#e6db74">&#34;We have two kinds of coffee beans - the Kenyan ones make a slightly sweeter coffee, and cost $6. The Brazilian beans make a nutty coffee and cost $5.&#34;</span>)    
}

<span style="color:#75715e"># Define send_messages()</span>
<span style="color:#66d9ef">def</span> <span style="color:#a6e22e">send_messages</span>(messages):
    state <span style="color:#f92672">=</span> INIT
    <span style="color:#66d9ef">for</span> msg <span style="color:#f92672">in</span> messages:
        state <span style="color:#f92672">=</span> send_message(state, msg)

<span style="color:#75715e"># Send the messages</span>
send_messages([
    <span style="color:#e6db74">&#34;what can you do for me?&#34;</span>,
    <span style="color:#e6db74">&#34;well then I&#39;d like to order some coffee&#34;</span>,
    <span style="color:#e6db74">&#34;what do you mean by that?&#34;</span>,
    <span style="color:#e6db74">&#34;kenyan&#34;</span>
])</code></pre></div><div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-python" data-lang="python"><span style="color:#75715e"># Define respond()</span>
<span style="color:#66d9ef">def</span> <span style="color:#a6e22e">respond</span>(message, params, prev_suggestions, excluded):
    <span style="color:#75715e"># Interpret the message</span>
    parse_data <span style="color:#f92672">=</span> interpret(message)
    <span style="color:#75715e"># Extract the intent</span>
    intent <span style="color:#f92672">=</span> parse_data[<span style="color:#e6db74">&#34;intent&#34;</span>][<span style="color:#e6db74">&#34;name&#34;</span>]
    <span style="color:#75715e"># Extract the entities</span>
    entities <span style="color:#f92672">=</span> parse_data[<span style="color:#e6db74">&#34;entities&#34;</span>]
    <span style="color:#75715e"># Add the suggestion to the excluded list if intent is &#34;deny&#34;</span>
    <span style="color:#66d9ef">if</span> intent <span style="color:#f92672">==</span> <span style="color:#e6db74">&#34;deny&#34;</span>:
        excluded<span style="color:#f92672">.</span>extend(prev_suggestions)
    <span style="color:#75715e"># Fill the dictionary with entities</span>
    <span style="color:#66d9ef">for</span> ent <span style="color:#f92672">in</span> entities:
        params[ent[<span style="color:#e6db74">&#34;entity&#34;</span>]] <span style="color:#f92672">=</span> str(ent[<span style="color:#e6db74">&#34;value&#34;</span>])
    <span style="color:#75715e"># Find matching hotels</span>
    results <span style="color:#f92672">=</span> [
        r 
        <span style="color:#66d9ef">for</span> r <span style="color:#f92672">in</span> find_hotels(params, excluded) 
        <span style="color:#66d9ef">if</span> r[<span style="color:#ae81ff">0</span>] <span style="color:#f92672">not</span> <span style="color:#f92672">in</span> excluded
    ]
    <span style="color:#75715e"># Extract the suggestions</span>
    names <span style="color:#f92672">=</span> [r[<span style="color:#ae81ff">0</span>] <span style="color:#66d9ef">for</span> r <span style="color:#f92672">in</span> results]
    n <span style="color:#f92672">=</span> min(len(results), <span style="color:#ae81ff">3</span>)
    suggestions <span style="color:#f92672">=</span> names[:<span style="color:#ae81ff">2</span>]
    <span style="color:#66d9ef">return</span> responses[n]<span style="color:#f92672">.</span>format(<span style="color:#f92672">*</span>names), params, suggestions, excluded

<span style="color:#75715e"># Initialize the empty dictionary and lists</span>
params, suggestions, excluded <span style="color:#f92672">=</span> {}, [], []

<span style="color:#75715e"># Send the messages</span>
<span style="color:#66d9ef">for</span> message <span style="color:#f92672">in</span> [<span style="color:#e6db74">&#34;I want a mid range hotel&#34;</span>, <span style="color:#e6db74">&#34;no that doesn&#39;t work for me&#34;</span>]:
    <span style="color:#66d9ef">print</span>(<span style="color:#e6db74">&#34;USER: {}&#34;</span><span style="color:#f92672">.</span>format(message))
    response, params, suggestions, excluded <span style="color:#f92672">=</span> respond(message, params, suggestions, excluded)
    <span style="color:#66d9ef">print</span>(<span style="color:#e6db74">&#34;BOT: {}&#34;</span><span style="color:#f92672">.</span>format(response))</code></pre></div>
<h2 id="question-and-queuing-answers">Question and Queuing Answers</h2>

<ul>
<li>State machine building up memory and rules</li>
<li><strong>complexity reduction</strong> is needed</li>
<li>e.g. coffee filter added in sales

<ol>
<li>add additional states with policy on handling yes or no</li>
<li>adding more states up complexity</li>
<li>solution: <strong>Pending actions</strong> -&gt; select action + pending_action (None)</li>
<li>pending_action is saved in outer scope</li>
<li>if &lsquo;yes&rsquo; intent, pending, else none</li>
<li>Pending state transitions -&gt; authentication states -&gt; request info -&gt; transition to order state -&gt; order</li>
</ol></li>
</ul>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-python" data-lang="python"><span style="color:#75715e"># Define policy()</span>
<span style="color:#66d9ef">def</span> <span style="color:#a6e22e">policy</span>(intent):
    <span style="color:#75715e"># Return &#34;do_pending&#34; if the intent is &#34;affirm&#34;</span>
    <span style="color:#66d9ef">if</span> intent <span style="color:#f92672">==</span> <span style="color:#e6db74">&#34;affirm&#34;</span>:
        <span style="color:#66d9ef">return</span> <span style="color:#e6db74">&#34;do_pending&#34;</span>, None
    <span style="color:#75715e"># Return &#34;Ok&#34; if the intent is &#34;deny&#34;</span>
    <span style="color:#66d9ef">if</span> intent <span style="color:#f92672">==</span> <span style="color:#e6db74">&#34;deny&#34;</span>:
        <span style="color:#66d9ef">return</span> <span style="color:#e6db74">&#34;Ok&#34;</span>, None
    <span style="color:#66d9ef">if</span> intent <span style="color:#f92672">==</span> <span style="color:#e6db74">&#34;order&#34;</span>:
        <span style="color:#66d9ef">return</span> <span style="color:#e6db74">&#34;Unfortunately, the Kenyan coffee is currently out of stock, would you like to order the Brazilian beans?&#34;</span>, <span style="color:#e6db74">&#34;Alright, I&#39;ve ordered that for you!&#34;</span></code></pre></div>
<h4 id="incorporate-it-into-send-message-func">Incorporate it into send_message() func</h4>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-python" data-lang="python"><span style="color:#75715e"># Define send_message()</span>
<span style="color:#66d9ef">def</span> <span style="color:#a6e22e">send_message</span>(pending, message):
    <span style="color:#66d9ef">print</span>(<span style="color:#e6db74">&#34;USER : {}&#34;</span><span style="color:#f92672">.</span>format(message))
    action, pending_action <span style="color:#f92672">=</span> policy(interpret(message))
    <span style="color:#66d9ef">if</span> action <span style="color:#f92672">==</span> <span style="color:#e6db74">&#39;do_pending&#39;</span> <span style="color:#f92672">and</span> pending <span style="color:#f92672">is</span> <span style="color:#f92672">not</span> None:
        <span style="color:#66d9ef">print</span>(<span style="color:#e6db74">&#34;BOT : {}&#34;</span><span style="color:#f92672">.</span>format(pending))
    <span style="color:#66d9ef">else</span>:
        <span style="color:#66d9ef">print</span>(<span style="color:#e6db74">&#34;BOT : {}&#34;</span><span style="color:#f92672">.</span>format(action))
    <span style="color:#66d9ef">return</span> pending_action
    
<span style="color:#75715e"># Define send_messages()</span>
<span style="color:#66d9ef">def</span> <span style="color:#a6e22e">send_messages</span>(messages):
    pending <span style="color:#f92672">=</span> None
    <span style="color:#66d9ef">for</span> msg <span style="color:#f92672">in</span> messages:
        pending <span style="color:#f92672">=</span> send_message(pending, msg)

<span style="color:#75715e"># Send the messages</span>
send_messages([
    <span style="color:#e6db74">&#34;I&#39;d like to order some coffee&#34;</span>,
    <span style="color:#e6db74">&#34;ok yes please&#34;</span>
])</code></pre></div><div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-python" data-lang="python"><span style="color:#75715e"># Define the states</span>
INIT<span style="color:#f92672">=</span><span style="color:#ae81ff">0</span>
AUTHED<span style="color:#f92672">=</span><span style="color:#ae81ff">1</span>
CHOOSE_COFFEE<span style="color:#f92672">=</span><span style="color:#ae81ff">2</span>
ORDERED<span style="color:#f92672">=</span><span style="color:#ae81ff">3</span>

<span style="color:#75715e"># Define the policy rules</span>
policy_rules <span style="color:#f92672">=</span> {
    (INIT, <span style="color:#e6db74">&#34;order&#34;</span>): (INIT, <span style="color:#e6db74">&#34;you&#39;ll have to log in first, what&#39;s your phone number?&#34;</span>, AUTHED),
    (INIT, <span style="color:#e6db74">&#34;number&#34;</span>): (AUTHED, <span style="color:#e6db74">&#34;perfect, welcome back!&#34;</span>, None),
    (AUTHED, <span style="color:#e6db74">&#34;order&#34;</span>): (CHOOSE_COFFEE, <span style="color:#e6db74">&#34;would you like Columbian or Kenyan?&#34;</span>, None),    
    (CHOOSE_COFFEE, <span style="color:#e6db74">&#34;specify_coffee&#34;</span>): (ORDERED, <span style="color:#e6db74">&#34;perfect, the beans are on their way!&#34;</span>, None)
}

<span style="color:#75715e"># Define send_messages()</span>
<span style="color:#66d9ef">def</span> <span style="color:#a6e22e">send_messages</span>(messages):
    state <span style="color:#f92672">=</span> INIT
    pending <span style="color:#f92672">=</span> None
    <span style="color:#66d9ef">for</span> msg <span style="color:#f92672">in</span> messages:
        state, pending <span style="color:#f92672">=</span> send_message(state, pending, msg)

<span style="color:#75715e"># Send the messages</span>
send_messages([
    <span style="color:#e6db74">&#34;I&#39;d like to order some coffee&#34;</span>,
    <span style="color:#e6db74">&#34;555-12345&#34;</span>,
    <span style="color:#e6db74">&#34;kenyan&#34;</span>
])</code></pre></div><div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-python" data-lang="python"><span style="color:#75715e"># Define chitchat_response()</span>
<span style="color:#66d9ef">def</span> <span style="color:#a6e22e">chitchat_response</span>(message):
    <span style="color:#75715e"># Call match_rule()</span>
    response, phrase <span style="color:#f92672">=</span> match_rule(eliza_rules, message)
    <span style="color:#75715e"># Return none is response is &#34;default&#34;</span>
    <span style="color:#66d9ef">if</span> response <span style="color:#f92672">==</span> <span style="color:#e6db74">&#34;default&#34;</span>:
        <span style="color:#66d9ef">return</span> None
    <span style="color:#66d9ef">if</span> <span style="color:#e6db74">&#39;{0}&#39;</span> <span style="color:#f92672">in</span> response:
        <span style="color:#75715e"># Replace the pronouns of phrase</span>
        phrase <span style="color:#f92672">=</span> replace_pronouns(phrase)
        <span style="color:#75715e"># Calculate the response</span>
        response <span style="color:#f92672">=</span> response<span style="color:#f92672">.</span>format(phrase)
    <span style="color:#66d9ef">return</span> response</code></pre></div><div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-python" data-lang="python"><span style="color:#75715e"># Define send_message()</span>
<span style="color:#66d9ef">def</span> <span style="color:#a6e22e">send_message</span>(state, pending, message):
    <span style="color:#66d9ef">print</span>(<span style="color:#e6db74">&#34;USER : {}&#34;</span><span style="color:#f92672">.</span>format(message))
    response <span style="color:#f92672">=</span> chitchat_response(message)
    <span style="color:#66d9ef">if</span> response <span style="color:#f92672">is</span> <span style="color:#f92672">not</span> None:
        <span style="color:#66d9ef">print</span>(<span style="color:#e6db74">&#34;BOT : {}&#34;</span><span style="color:#f92672">.</span>format(response))
        <span style="color:#66d9ef">return</span> state, None
    
    <span style="color:#75715e"># Calculate the new_state, response, and pending_state</span>
    new_state, response, pending_state <span style="color:#f92672">=</span> policy_rules[(state, interpret(message))]
    <span style="color:#66d9ef">print</span>(<span style="color:#e6db74">&#34;BOT : {}&#34;</span><span style="color:#f92672">.</span>format(response))
    <span style="color:#66d9ef">if</span> pending <span style="color:#f92672">is</span> <span style="color:#f92672">not</span> None:
        new_state, response, pending_state <span style="color:#f92672">=</span> policy_rules[pending]
        <span style="color:#66d9ef">print</span>(<span style="color:#e6db74">&#34;BOT : {}&#34;</span><span style="color:#f92672">.</span>format(response))        
    <span style="color:#66d9ef">if</span> pending_state <span style="color:#f92672">is</span> <span style="color:#f92672">not</span> None:
        pending <span style="color:#f92672">=</span> (pending_state, interpret(message))
    <span style="color:#66d9ef">return</span> new_state, pending

<span style="color:#75715e"># Define send_messages()</span>
<span style="color:#66d9ef">def</span> <span style="color:#a6e22e">send_messages</span>(messages):
    state <span style="color:#f92672">=</span> INIT
    pending <span style="color:#f92672">=</span> None
    <span style="color:#66d9ef">for</span> msg <span style="color:#f92672">in</span> messages:
        state, pending <span style="color:#f92672">=</span> send_message(state, pending, msg)

<span style="color:#75715e"># Send the messages</span>
send_messages([
    <span style="color:#e6db74">&#34;I&#39;d like to order some coffee&#34;</span>,
    <span style="color:#e6db74">&#34;555-12345&#34;</span>,
    <span style="color:#e6db74">&#34;do you remember when I ordered 1000 kilos by accident?&#34;</span>,
    <span style="color:#e6db74">&#34;kenyan&#34;</span>
])  </code></pre></div>
<h2 id="frontiers-of-dialogue-tech">Frontiers of dialogue tech</h2>

<ul>
<li>many applications but not in chatbot</li>
<li>context-relevant data needed</li>
<li>Neural Conversational Model <strong>Seq2seq</strong>

<ol>
<li>machine translation</li>
<li>NN reads message, buiding up hidden vectors meaning</li>
<li>reverse -&gt; output sequence</li>
<li>totally different</li>
</ol></li>
</ul>

<h3 id="no-specified-intent-or-etc-utterly-data-driven">No specified intent or etc, utterly data-driven</h3>

<ul>
<li>not easy to integrate DB and API logic</li>
<li>previous hand-crafted, seq2seq data-driven</li>
<li>ML based:

<ol>
<li>NLU</li>
<li>Dialogue state manageuer</li>
<li>API logic (connector to real world, DB)</li>
<li>NL reponse generator</li>
</ol></li>
<li>&lsquo;Human pretend to be a bot: &lsquo;Wizard of Oz&rsquo; technique</li>
<li>RL - receives reward for successful conversation, improves over time</li>
</ul>

<h3 id="language-generation">Language generation</h3>

<ul>
<li>practically, bot not recommended - better crafted than generated</li>
<li>but fun topic</li>
<li>NN trained can generate text from certain topic database, e.g. simpson scripts</li>
</ul>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-python" data-lang="python"><span style="color:#75715e"># Feed the &#39;seed&#39; text into the neural network</span>
seed <span style="color:#f92672">=</span> <span style="color:#e6db74">&#34;i&#39;m gonna punch lenny in the back of the&#34;</span>

<span style="color:#75715e"># Iterate over the different temperature values</span>
<span style="color:#66d9ef">for</span> temperature <span style="color:#f92672">in</span> [<span style="color:#ae81ff">0.2</span>, <span style="color:#ae81ff">0.5</span>, <span style="color:#ae81ff">1.0</span>, <span style="color:#ae81ff">1.2</span>]:
    <span style="color:#66d9ef">print</span>(<span style="color:#e6db74">&#34;</span><span style="color:#ae81ff">\n</span><span style="color:#e6db74">Generating text with riskiness : {}</span><span style="color:#ae81ff">\n</span><span style="color:#e6db74">&#34;</span><span style="color:#f92672">.</span>format(temperature))
    <span style="color:#75715e"># Call the sample_text function</span>
    <span style="color:#66d9ef">print</span>(sample_text(seed, temperature))</code></pre></div>
<p><a href="https://www.datacamp.com/community/tutorials/facebook-chatbot-python-deploy">https://www.datacamp.com/community/tutorials/facebook-chatbot-python-deploy</a></p>
</div>

        
        <div class="section bottom-menu">
<hr />
<p>


    

    
        
            <a href="/code">
                code
            </a>
        
    
    
        
            &#183; 
            <a href="/prose">
                prose
            </a>
        
            &#183; 
            <a href="/gallery">
                gallery
            </a>
        
            &#183; 
            <a href="/qui">
                qui et quoi?
            </a>
        
    
    &#183; 
    <a href="/">
        main
    </a>

</p></div>
        

        <div class="section footer">Ocean Ode</div>
    </div>
</body>

</html>